jobOrResumeDescription,role,contact,jobSource,sourceType
": Artificial Intelligence / Machine Learning Developer
 
: Irving TX

Terms: Contract

 Details: 
 
 :

    Bachelor's degree or 7-10 or more years of relevant  experience.
    7+ years of server app development (design/develop/deploy).
    3+ years of Python 3.x, experience in ML algorithms/data analytics.
    5+ years of advanced SQL development (ER modeling, SQL scripts, stored procedures, functions, s) with RDBMS such as PostgreSQL/MS SQL Server.
    3+ years on AWS S3, EC2, Serverless computing (Lambda).
    3+ years of experience/familiarity with DevOps using Stash/Jenkins/Chef and Puppet.
    Excellent communication  in interfacing with different cross-functional teams.

:

    5+ years of experience in designing, building applications using .NET platform using C#, .NET Core, ORM, SQL, MS SQL Server, Visual Studio.
    1+ years' experience in developing containerized Docker .net core apps.",Developer,"CYNET SYSTEMS Inc. 
SAWAN.B@CYNETSYSTEMS.COM
Ph:(571) 645-5932  |  Fx:866.838.0907  | WWW.CYNETSYSTEMS.COM
IT & ENGINEERING CONSULTING
How did I do? For feedback please email myfeedback@cynetsystems.com 
 
Please visit http://jobs.cynetsystems.com to view our Open Jobs.",outlook,job recruiter
" : Data Scientist/Architect
: 6+ months + High possibility of extension
: New York, NY 10036
: $101/hr W2
 
:

    Develops cutting-edge algorithms and solutions for an industry disrupting product in the insurance industry
    Plays a critical  in guiding the data science team on effectively developing models
    Stays up-to- on current trends and state-of-the-art in the field of deep learning and computer vision and educates junior data scientist
    Leverages research and applies state-of-the-art deep learning techniques to inform recommendations and decisions
    s closely across different functions of the product team (engineering function, architecture function, business function, sales functions, etc.) to identify and incubate business cases

 
 

    Ph.D. in Computer Science, Statistics, Mathematics, or other related field
    Strong experience developing computer vision algorithms/models (image registration, thresholding, camera calibration, keypoint extraction, etc.) with state of the art deep learning frames
    Mastery of a wide variety of programming languages (Python, C++, etc.) and frames (TensorFlow, Keras, Caffe, Torch, etc.), demonstrating strong  in the development, articulation and implementation of coding practices for the team
    Demonstd experience ing with large, complex datasets using big data  and script language
    Experience on cloud platform such as AWS, AZURE, etc.
    Familiarity with version control systems
    Ability to turn research into industrial production
    Experience on 3D modeling from 2D images is a plus",Data Scientist,"Senior Technical Recruiter
eTeam Inc.
lpaulose@eteaminc.com
(732) 302-8953",outlook,job recruiter
" : Data Analyst
: Davidson, NC
: 04+ Months
 
 :
  and Experience:

    You are an expert in one or more business processes and  practices and are accountable for translating a business case into a detailed technical design.
    Alternatively, you are responsible for operational and technical issues and translate technical blueprints into  and specifications.
    You may also be responsible for integration testing and user acceptance testing.
    You act as a stream lead, guiding team members by experience.
    You are seen as active member within  communities.

 
Qualification: 6-8 years of experience (2 years minimum relevant experience) Bachelor?s Degree.
Certification: SE level 1 and  level 2.
Must have experience in Package Configuration.
Should be proficient in Business Analysis, Business Knowledge, Testing, Architecture Knowledge, Technical Solution Design and Vendor Management.
 
:

    The Data Scientist will  on a small team to  Reinforced Learning approach to Control Systems used in Compressed Air systems.
    Client is planning to use a toolkit called Reinforced Learning Toolkit from Maths which was recently launched, along with Matlab and Simulink toolkits.
    The Data Scientist will build data models, analyze data and  alongside system experts and project manager to deliver a proof of concept which uses AI to solve control problems and make improvements.

 
:

    Data Science background in Machine Learning, Artificial Intelligence (AI), and/or Neural Nets
    Maths Matlab and Simulink experience
    Maths Reinforced Learning Toolkit highly  (this is a new toolkit)
    Reinforced Learning expertise highly 
    Able to build Data Models
    Control System domain experience 
",Data Analyst,"Kunal Chourey
(973) 394-3943 Ext.3943
Kunal.Chourey@artech.com",outlook,job recruiter
" : Big Data Architect or Data Scientist
: New York, NY 
: 6 Months Contract
 
 :
? Develops cutting-edge algorithms and solutions for an industry-disrupting product in the insurance industry
? Plays a critical  in guiding the data science team on effectively developing models
? Stays up-to- on current trends and state-of-the-art in the field of deep learning and computer vision and educates junior data scientist
? Leverages research and applies state-of-the-art deep learning techniques to inform recommendations and decisions
? s closely across different functions of the product team (engineering function, architecture function, business function, sales functions, etc.) to identify and incubate business cases

 
? Ph.D. in Computer Science, Statistics, Mathematics, or another related field
? Strong experience developing computer vision algorithms/models (image registration, thresholding, camera calibration, keypoint extraction, etc.) with state of the art deep learning frames
? Mastery of a wide variety of programming languages (Python, C++, etc.) and frames (TensorFlow, Keras, Caffe, Torch, etc.), demonstrating strong  in the development, articulation, and implementation of coding practices for the team
? Demonstd experience ing with large, complex datasets using big data  and script language
? Experience on cloud platforms such as AWS, AZURE, etc.
? Familiarity with version control systems
? Ability to turn research into industrial production
? Experience on 3D modeling from 2D images is a plus",Data Scientist,"Technical Recruiter
eTeam Inc.
achhipa@eteaminc.com
(732) 307-0836",outlook,job recruiter
": Data Engineer
: Woonsocket, RI
: 6+ Months

 :

    Top tier consultant firm experience
    5 years of building products and architecting solutions.
    Experience advising clients on Data Modernization initiatives. Ability to deal with structured, semi-structured, unstructured and streaming data.
    Ability to lead proofs-of-concept and then effectively transition and scale those concepts into production at scale through, engineering, deployment, and commercialization.
    Serve as an expert; envision and integ emerging data , anticipate new trends to solve complex business and technical problems.
    Experience in sales, pre-sales functions, leading pursuits, proposal development, and statement of s.
    Ability to  in the United States without visa sponsorship now or in the future


Technical :

    Experienced ing with multiple top tier customer scenarios and loads to meet customer needs for performance, scalability, latency, & reliability.
    Experienced in Azure Cloud Platform. (HDI, CLI, Storage (ADLS, Blob, Tables, etc..), CosmosDB, SQL Data Warehouse, and Azure Functions).
    Experience designing, developing, optimizing and troubleshooting complex data-intensive applications using Spark, HDFS, Kafka, MapReduce, MongoDB, and other big data-related .
    Comfortable designing and implementing data warehouse and pipelines?such as ETL, data integration, and streaming?to support teams focused in Analytics.
    Must know pySpark, Python and SQL; comfortable with Java and/or Scala.
    Knowledge about DevOps like Azure DevOps methodologies and best practices. 

",Data Engineer,"Phone: (732) 227-1772 Ext.310
Mobile/Text: (732) 658-2580
Fax: (732) 909 2358
2050 Route 27, Suite 203, North Brunswick, NJ 08902
www.ittblazers.com  / Email: kranthi@ittblazers.com",outlook,job recruiter
"PhD or MS with 3 years post MS experience Computer Science, Engineering Electrical Engineering Physics Mathematics Statistics or an analytics discipline Proficient in Python or CJava
    Proficiency with image processing and or computer vision algorithms remote sensing
    Proficiency in Machine Learning algorithms and concepts Ensembles Deep Learning SVM etc
    Experience analyzing and presenting complex data and proven problem solving abilities
    Strong publication record in leading scientific journals Strong organizational interpersonal and written communication  Experience ing with agricultural biological scientific data is highly desired
    Drive for translating business problems into research initiatives that deliver business value Creativity in defining challenging exploratory 

",Data Scientist,"Next Level Business Services, Inc.
Staffing|Consulting|Outsourcing
Phone: (904) 371-3876 | Fax: +1 (608)646-8326
E-mail: rahul.sharma@nlbservices.com | Web:www.nlbservices.com",outlook,job recruiter
"  :
? Bachelor degree in Computer Science (or another  related degree) and 2 years of experience OR 3 years of experience in front-end development
? Must have ing experience with Developing programs running on Java/Python.
? Must have experience with strong UNIX shell scripting
? Must have experience with one of the IDE  such as Eclipse.
? Must have experience with SDLC Methodology (Agile / Scrum / Iterative Development).
? Must have experience with Problem solving /analytical thinking
?  experience with Spark and Scala/Python.
?  experience with NoSql Databases like HBASE, Mongo or Cassandra.
?  experience with developing Pig scripts/Hive QL, SQOOP
?  experience with developing MapReduce programs running on the Hadoop cluster using Java/Python.
?  experience using Talend with Hadoop .
?  experience with cloud computing infrastructure (e.g. Amazon Web Services EC2) and considerations for scalable, distributed systems
?  experience ing in an Agile Matrix environment like SDLC Methodology (Agile / Scrum / Iterative Development).
?  experience with Jenkins and Jira 
? Ability to  both independently and as part of a team.
? Ability to solve problems and have analytical thinking capabilities
? Excellent communication 
 ",Data Scientist,CRSGROUP | IT Staffing Team Lead | 630.467.0765 | visit us at www.crscorp.com,outlook,job recruiter
" ? Data Scientist
:  Irvine, CA 17951
 ? 1 year (possible conversion)

SAS enterprise miner, Predictive modelling, R, Python
Using tableau as excel spread sheet.
AI or Machine learning (Plus to have)
Education: BS or MS
 
 
The Data Scientist will be conducting statistical, predictive and descriptive analytics in partnership with our marketing, digital and product teams to turn data into critical information and knowledge that can be used to make sound business decisions. This individual must function in a fast-paced environment, be adaptable to the changing processes and  related to the business intelligence and analytical space.
 
? Leveraging large data sets to conduct end-to end analytics that will include data gathering,  specifications, processing, analytics, ongoing deliverables, and presentations;
? Incorpo machine learning, statistical modeling, visualization and analytics  and data sources into the development of processes and techniques to use data effectively and efficiently.
? Conduct exploratory data mining and analysis to understand customer engagement and retention
? Develop predictive scoring models including linear/logistic regressions, decision trees, neural net, clustering, etc. preferably in SAS Enterprise Miner
? Develop and collabo to build artificial intelligence /machine learning-based next best action recommendation systems
? Ability to leverage machine learning, clustering, natural language processing techniques to mine through unstructured data to uncover insights, topic detection, keyword extraction, and sentiment analysis using R and/or Python
? Ability to summarize and present findings in clean, easy to understand data visualizations and presentations 
? Excellent communication and presentation  (written and verbal)
? Independent, self-motivated, organized, able to multi-task in time-sensitive environments, and skilled in facilitation and collaboration
? Experience with Tableau
? Strong data querying  using SQL, hands-on data analysis experience using SAS and Python or R
? Strong predictive data modeling experience is  with proven application in ing Decision Trees, Regression analysis, and other data mining techniques preferably in SAS Enterprise Miner
 
:
? Bachelor?s Degree or higher in a quantitative field such as Financial Mathematics, Business Analytics, Financial Engineering, Statistics, Economics, or Data Science 
? Financial Services experience 
? Expertise ing with large data sets, data mining, and machine learning 
? Experience in various artificial intelligence applications",Data Scientist,"Pyramid Consulting, Inc
3060 Kimball Bridge Rd. Suite 200
Alpharetta, GA
Email: Vinay.Dubey@pyramidci.com; Desk: (415) 943-9169
Web: www.pyramidci.com",outlook,job recruiter
"Research Computing SME - 19-03563
 

    : Somerville, MA
     Type: Contract
    : 5+ Months


 
General Summary/Overview Statement

    Our client HealthCare is embarking on a new Enterprise Data and Digital Health (EDDH) initiative focused on establishing best-in-class data analytics, digital competencies and  to deliver superior care and experience for our patients.
    The organization is dedicated to creating a cutting-edge data science environment that supports patient discovery, cohort formation and patient disease stratification.
    The Research Information Science & Computing (RISC) group of *** HealthCare plays a critical  in ensuring the success of this initiative. Our focus is on accelerating the digital transformation of healthcare by redefining how data is used to improve patient outcomes.
    To better enable researchers and clinicians, we are building , repositories, and new data flows optimized for data science, machine learning and artificial intelligence.
    Our group is looking for an experienced, high performing, hands-on Big Data/Cloud Engineer to be a key member of this fast-paced team within the Big Data Commons project, as well as participating in other project activities, including migrating different functions to a cloud-based system.
    This  will be responsible for creating, modifying and expanding our data analytics to include Big Data and Cloud . You will help define and build the next iterations of features for enterprise-wide data integration and data science.
    The ideal candi should have prior experience in the implementation of a modern Big Data architecture and the design and implementation of analytical data platforms. S/he should have deep technical and analytical  with a rich knowledge of accepted best practices around data flow, data transformation and data ingestion.
    To quickly get up to speed, a solid understanding of metadata catalogs, data governance, data science and data mining in Big Data  in Cloud, hybrid and/or on-premise environments is essential.

 
Principal Duties And 

    Under the guidance of Sr Manager  with cross functional technical and analytical teams to understand current and future enterprise-wide Big Data analytics goals spanning dispa platforms
    Participate hands-on in the  to design, develop and deploy advanced systems for the collection, aggregation and analysis of those data in alignment with business s
    Build serverless data ingestion and refresh pipelines in terabyte scale using MS Azure cloud services
     closely with the Big Data Architect to design nets, systems, and storage environment that effectively reflect business needs, security , and service level 
    Help with Big Data and Cloud based  assessments, stgies, and roadmaps in several technical domains and act as a subject matter expert on Big Data
    Collabo with data engineers to drive and build innovative solutions, defining best practices and methodologies
    Participate in configuring the architecture and advise on solutions configured for efficient performance
    Assist data scientists, SMEs, Data Engineers and Big Data Cloud Architects in deploying and testing AI and machine learning algorithms
    Participate in cross-team code re among data engineering personnel
    Stay informed on initiatives across the industry and the enterprise to help leadership effectively prioritize current and future Big Data needs
    Exhibit willingness and participate actively in agile practices by collaborating with the Senior Manager and team members
    Provide adequate documentation to effectively communicate architectural designs and data inputs, outputs and flows for technical and nontechnical audiences
    s closely with the RISC leadership, RISC Technical, other RISC managers and IT colleagues to support corpo/functional business and information needs
    Help to establish processes to ensure HIPPA and institutional compliance in all aspects of the 

 


    Bachelor?s/Master?s degree in computer science or a healthcare related informatics field
    5+ years of building and maintaining sophisticated enterprise software systems and/or large analytical platforms through multiple release cycles and upgrades
    Experience in a wide variety of traditional databases and Big Data , developing stgies for data flow, data obfuscation, archiving and other aspects of data warehouses or data lakes
    3+ years of experience ing in a public and/or private cloud environments such as Azure, AWS, or Google Cloud with knowledge of how to integ with processing and messaging stacks such as Databricks, Spark, Azure EventHub, and/or Kafka.
    Knowledge of Big Data & Data Engineering, with deep, hands-on technical experience in , libraries, and frames using traditional database enterprise-class RDBMS , preferably MS SQL Server, and Big Data NoSQL like , such as CosmosDB, Dynamo, ArangoDB, HDFS and Hive, and/or Hbase. Experience includes creating data integration systems spanning a variety of modern architectural paradigms, including event based, streaming, and near real time design patterns
     experience ing with data/Big Data in HealthCare, including genomics, imaging and HER
    Successful track record of delivering results on scope, on time, on budget
    Knowledge of HIPPA, privacy regulations regarding patient data security and/or research related practices is a plus

 
/Abilities/Competencies 

    Exceptional problem-solving 
    Excellent oral and written communication 
    Able to  in one or more cloud computing environments
    Able to  in a highly-collaborative team and provide valuable insight
    Able to document  and complex data architectures
    Able to inform the project management process with thoughtful and accu timeframe estimates
    Able  efficiently under pressure and to manage to tight deadlines or shifting priorities
    Enjoy being challenged and solving complex problems
    Self-motivated, independent and possesses the ability to learn quickly
    Familiarity with medical terminology and healthcare concepts is a significant plus
",Data Scientist,"Global Technical Talent, Inc. - All current GTT Openings
233 Vaughan Street, Suite 102
Portsmouth, NH 03801
Ph: (603) 516-4437 / Fax: 800-775-3135
hkhan@gttit.com / www.gttit.com",outlook,job recruiter
" Ref.                         : 19-62193
                  : Software Engineer - Senior (CPE)
              : Los Angeles, CA
Contract         : 6 Months
Hire Type                     : W2 only
 
Additional  Details:
Our group develops software for the PlayStation Net. This net provides online services for the PS4, Web, PS3, PSP, Vita, Tablets, mobile phones, PC, Bravia TVs and more. We build large internal enterprise applications which integ and support services such as Movie/TV Show streaming, PSN gaming commerce and much more. Our environment is fast, agile and we use some of the latest  out there. The Content and Customer Service Data Engineering team is looking for creative, curious, energetic professionals who are passionate about solving complex problems with data in creative and innovative ways. Our mission is to deliver timely, scalable and high quality data solutions and be a force multiplier by unlocking the full potential behind our data.
 
: Collabo with product and engineering teams in multiple  building forward thinking, innovative data solutions that up-level our features and get results in a data driven way. Build highly scalable resilient data pipelines and models which produce high quality datasets. Deliver visualizations that distill clear, actionable insights from large, complex datasets. Improve our tooling by building generic data features such as data quality and anomaly detection and drive overall improvements in our data infrastructure. Drive ideas and  through deep understanding of our data and how it applies in the broader sense of the organization.
 
:

    4+ years of industry experience building highly scalable data pipelines (batch and/or streaming) utilizing Spark, Hive, Presto or other open source frames architecture.
    Python and shell scripting experience for automation and data manipulation.
    Prior experience utilizing dashboarding  such as Domo, Tableau, Superset or similar.
    Experience translating ambiguous business needs to highly scalable data models and datasets.
    Background in software engineering - able to write elegant, scalable and maintainable code.
    Strong SQL  Desired:

    Experience with Airflow, Superset or similar open source .
    Previous experience with AWS or similar cloud environment.
    Experience preparing large, complex datasets for Machine Learning pipelines a plus.
    Prior experience ing in an Agile environment.
    Gamer or experience in the gaming industry a plus.",Software Engineer,"shivashankar@eteaminc.com
(732) 210-9068
1001 Durham Avenue Suite 201
South Plainfield, NJ, 07080
linkedin.com/in/shivashankar-gs-ab8181132
www.eteaminc.com",outlook,job recruiter
" : Kinesis Developer
: Mechanicsburg, PA
: 9+ Months
 
 :
?             Kinesis streams, Kinesis data analytics, Kinesis Firehose and Java/J2EE
 
 :
?             Experienced Developer with hands on knowledge, AWS Kinesis, Streams, Kinesis data analytics, Kinesis Firehose, Lambda, Apache Flink, Java/J2EE, AWS Cloud Formation Script, DevOps and AWS Cloud
 
:
?             Design and developer event streaming solution using Kinesis Data Streams
?             Analyze application  and configure Kinesis data streams for producers/consumers
?             Modify legacy Java application to send data streams to Kinesis
?             Write Kinesis data analytics SQL for data transformation and aggregation
?             Create s using Apache Flink for data processing
?             Create Lambda function using Java to persist stream data in MySQL RDS database
?             Create program to retry failed events
?             Perform data stream archival for replay
?             Create step by step guide for provisioning AWS resources
?             Create documentation on streaming / aggregation process WS platform",Developer,"Technical Recruiter
eTeam Inc.
atkumar@eteaminc.com
(732) 338-2599",outlook,job recruiter
" : Sr. Consultant/ Manager Analytics
 : Dodgeville, WI
: Full Time
 
  :-
 
s & 
 

    ed as consulting lead for data science ? also acting as a conduit between data scientists and the business.
    Should be business oriented but should be able to translate business problems for data scientist team.
     on the latest applications of data science to solve business problems
     directly with client stakeholders to translate business problems into high level analytics solution designs
    Present analytic solutions to business audiences highlighting robustness of the solution and how it could help gene business value
    Responsible for managing analytics , collaborating with client stakeholders.
    Participate in discussions with team members to select and  relevant analytic techniques and create actionable business insights
    Responsible for making presentations to senior management, communicating results to business teams, and develop plans to help operationalize analytic solutions

 

 

    6+ years of professional  experience with at least 5 years in data analytics consulting
    Excellent knowledge of consulting on data science , common data sources, and business problems that could be addressed through analytics is 
    Ability to translate business problems to high level analytics solution approach
    Knowledge of statistical and machine learning algorithms
    Strong project management and team management  and ability to  with global teams
    Strong SQL  and hands-on experience with analytic  like R & Python & visualization  like Qlik or Tableau
    Ability to engage with executive/VP level stakeholders from client?s team
    Graduate in Business Analytics or MBA with equivalent  experience",Data Analyst,"Pyramid Consulting, Inc
3060 Kimball Bridge Rd. Suite 200
Alpharetta, GA
Email: Shivani.Saini@pyramidci.com; Desk: (770) 255-7641 Ext.6334 or Cell: 
Web: www.pyramidci.com",outlook,job recruiter
"
  and Key : Data Engineer, Python, SQL and Unix/Shell scripting
 
My name is Bill Stevens and I have a new six month contract to hire Data Engineer  located in either Midtown, Manhattan, Holmdel, New Jersey or Bethlehem, Pennsylvania that could be of interest to you, please review my specification below and I am available at any time to speak with you so please feel free to call me. The ideal candi will have the ability  from home 3-4 days per week, however he / she but must be within a reasonable distance to travel to one of the office s as needed for meetings. All potential candis will be  to take an SQL assessment quiz.  
 
Please let me know if Midtown, Manhattan, Holmdel, New Jersey or Bethlehem, Pennsylvania could be an option for you.
 
This  pays $70.00 per hour on a w-2 hourly basis or $77.00 per hour on a corp basis. The corp  is for independent contractors and not third party firms.
 
 Summary:
This  will be responsible for implementing and maintaining the enterprise wide data management solutions for certified analytics and reporting rectangles and data assets. This includes the analysis, design, development, testing, implementation, and initial maintenance of the solution and acting as interface with SME/Product owners and enterprise data analytics team, by taking a holistic approach for delivering reusable and data assets enabling data solutions.
 
:
Lead the design, build and execution of post source system extraction and data lake ingestion and business transformation, creation frame and development processes in production
Enhance analytic environments and platform  for structured, semi-structured and unstructured data
Develop data quality metrics that identify gaps and ensure compliance with Enterprise wide standards
Provide technical guidance for  and team members, along with SMEs and product owners
Build data pipelines to feed descriptive and predictive analytics use cases, KPI and enterprise wide reporting
Interface with architects, product managers/SMEs and product analysts to understand data needs and implement the business rules into transformation
Document the data blending process along with the specifications and flow/data lineage and automate the end-to-end process
Maintain/add to existing data dictionaries and  with UI team for creating the profiling platform and data profiling for already created CARRs
Lead the execution of project-based governance processes
 
Competencies//Knowledge:
BS or MS in Management Information Systems, Computer Science or a related field
Four to six years of data management experience
Proficient programming capability in Python, SQL/NoSQL and Unix/Shell scripting, as well as in data engineering, databases (e.g., SQL, MongoDB), platform architecture and ETL concepts
Amazon Web Services experience
Excellent communication and collaboration  to  across multiple groups within the organization
Experience providing technical leadership and mentoring other engineers for best practices on data engineering
Expert knowledge of software engineering best practices across the development lifecycle, including agile methodologies, coding standards, code re, source management, build processes, testing, and operation",Data Engineer,"PRI Technology
Denville, New Jersey  07834
Direct Line: 1-973-732-5454 x21
Bill.Stevens@PRITechnology.com 
http://www.pritechnology.com/  ",outlook,job recruiter
"
: Data  Engineer 

: Plano TX

Terms: Long Term Contract

 Details:

    Very strong data modeling 
    Analytical and problem-solving  & experience, applied to Big Data domain is a MUST
    1+ years of hands-on experience with SQL, Hadoop, Hive, Pig, Impala, and Spark
    Hive is a mandatory skill
    5-8 years of Python or Scala programming or Java/J2EE development experience
    3+ years of demonstd technical proficiency with Hadoop and big data ",Data Engineer,"
Title: Data  Engineer 

: Plano TX

Terms: Long Term Contract

Job Details:

    Very strong data modeling skills
    Analytical and problem-solving skills & experience, applied to Big Data domain is a MUST
    1+ years of hands-on experience with SQL, Hadoop, Hive, Pig, Impala, and Spark
    Hive is a mandatory skill
    5-8 years of Python or Scala programming or Java/J2EE development experience
    3+ years of demonstd technical proficiency with Hadoop and big data projects",outlook,job recruiter
" : Cloud Consultant
: Glen Allen, VA
: 5 months

Mandatory  :
Experience in Azure SQL, VNet, Redis Cache, Azure Diagnostics, OMS, Traffic Manager, CDN, Azure Notification Hubs, Azure Identity and Access Management Architect
In depth experience in integration of SSO / IAM with Azure.
Extensive use and experience in OpenID, OAuth2.0, SAML, MFA or SSO.
Resource should have experience in integrating application with MS Azure and ADFS
Experience with AD connect and Application proxy
Experience in creating session sharing mechanism between MS Azure/ADFS and CA SiteMinder.
Extensive experience in integrating and sharing sessions from CA SiteMinder to help client sunset CA Site Minder

Secondary  :
Certification in Microsoft Azure solutions",Data Scientist,"eTeam Inc.
ravindrak@eteaminc.com
(732) 338-2701",outlook,job recruiter
": Data Analytics Consultant
: Basking Ridge, NJ 
: 6 to 30 months Contract 
           
 :
 Immediate need for a talented Software Engineer with experience in the Telecom Industry.
This is a 07+ Months Contract opportunity with long-term potential and is located in Basking Ridge, NJ. Please review the   below.

Key :

    Build data analytics visualization app.
    This  will be responsible for design and implementation of big data analysis application.
    Data integration from various data sources.
    Build web service API to support data analytics algorithms.
    Perform data mining to study wireless net and device performance and quantify device quality.
    The candi should have a solid background in data analysis, wireless, and computer science.

Key  and  Experience:

    Master or PhD Degree in Electrical Engineering, Computer Science, Software Engineering, Computer Engineering, Information System etc.
    6+ years? hands-on experience designing and implementing data applications in production using Java/ Python/ R and etc on big data platform.
    6+ years? hands-on experience with related/complementary open source software platforms and languages such as Java, Linux, Apache, Open Street Map, D3.js and etc.
    Must have proven records of papers published in wireless analytics space in IEEE or other effective journals.
    Must be proficient in the use of different databases such as Spark, Hadoop, Hive, MySQL, TeraData, MS SQL Server, Oracle and etc.
    Must be proficient in the programming languages of Java, Java Scripts, Python, R, HTML and etc.
    Data extraction/transformation/loading, data mining, and statistical modeling experience .
    Strong analytical capabilities, creativity and critical thinking .
    Good mobile telecommunications industry knowledge, including experience with handset manufacturers, net equipment vendors, and/or chipset vendors.",Data Analyst,"Pyramid Consulting, Inc
3060 Kimball Bridge Rd. Suite 200
Alpharetta, GA
Email: Jagadishwar.N@pyramidci.com; Desk: (732) 313-0314 
Web: www.pyramidci.com",outlook,job recruiter
": Data Engineer
:  Wilmington DE

ing Experience Software/;
? Experience with big data : Hadoop, Apache Spark, Kafka, etc.
? Experience with relational SQL, Snowflake and NoSQL databases, including Postgres and Cassandra.
? Experience with data pipeline and flow management : NiFi, Kylo, Luigi, Airflow, Azkaban etc.
? Experience with AWS cloud services: S3, EC2, EMR, RDS, Redshift ? Experience with stream-processing systems: Storm, Spark-Streaming, etc.
? Experience with object-oriented/object function scripting languages: Python, Java, C++, PySpark, Scala, etc.

 for Data Engineer:
? Create and maintain optimal data pipeline architecture,
? Assemble large, complex data sets that meet functional / non-functional business .
? Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
? Build the infrastructure  for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and AWS ?big data? .
? Build analytics  that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics.
?  with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs.
? Keep our data sepad and secure across national boundaries through multiple data centers and AWS regions.
? Create data  for analytics and data scientist team members that assist them in building and optimizing our product into an innovative industry leader.
?  with data and analytics experts to strive for greater functionality in our data systems.",Data Engineer,"Pyramid Consulting, Inc
3060 Kimball Bridge Rd. Suite 200
Alpharetta, GA
Email: Parul.Tyagi@pyramidci.com; Desk: (770) 255-7719 Ext.6412 or Cell: 
Web: www.pyramidci.com",outlook,job recruiter
": BI Analyst
 
: Fremont CA

Terms: Contract

 Details: 
 
 :

    Strong ability in translating business  and needs into analytic solutions, within multiple areas in IT and with various stakeholders, including key leaders and managers.
    Leverage data to understand in-depth IT business processes, identify areas of opportunity for process improvement.
    Write queries, analyze, visualize, and provide analytics on data to build reporting solutions to support various company initiatives. e.g., build rich and dynamic dashboards using Tableau.
    Develop a deep understanding of analytical data models.
    Support project development life cycles through data modeling, reporting, and analytics.
    Participate in the on-going development of the business intelligence and data warehousing functions within the wider organization.
    Create training materials to guide business users on how to use dashboards.
    Participate in the creation and support of development standards and best practices.
    Explore and recommend emerging  and techniques to support/enhance BI landscape components.
    Automate solutions where appropriate.

:

    At least 2-5 years of business intelligence and data warehouse experience.
    At least 2-year experience with ANSI SQL/ Presto / Hive/ MySQL.
    At least one year of experience with Tableau.
    Prefer a candi with scripting experience (Python/R/Javascript/PHP/Perl/Ruby/etc.).
    Prefer a candi with experience building and maintaining pipelines.
    Knowledge of ETL processes and designs.",Data Analyst,"CYNET SYSTEMS Inc. 
FARHAN.S@CYNETSYSTEMS.COM
Ph:(571) 645-5973  |  Fx:866.838.0907  | WWW.CYNETSYSTEMS.COM
IT & ENGINEERING CONSULTING",outlook,job recruiter
" : Junior Python Developer
 
My name is Bill Stevens and I have a new twelve month plus Junior Python Developer  located in either Dayton, New Jersey or Orangeburg, New York that could be of interest to you, please review my specification below and I am available at any time to speak with you so please feel free to call me.  
 
Please let me know if Dayton, New Jersey or Orangeburg, New York could be an option for you.
 
Due to client confidentiality please call me for the w-2 and contract s. The s that are being offered are lucrative and are above fair market value.
 
The firms Data Retention Team manages a complex Data backup, retention, and recovery system within their world class Data Center environments. The team?s mission is to ensure stability within Engineering by providing Data Retention, Recovery, and Archiving services for the firms production systems.
 
:
Automate operational processes.
Build infrastructure as code and automate away the toil.
Be involved in transforming the legacy infrastructure and processes into cutting edge self-service as well as providing dynamic transparency.
Help ensure daily backup activities are reliable throughout our enterprise infrastructure.
Assist in designing, developing, implementing and documenting  to provide services to clients which will interface with the firms data retention operations
Help build out a RESTful API using Swagger
Understand and interact with RDBMS and NoSQL databases
 
:
Someone who has experience in automating repeatable processes.
Have a proven track record of successfully integrating processes by building Infrastructure as code.
ing with infrastructure with an eye on streamlining operations.
The ideal candi should be well versed with Python and who understands GitHub and CI/CD  such as Jenkins.
Experience with programming - Python 
Knowledge of GitHub, APIs, and SDLC concepts
Knowledge of UNIX, Linux (all flavors), Windows
Knowledge of HAProxy, gunicorn, and Flask. (Or equivalents)
ing knowledge of JIRA concepts and processing
ing knowledge of monitoring and reporting 
Knowledge of Veritas NetBackup software administration (NetBackup 8.x or higher)
ed with large scale tape hardware management as well as net storage (Netapp and Isilon)
The ideal candi should possess the ability to  independently and in a team environment under multiple deadlines
The ideal candi should possess the ability to quickly pick up new  and learn new software quickly
 
Education Qualification:
BS in Computer Science/Information  or equivalent experience
 
US citizens and those authorized to  in the US are encouraged to , sponsorship for this  is not available at this time.
 
The interview process will include an initial telephone screening.
 ",Developer,"PRI Technology
Denville, New Jersey  07834
Direct Line: 1-973-732-5454 x21
Bill.Stevens@PRITechnology.com
http://www.pritechnology.com/  
 ",outlook,job recruiter
" : Data Scientist

 : Philadelphia , PA

Experience : 7-10 Years

Interview: Telephonic & Skype

 Type: Full-time

 

 :

    Data Scientist will be responsible for preprocessing of structured and unstructured data.
    Analyze large amounts of information to discover trends and patterns.
    Build predictive models and machine-learning algorithms.
    Propose solutions and stgies to business challenges

 

 :

    Experience using Statistical and Machine Learning algorithms on real data, in commercial environments. Experience in the Telecom Domain is a big plus.  
    Excellent and wide ranging experience in supervised and unsupervised learning. Reinforcement learning a plus.
    Experienced in the use and design of logistic regression, support vector machines, ensemble trees, and neural nets. Optimization problems a plus
    Familiarity and experience with the standard machine learning packages, such as numpy, scipy scikit-learn, TensorFlow, keras and Theano
    Experience in data munging, data cleansing etc.
    Experience in feature selection, feature engineering and development of recommender systems.
    Very Good Knowledge on one and more Statistical Tool like R/Python
    Excellent communication and interpersonal , with proven ability to take initiative and build strong, productive relationships.
    Good to have experience in Net Analytics. Building a strong intuitive understanding of the problem domain (Next Generation Access Nets).
    Influence and transform the end-to-end delivery process to maximize the value Customers gain from Analytics
    Should be able to handle multiple  and liaison with customer different teams to bring overall value add.
    Should have strong people  and good team management experience.
    Identify testable hypotheses to explain interesting phenomena in this domain
    Constructing an automated system test frame
    Develop and communicate goals, stgies, tactics, project plans, timelines, and key performance metrics to reach goals
    Experience with public cloud (AWS) 
    Great communication 
    Proficiency in using query languages such as SQL
    Good scripting and programming , such as R, Python, or Spark",Data Scientist,"United Software Group Inc.

565 Metro Place South. Suite # 110

Dublin, OH 43017

Desk: +1- 614-495-9222 EXT: 477

Email: Sahaj.a@usgrpinc.com | Hangouts: Sahajtek01@gmail.com",yahoo,job recruiter
": Big Data Lead

: Raritan- NJ

: -12 months

 

 & :

        Bachelor's degree in Computer Science, Software Engineering, Information , or related field 
        10-12 years of overall experience in architecting and building large scale, distributed big data solutions.
        Experience in at least 2-3 Big Data implementation .
        Solid experience in Hadoop Ecosystem development including HDFS, Hive, Spark,  Sqoop, Kafka, NiFi, and real time streaming  and host of big data open source stack.
        ing experience in Cloudera distribution is .
        Must possess excellent communication .
        Strong analytical, technical and trouble-shooting .
        Experience leading teams and/or managing loads for team members.
        Nice to have ing experience in Informatica BDM, StreamSets.",Data Scientist,"Ankit Panchal  | Zodiac-Solutions

880 E Swedesford Road, Suite #210, Wayne, PA 19087

(Desk):484-252-2804; (Fax):866.655.7805

ankit@zodiac-solutions.us | www.zodiac-solutions.com |",yahoo,job recruiter
"  - Azure .Net Developer
 - N/A, Georgia
 : 
 : Azure .Net Developer
: Atlanta, GA
 
? :
 
:

    Experience developing applications using Azure, .NET Core and C#
    Docker, Cosmos DB, Angular and Azure Data Factory
    Expert in ing with XSD and JSON
    Intermediate knowledge of Angular and Entity Frame
    Intermediate knowledge of SQL Server
    Experience using Agile development methodology
    Analyzes, designs, codes, tests, corrects, and documents mode to highly complex programs to ensure optimal performance and compliance
    Strong team player, excellent communication  both verbal and written
    Project background:
    Azure Data Factory pulls incoming data from Integration Point
    Azure Blob storage for Global Content data storage and archive
    Azure Kubernetes Service to host containerized Global Content services
    Azure API Management to expose Global Content services to BSIS and UI
    Azure App Services to host UI
    Azure Cosmos DB to storage Global Content data
    Azure Key Vault  
",Developer,"Bhargav RNR
(469) 229-7499
bhargav@rnritsolutions.com",yahoo,job recruiter
"s: Hadoop/Spark Developer

: Mooresville, NC

Contract 

 

Desired Experience :

Hadoop / Spark developer

Min 3 years Scala (or Python), Spark

Expertise in Hive/ HBase, Scoop, Oozie

Exposure to Cloud would be nice to have

 

Primary : Spark Hive

Secondary HBase Oozie

 



Build Data pipeline

Building data source from source to target

As source changes ability to adapt

Build Production process to be completely automated and self-governed

? Education:
? Bachelor's Degree in computer science, Engineering or related field",Developer,"Ravinder Singh
ravinder@amigainformatics.com",yahoo,job recruiter
" : Clinical Data Analyst ll
: 7+ months Contract
:  La Jolla, CA
 
The Scientist in this  will perform multivariate analysis of biomarker data in connection with Company Neuroscience clinical trials with the goals of providing  for patient screening, risk assessment, diagnosis and staging, and for developing biomarker-based patient subgroups. The scientist will gene hypotheses and vali those using independent data sets.  also include interfacing with CROs and data management teams to QC biomarker data and transfer them to clinical trial databases.

The successful candi will  with internal collaborators, and CROs to develop and vali biomarker signatures. S/he will interface internally with Project Scientists and Laboratory Personnel and interact outside of the immediate group with data management and scientists in Quantitative Sciences, Experimental Medicine and Clinical Development.

:
? A bachelor's degree in statistics/bioinformatics/biomedical engineering/genetics and genomics or related fields is .
? Hands-on experience with data processing and analysis, statistical inference techniques, multivariate analysis and/or machine learning is .
? A publication record, including first author publications in peer-reviewed journals, is a plus.
? Good communication  are .
 ",Data Analyst,"Avnish Pandey ? Talent Acquisition 
apandey@UFCTechnology.com
Desk: 908-838-7649
UFC Technology -- Schaumburg, IL | Chicago, IL | Frisco, TX | Hillsborough, NJ |
www.UFCTechnology.com
 ",yahoo,job recruiter
" : -Lead BI

 :- Cheektowaga, NY
: -Full Time
 
 :-
Lead Business Intelligence Solutions

This is a lead  on the BI team to build solutions for data analytics and reporting for the future.
In this  you will function as the technical lead to design and implement innovative BI solutions. Define functional  partnering with diverse business teams including Finance, Supply Chain, HR, CRM, as well as IT peers. Responsible for designs, building, and implementing solutions utilizing appropriate client BI . Requires the ability to mode data coming in from different data sources into BI , including existing data sources and new data sources. Advanced knowledge in designing and delivering BI visualizations, dashboards, and reports to support standardized and augment ad hoc reporting needs. Responsible for seamless integration of reporting  in the landscape existing and in transition. Development of full technical documentation. Participation in IT standard project management office processes. Support test case scenario's and partner with UAT teams. Participate in change control processes, and lead efforts around training of BI solutions to consumers.
 

    Experience with Power BI, Power BI Desktop, BI Service
    Strong SQL 
    Experience with Power Apps
    Experience with Microsoft Azure
    Experience with Oracle / SQL Databases
    Experience with DAX expressions, power query
    Experience with managing 
    Knowledge of SSAS, SSRS
    Knowledge of Oracle Business Intelligence platform and data warehousing concepts

Non- needed
Requires effective communications with strong business acumen and strong . Ability to effectively interact with team members and senior level management. Strong  ethic with attention to detail. Excellent problem-solving . Creative individual with a track record of building and implementing innovative technical solutions.


Basic Education Requirement - Bachelor's Degree or equivalent experience
Basic Functional Experience - 5 years of experience as a senior software developer, applications architect or applications systems analyst programmer including 3 years project management
",Data Scientist,"E TalentNet
http://etalentnet.com
8251 Greensboro Drive Suite 250
McLeanVA
jeremyw@etalentnet.com
(703) 261-7028 Ext.242",yahoo,job recruiter
"                        : Python Architect

                      :  Greenville - South Carolina

                      :  12+Months   

   :

. Minimum  experience: 5 - 8 YEARS. As an Architect you are responsible for providing technical leadership to small size/complexity/order-value .

. You are expected have depth of knowledge of specified technological area, which includes knowledge of applicable processes, methodologies, standards, products and frames.
. You would be responsible for defining and documenting architecture, capturing and documenting non-functional (architectural) , preparing estimates and defining technical solutions to proposals (RFPs).

. you should provide technical leadership to project team to perform design to deployment related activities, provide guidance, perform re, prevent and resolve technical issues. 

  :

. Apache Spark, Python Scripting,",Data Engineer,"Agile Enterprise Solutions Inc.
P: 972-440-2128
Email: james_mill@aesinc.us.com   || www.aesinc.us.com ",yahoo,job recruiter
"Hive Big Data Developer

Phoenix, AZ

12+ Months Contract

Phone + Skype

Visa: Any

 :

Hive Big Data

Hive Big Data Developer has many . And the   are dependent on your domain/sector, where some of them would be applicable and some might not. The following are the tasks a Hive Big Data Developer is responsible for:

    Hive development and implementation.
    Loading from dispa data sets.
    Pre-processing using Hive.
    Designing, building, installing, configuring and supporting Big Data.
    Translate complex functional and technical  into detailed design.
    Perform analysis of vast data stores and uncover insights.
    Maintain security and data privacy.
    Create scalable and high-performance web services for data tracking.
    High-speed querying.
    Managing and deploying Hive.",Developer,"Insigma, Inc

24805 Pinebrook Rd,

Suite 315, Chantilly, VA 20152

Phone: (Desk): 703-344-9674 ext 173

Email: sanjay.kumar@us.insigmainc.com I Fax: 888.428.2340

 ",yahoo,job recruiter
"Data Scientist

:  Quincy (Boston) or Salisbury (NC)

 : 06-12 months (Possible extension)

 

 :

 

Exp. 7+

 

Programming  complete with Machine Learning using the latest open source  and languages

Statistics and Mathematical background

Expert in Data Wrangling, Data Visualization & Communication",Data Scientist,"Technical Recruiter

Brain Bee LLC

215-809-1617  /717-473-9932

tejpal.yadav@brainbeeus.com",yahoo,job recruiter
": EDI Tester

: Mountain View, CA

Contract: 6+ Months

Onsite

 

 :

    Excellent functional knowledge in Oracle Apps/ Oracle ERP testing experience such as O2C, Order Management, Inventory Management, Shipping, and Accounts Receivables.

    EDI testing and B2B integration testing experience

    Experience in testing web applications

    Experience in writing simple to medium complexity PL SQL queries

    Extensive experience in EDI domain, understand implementation guides and if  create Implementation guide for given EDI message based on business requirement.

    Excellent knowledge of Retail/ Manufacturing EDI transactions like 850, 860, 867, 852, 830, 856, 810, 947, 846, 940 and 945 etc.

    Test case design, Test case execution, test data creation, defect management and reporting

    Streamline test cases - identify reusable test cases for different test scenarios.

    Create & maintain Application overview document (AOD) document.",Data Engineer,"Yochana IT Solutions Inc.

23000 Commerce Dr, Farmington hills, MI-48335,

Contact: 248-537-0766 (D)

prathyusha@yochana.com  || www.yochana.com",yahoo,job recruiter
": Marketing Analyst/ Data Scientist
: Sunnyvale, CA
: long term contract
 
 Summary:
 
The ideal candi is a self-starter with website or marketing analytics experience and a wide range of . The ideal candi is highly proficient in turning data discoveries into analytical insights. This is a stgic partnership  and will report into the customer Retail Data Sciences organization. This  will involve development and management of key data science goals tied to customer engagement and business outcomes. In this , you will be ing closely with Marcom producers and management defining, planning, prioritizing and delivering the online and business analytics  for all customer interactive .
 
Key :
You are a strong communicator with excellent verbal and written presentation 
You are a seasoned data analyst with 6+ years of experience measuring impact of online site changes.
You have strong application knowledge and experience with website measurement  like Adobe Analytics
You have strong application knowledge of Data Mining techniques
Proficient with Tableau or other Data Visualization  is 
Proficient developing effective presentations in Keynote or Microsoft PowerPoint.
ing knowledge of SQL (Teradata especially) is a plus.
ing knowledge of Python is a plus
 

 
To be successful in this , you should
 
Be a driven, accountable and a highly spirited team player.
 
Be able to deliver high quality  within deadlines.
 
Education
 
Bachelor degree  in - Applied Social Sciences, Business Administration,
 
Marketing, Finance, Operations Research, or related fields",Data Analyst,"Trinity Global Tech Inc.
Email ID: ashish.singh@tgtus.com
Direct: +1-614-600-1955(USA)
Personal number :- +1-614-701-9422",yahoo,job recruiter
" : Data Scientist  
: Richardson, TX
 : Full Time
 
                                      
Data Scientist (3)            
As a data scientist on the SOI team, you will primarily develop machine learning solutions that will predict a customer?s needs/intentions in real-time at the moment of interaction, in batch prior to the event, or in batch after the event for analytical labeling/categorization. The outputs of these models will then be used to determine the optimal engagement and experience to provide to the customer.
 Duties - You will partner with functional and channel stakeholders to define use cases and model targets. You will  with domain experts to understand data sources and complete feature engineering. You will design and execute experiments with training data sets to determine the best models to  and perform cross validation to test the efficiency of the resulting algorithms. When algorithms reach adequate efficiency thresholds, you will partner with production engineers to deploy your models for run-time scoring. You will compare the live performance of your models to expectations and evaluate any gaps.
In addition to model development, you may, independently or within a team, need to build solutions to support the overall project, including label generation, data schemas, ad hoc analytics, output structure/packaging, or end-to-end prototype demos. You will be responsible for presenting your own  to team members, peers, and leadership, and will build any visualizations or presentations necessary for those communications.      -
Must Have 
Proficiency in Python, SQL, and Linux Shell/equivalent, and familiarity with HDFS.
- Experience leading the end-to-end design, development, and deployment of predictive modeling solutions.
- Experience and knowledge of NLP including NER and text summarization.
- Experience and knowledge of deep neural nets, including CNN and RNN.
- Experience with text parsing/regular expressions.         -
Desired  -Six or more years of relevant  experience.
- Deep expertise in ing machine learning to solve a class of AI problems such as NLU/NLP, Reinforcement Learning, Voice Biometrics, Text Mining, and Intelligent Process Automation.
- Experience with using big data to develop models.
- Experience with Spark.
- Strong problem solving, analytical, and research capabilities.
- Strong verbal and written communication .
- Experience presenting to and influencing functional leaders and stakeholders.               
 
Education -Masters in Statistics, Math, Economics, Engineering, Computer Science, Business Analytics, Data Science, or a related field.",Data Scientist,"Nityo Infotech
Plainsboro, NJ
(609) 853-0818 Ext.2312
atul.k@nityo.com",yahoo,job recruiter
": Hadoop Developer

: SFO, CA / Bentonville, AR

: Long Term Contract

 

 :

? Troubleshoot and triage issues related to data pipeline failures or slowness, built using Map Reduce, hive or Spark to ensure SLA adherence. These may be batch as well as streaming processes.

? Resolve issues related to several data platform components and open source database solutions like Presto/Druid or cloud native components

? Resolve issues related to commercial  and products, which are an integral part of platform

? Be a gate keeper to ensure sanity of production systems

? Builds  to continuously monitor and alert platform components & data pipelines

? Continually improve CI/CD , processes and procedures

? Participate in ongoing design, implementation, and maintenance of systems and  across our data platform

? Write and maintain infrastructure documentation

 ",Developer,"Insigma Inc. | 43490 Yukon Dr, Suite 102  

Ashburn, VA, 20147 

Phone: (Desk) 703-344-9674 ext 126

Email: ankur.dixit@us.insigmainc.comI Fax: 888.428.2340",yahoo,job recruiter
": Business Analyst with Tableau
: Austin, TX
 
 :

    5 Years? experience ing with SQL.
    4 Years? Development experience ing with Tableau Reports and Dashboards(Up to two years using Version 10.x)
    4 Years? strong experience developing visualizations with Tableau and deploying those visualizations to Tableau Server.
    4 Years? Technical and business analyst experience.
    4 Years? experience with reporting development from large, complex systems.
     :
    Strong Experience with Federal, State, or Local government data reporting; with strong understanding of state government.

",Data Analyst,"RNR IT Solutions Inc
Contact NO.3154172219
anil@rnrits.com",yahoo,job recruiter
"                 :    Big Data/Hadoop Developer

        :     Redmond, WA

Contract 

 

Mandatory :-

? Leverage data and business principles to solve large scale web, mobile and data infrastructure problems.

? Onsite customer interaction and co-ordinate with onsite and offshore data scientists.

? Organize/coordinate weekly progress review meetings with project team members.

? Extensive experience in project management & delivery.

? Industry experience in Data analytics/BI, Data modelling and visualization, Optimization and statistics.

? 5+ years of Scripting with one of these languages (PHP, R, PYTHON).

? Three or more years of experience in, big data and data virtualization management including experience in leveraging Hadoop

? Knowledge and experience with Statistical Modeling, and Machine Learning algorithms such as XGBoost, Random Forest,  Decision Trees with association rules, Ensemble Learning etc.

? Understand the internals of R and Python (Who can perform root cause analysis for the issues encountered in production)

? Prior experience with cloud services or cloud data services and/or data analytics   - Platform knowledge (Azure, Windows and Linux)

? Familiarity with ?IaaS? and ?DBaaS? Service oriented concepts

? Familiarity of Cloud Architecture (Public and Private clouds) ? AWS , AZURE

? Experience in the implementation details of Hadoop Clusters, Impala, and HBase and other emerging data techniques

 

 :-

    Years of Scripting with one of these languages (PHP, R, PYTHON).
    Years of SQL experience.
    Expert in querying and analyzing big data using Hive, Python, SQL, Scope and/or C#
    Experience ing with unstructured big data (Hadoop and/or Cosmos)
    Experts in advanced Excel functions (e.g., creating formulas, pivot tables) and PowerBI
    Prior knowledge of data modelling and processing techniques for big data systems
    Solid understanding of BI and data solutions, including Power-pivots, cubes, and datamarts.
    Self-motivated, agile and driven to think out-of-the-box
    Ability to influence diverse audiences and build strong partnerships with stakeholders
    Ability to  independently or to manage a virtual team that will research innovative solutions to challenging business problems
    Ability to collabo with partners and drive analytic  end to end,
    Experience with ing in cross-functional teams as a product owner or similar 
    Experience with Agile ",Data Scientist,"Insigma Inc. |

Phone: 703-344-9674 Extn. 112

Email: jitendra@us.insigmainc.com",yahoo,job recruiter
"  :- Data Scientist / Data Engineer

 :- Chicago, IL

 :- Long Term

Exp Req :- 8+ Years

 

  :-

    Senior data scientist / engineer
     Financial Domain Knowledge & experience
    Strong Experience in AI related 
    Knowledge & exposure in rendering ML functionality
    Understanding  AI/Deep Learning algorithm such as CNN, RNN, LSTM
    Experience in building AI based NLP and OCR solution using Keras, Google Tensorflow, Theano, Caffe 2 etc?
",Data Scientist,"Kishore  Rekapalli
(469) 327-6019
kishore@rnritsolutions.com",yahoo,job recruiter
" : Data Scientist

: Chicago, IL

: Contract & Fulltime both

 

 

     Financial Domain Knowledge & experience
    Strong Experience in AI related 
    Knowledge & exposure in rendering ML functionality

    Understanding  AI/Deep Learning algorithm such as CNN, RNN, LSTM
    Experience in building AI based NLP and OCR solution using Keras, Google Tensorflow, Theano, Caffe 2 etc",Data Scientist,"IDC  Inc. | 920 Hillview Court, Suite 250 | 95035, Milpitas, CA 
e: aman.rastogi@idc.com | w: idc.com 
m: +1 408-882-6727 |",yahoo,job recruiter
" 	Machine Learning Consultant
Mandatory  	Knowledge of theory and practice of machine learning.
Programming  in rapid prototyping environment such as - MATLAB. Java or Python; C++ and parallel programming (e.g., CUDA) is a plus.
Experience with Node.js, ruby, python
Knowledge of common machine learning frames.
Track record of research excellence or significant product development.
Knowledge of application areas, such as computer vision or natural language processing, is a plus.
Excellent communications 
 
 
  	Knowledge of application areas, such as computer vision or natural language processing, is a plus.
MATLAB. Java or Python; C++ and parallel programming is a plus.
 
 
 
 
 
 
Mandatory Functional  	2-5 years of experience in Development of Machine Learning / Deep Learning systems
Conceive deep learning approaches to solving particular product problems.
Construct and cu large problem specific datasets.
Design and implement machine learning techniques aimed at solving specific problems.
Collabo with other team members, both in research and product s.
Transfer  to product groups.
 Functional  	Knowledge of application areas, such as computer vision or natural language processing, is a plus.
 Experience  	6+ yrs
  	Sunnyvale, CA,",Data Scientist,"Diverse Lynx, LLC
300 Alexander Park Suite # 200
Princeton, NJ 08540
Tel: (732) 452-1006 Ext.236
lakshay.jassal@diverselynx.com
www.diverselynx.com",yahoo,job recruiter
"Big Data Architect
DE, PA, NJ, NYC, MA s (50-60% Travel is a must)

Fulltime 

 

Must have: Kafka, Spark Streaming, Scala, Hive, SQL. Shell Scripting, Java Programming

 

Mandatory  ?
Hands on Design and development experience in Kafka, Spark Streaming, Scala, Hive, SQL. Shell Scripting, Java Programming
Involve in planning, designing and stgizing the roadmap around On-prem and cloud solutions.
Experience in designing and developing real time data processing pipelines
Expertise in ing with Hadoop data platforms and  like Kafka, Spark, Impala, Hive and HDFS in multi-tenant environments
Expert in Java programming ,SQL and shell script, DevOps
Good understanding of current industry landscape and trends on  to ingestion, processing, consumption and security management
Expert in designing and implementation solutions for multi-tenancy with ability to drive automation using Big Data  and customer?s business , design and document a comprehensive technical architecture / solution.
Experience ing in a client delivery  in an on off-shore model.
Good to have 
- Exposure to Agile process models
- Resolve and troubleshoot problems and complex issues.
- Excellent written and verbal / Documentation  needed
- Good written and verbal communication 
- Bachelors in Computer Science or related field is a Plus",Data Engineer,"IDC , Inc.

1851 McCarthy Blvd Milpitas, CA 95035

: 408-500-0673

Mailto: amaranath.s@idc.com
Website: www.idc.com
  ",yahoo,job recruiter
": Sr Database Administrator
:Pleasanton, CA
: 12+ months
 
  ? PostgreSQL / MySQL / Maria DB

The DBA will perform administration and engineering for multiple production databases, and will be responsible for fast-paced, complex distributed database environments supporting both OLTP and OLAP systems across multiple platforms.
?  with Engineering / Development and CSS Operations teams to deliver and support new and existing PostgreSQL database systems.
? Help guide best practices in PostgreSQL database administration and introduce innovation with automation.
? Maintain an environment of dozens of PostgreSQL databases running in a Linux / Unix / Window environment.
? Responsible for providing day-to-day support and maintenance for existing and PostgreSQL databases.
? Design and build database solutions that are both highly available and highly performing.
? Assist in the administration, management, and monitoring of all PostgreSQL database systems.
? Accountable for proper backup and disaster recovery procedures.
? Good understanding of Database Architecture, Performance tuning of databases, pro-actively Identify bottlenecks and resolve them quickly.
? Up  continuously with emerging  and  them as and when needed.
?  with developers on performance tuning, query optimization, index tuning.
? Very strong experience in replication, clustering, tuning, sizing and proactive monitoring.
? Actively  with developers to solve database / application issues without compromising database best practices and  with them to promote changes to production.
? Experience with PostgreSQL database systems and a desire to learn new database .
? Support PostgreSQL database in external cloud and  with various cloud providers as .
? Rotate with other DBAs to be on call to quickly fix issues that arise in our production and non-production environments.
MySQL ?
Strong proficiency in MySQL database management
Establish enterprise level infrastructure support environment for MySQL DBA
Hands on Experience in setup, configuration, troubleshooting MySQL replication
Handle common database procedures, such as upgrade, backup, recovery, migration, etc.
Establish mechanisms for data backup/restore of relational databases and provide restoration services as needed
Estimate MySQL database capacities; develop methods for monitoring database capacity and usage
Resolve problems identified via Monitoring/Health Check
Develop application-specific fault-tolerant distributed database mechanisms
Database monitoring and Supporting in Production environment
Hands On experience with relational database performance tuning
Good knowledge of Day to Day commands of Linux/AIX Environments, Shell Scripting for configuring DB.",Data Analyst,"eTeam Inc.
nmubeen@eteaminc.com
(732) 302-8968",yahoo,job recruiter
": Data scientist
: Denver, CO
Contract:
 
Data scientists use data and analytical ability to find and interpret rich data sources; manage large amounts of data despite hardware, software, and bandwidth constraints; merge data sources; ensure consistency of data sets; create visualizations to aid in understanding data; build mathematical models using the data; and present and communicate the data insights/findings. Python 2-3 years? experience minimum Nume postgraduate  Must be familiar with Git. Experience w/ SDLC, B2B enterprise environments is a plusDeep experience with regression methods or other machine learning like random forest, gradient boosting, etc.",Data Scientist,"Diverse Lynx, LLC
300 Alexander Park Suite # 200
Princeton, NJ 08540
Tel: (732) 452-1006 Ext.244
pramod.kumar@diverselynx.com
www.diverselynx.com",yahoo,job recruiter
": Data Engineer
: Charlotte, NC
: Long Term Contract
 
 : (Data Engineer)

    Support or collabo with application developers, database architects, data analysts and data scientists to ensure optimal data delivery architecture throughout ongoing /operations.
    Design, build, and manage analytics infrastructure that can be utilized by data analysts, data scientists, and non-technical data consumers, which enables functions of the big data platform for Analytics.
    Develop, construct, test, and maintain architectures, such as databases and large-scale processing systems that help analyze and process data in the way the Analytics organization requires.
    Develop highly scalable data management interfaces, as well as software components by employing programming languages and .
     closely with a team of Data Science staff to take existing or new models and convert them into scalable analytical solutions.
    Design, document, build, test and deploy data pipelines that assemble large complex datasets from various sources and integ them into a unified view.
    Identify, design, and implement operational improvements: automating manual processes, data quality checks, error handling and recovery, re-designing infrastructure as needed.
    Create data models that will allow analytics and business teams to derive insights about customer behaviours
    Build new data pipelines, identify existing data gaps and provide automated solutions to deliver analytical capabilities and enriched data to applications.
    Responsible for obtaining data from the System of Record and establishing batch or real-time data feed to provide analysis in an automated fashion.
    Develop techniques supporting trending and analytic decision making processes
      for responsive front-end experience
    Ensure systems meet business  and industry practices
    Research opportunities for data acquisition and new uses for existing data
    Develop data set processes for data modeling, mining and production
    Integ data management  and software engineering  into existing structures
    Employ a variety of languages and  (e.g. scripting languages)
    Degree in Computer Science, Engineering, or related fields.
    5-8 years of experience

 
 
Python, Hadoop Development, Spark/Pyspark, DBMS (Postgres, MySQL, MongoDB)
 
Desired 
Graph DB (TigerGraph), Cloud (AWS), R, Statistical Modeling, Machine Learning/Deep Learning/GAN, Model Validation and Deployment",Data Engineer,"My name is Sai and I'm a recruiter at Artech, the #1 largest women-owned IT staffing firm in the US. We are constantly on the lookout for professionals to fulfil the staffing needs of our clients, and we currently have a job opening that may interest you.  Please find below, summary of the position.
If you are currently not in the job seeking market, feel free to refer this position to someone you may know to be a fit. However, should this position be of interest to you, please send me your resume or contact me directly at (973) 507-7547.
 
(Looking for consultants who can  on W2)",outlook,job recruiter
"Data Scientist - HCI

    Long Beach, CA
    Full-time


About SCAN

As one of the nation?s largest not-for-profit Medicare Advantage plans, serving more than 200,000 members in California, SCAN Health Plan has been a mission-driven organization dedicated to keeping seniors healthy and independent for over 40 years. SCAN employees are passionate about what they do, and understand that success is based on achieving the mission. Employees are afforded with the training and  necessary to do their s and are rewarded for their efforts and recognized as experts in their fields. To learn more, visit scanhealthplan.com or follow us on social media: LinkedIn; Facebook; and Twitter.

The 

The Data Scientist will support leadership and Healthcare Informatics teams with insights gained from analyzing company and external data to help make smarter decisions and drive better outcomes. You must be adept at using large data sets to find opportunities for quality improvement and process optimization. You must have strong experience using a variety of data mining/data analysis techniques, using a variety of data , building and implementing predictive models, using/creating algorithms upon structured and unstructured data. You must have a proven ability to drive business results with their data-driven insights. You must be comfortable ing with a wide range of stakeholders and functional teams. The right candi will have a passion for discovering solutions hidden in large data sets and ing with stakeholders to improve business outcomes.

You will

     with stakeholders throughout the organization to identify opportunities for leveraging big data to drive business solutions.
    Mine and analyze data (both structured and unstructured data) from company databases and external data when needed to drive quality improvement, process optimization and business stgies.
    Select features, building and optimizing models using machine learning techniques.
    Process, clean, and verify the integrity of data used for analysis.
    Develop custom data models and algorithms to  to data sets.
    Use predictive models to increase and optimize intervention targeting, coding accuracy, and other business outcomes.
    Do ad-hoc analysis and present results in a clear manner.
    Coordinate with different functional teams to implement models and monitor outcomes.
    Develop processes and  to monitor and analyze model performance and data accuracy.


Your 

    Master's Degree or PhD in Statistics, Mathematics, Computer Science, or another quantitative field.
    3-5 years of experience handling data sets and building statistical/machine learning models
    Excellent understanding of machine leaning techniques (clustering, decision tree learning, neural nets, etc.), the natural language processing techniques, and their real-world advantages/drawbacks.
    Knowledge of advanced statistical techniques and concepts (regression, properties of distributions, statistical tests and proper usage, etc.) and experience with applications.
    Experience using common data science toolkits (R, Python, SAS, etc.) to handle data and draw insights from large data sets. Excellence in at least one of these is highly .
    Proficiency in using query languages such as SQL.
    Excellent written and verbal communication  for coordinating across teams.
    A drive to learn and master new  and techniques.


What's in it for you? Qualified employees will receive:

    A competitive compensation and benefits program
    An annual employee bonus program
    Generous paid-time-off (PTO)
    Ten paid holidays per year
    Excellent 403(b) Saving Plan, providing up to 4% match and vesting after three years
    Casual attire
    A -life balance and much more!",Data Scientist,"
Data Scientist - HCI
 SCAN Health Plan Company  Long Beach, CA, US

 Posted 1 week ago Number of applicants 25 applicants",LinkedIn,Job post
"At TGS, data is a core part of our business and our data focused software developers are among our most valued resources. This  is mission critical, so we?re  for an uncommonly reliable professional who enjoys coding, ing with and analyzing data, and providing support for production systems. In addition to programming and data analysis, this  is likely to involve interaction with external resources such as data providers, brokers, dealers, and software vendors.

Successful candis will have experience in a number of the following general areas:

    Programming: strong experience with Python, Java, SQL, and/or similar languages
    Large data sets: experience developing programs to parse, process, clean, organize, and analyze large data sets
    Applications: experience designing, developing, and maintaining software applications
    Vendor interaction: ing with external resources to solve problems, acquire data, and improve relationships.
    System : experience with scripting languages (Perl, Python, shell scripts, etc.), and Unix-based operating systems (especially Linux)

Indeed Hire and TGS Management Company, LLC are ing together to find the best candi for this .

By ing, you agree to be contacted by our agent, Indeed Hire, and receive ups via text and phone about your application.

 Type: Full-time

Experience:

    Python, Java, SQL, and/or similar languages: 1 year ()
    designing, developing, and maintaining software applications: 1 year ()
    Large data sets: 2 years ()

 distance:

    Irvine, CA: Between 31 and 40 miles ()",Developer,"My name is Kendall and I am ing with TGS Management Company, LLC in Irvine, CA to fill their Software and Data Engineer role.",outlook,job recruiter
"KORE1, a nationwide provider of staffing and recruiting solutions, has an immediate opening for a Quality Assurance Auditor.


The Quality Assurance Auditor is responsible for validating the quality of a variety of enterprise reports. This includes validating the consistency and accuracy of reported metrics, as well as ensuring the consistent application of report style standards, overall layout and organization, and the naming convention used in the reports. As part of the metric validation process, the Quality Assurance Auditor will need to retrieve basic data sets using SQL Management Studio, and then replicate simple calculations using Excel.


Inputs to QA Audit Process

    List of reports for Quality Assurance
    Set of standard test case parameters to be performed ? i.e. identified variations in time period, time of the month (needed for MTD reports), geography, specialty
    Report Style Standards: font, colors, element page s (e.g. s, export/print, filters, etc.), element behaviors: (e.g. filter drop downs, tabs etc.), naming conventions (e.g. Providers/Dentists; Office/Clinic; etc.)


QA Audit Process

    Identify test case parameters
    Ensure  test data is in place
    Gene current state and new reports
    Perform a comparison of the report metrics
    Vali application of Report Style Standards (see list above)
    Document errors/issues encountered from #4 and #5
    Review errors with developer
    Create remedial user stories as 


The Quality Assurance Auditor plays a major  in improving the quality, functionality, reliability and usability of reports. 


 include

    Executing the QA Audit process
    Reviewing available project assets such as requirement documents, data mapping and systems documentation, test cases, defects, project notes, development notes, test data, and preparing QA audit reports.
    Learning and understanding the applications under test and perform test activities and make recommendations on the readiness of the target report.
    Effectively communicating with the project team, stakeholder and IT organization on the status of the testing efforts and readiness recommendation with an impact on analysis, reports and metrics.
    Identify issues in data quality, sourcing, or documentation by reviewing existing data and processes. Read, analyze, and interpret existing documents and data for completeness and accuracy.


 and minimum 

    Bachelor Degree in Computer Science, Computer Engineering or related field. 
    QA testing/Data Validation experience that includes 2+ years hands experience on data validation for financial application and/or health care
    Minimum 2 years? experience with MS SQL writing and executing basic SQL scripts.
    Excel 


#JBU

Seniority Level

Mid-Senior level
Industry

    Information  & Services Computer Software Hospital & Health Care 

Employment Type

Contract
 Functions

    Information  Engineering Other 

Applicant rank
Top 10% of 21 applicants
How you match
Criteria provided by  poster


    Match

Healthcare Information  (HIT)
Match
SQL
Match
Microsoft Excel
Match
Testing
No match
Quality Auditing
No match
Data Validation
No match
Test Cases
No match
Financial Systems
No match
Quality Assurance
No match
Data Quality ",Data Analyst,,LinkedIn,Job post
"
Manager Data Analytics, Risk Management - Secured Auto
 Oportun Company  Irvine, California, United States

 Posted 1 week ago Number of  160 



    5/10  match
    58 applicants

Company

    1,001-5,000 employees
    Financial Services

Connections
You have 0 connections at this company.
Add >

 MAY BE REMOTE WITH OCCASIONAL OFFICE VISITS -OR- BASED IN OUR IRVINE, CA, SAN FRANCISCO, CA, OR FRISCO, TX OFFICE**


About Oportun

Oportun is a mission-driven, -powered provider of inclusive, affordable financial services and a certified Community Development Financial Institution (CDFI). We seek to serve the 100 million people in the US who are shut out of the financial mainstream because they are credit invisible or are mis-scored because they have limited credit history. By lending money to harding, low-to-mode income individuals, we help them move forward in their lives, demonst their creditworthiness, and establish the credit history they need to access new opportunities.


Since 2006, we have lent over $6.8 billion through over 3.1 million affordable small dollar loans and have helped over 730,000 people start establishing credit. In recognition of inventive approach, we were recognized by Time Magazine as one of 50 Genius Companies inventing the future.


The Bay Area News Group recognized Oportun as a Top place in 2019. Come and be a part of our community of employees, partners, and customers who are devoted to expanding financial opportunity for millions. When we  together, we can make life better.


Summary


The Sr. Analytical Stgist/Manager of Risk Management helps manage credit policy and risk performance for Oportun?s Installment loan and Secured Auto products. This  supports improving Oportun?s core intellectual property leveraged in risk decisioning for underserved consumers that often lack a traditional credit bureau score. This is an exciting opportunity in a fast-paced organization where your contributions can have a meaningful impact on increasing access to Oportun?s affordable, credit building loan product for consumers with little or no credit history.




    Develop credit and pricing stgy to support the launch of Oportun?s Secured Auto and Refinance product
    Develop Underwriting and Pricing frames by leveraging Oportun?s Unsecured portfolio and Auto industry data to determine loss estimates for Oportun?s Auto products
    Develop and maintain Valuation frames to determine profitability at the margin. Recommend changes to risk stgies to maximize profitability and ensure  return on capital for all risk segments
     in collaboration with other Oportun product teams (Installment loans, Credit cards) to develop an optimal multi-product credit stgy
    Design and implement new tests to determine optimal loan size amount and term by risk segment. Optimize NPV of loan amount assignment stgy given a set of constraints and assumptions.
    Closely monitor the credit performance of Oportun?s loan portfolio adjusting risk stgies to ensure desired loss s 




    Bachelor?s Degree in Economics, Finance, Engineering, Mathematics, or other analytical discipline
    3+ years of experience in an analytic , preferably in the financial services industry. Highly talented recent college graduates will also be considered (Masters Degree in a quantitative discipline is a plus)
    Exceptional problem solving and analytical  with the ability to maintain the highest standard of integrity, accuracy and precision
    Solid Microsoft Excel and PowerPoint 
    Foundational knowledge and understanding of statistical concepts
    Strong SQL/SAS  and experience with other programming language a plus (R, Python, STATA, Matlab, etc)
    Excellent written and oral communication  (Spanish language is a plus)
    A relentless problem solver and out of the box thinker in finding ways to improve stgies and processes
    Smart, dedicated, compassionate, hungry, fun, and generally awesome personality

Seniority Level

Associate
Industry

    Information  & Services Computer Software Financial Services 

Employment Type

Full-time
 Functions

    Analyst Management Finance 
",Data Analyst,,LinkedIn,Job post
"
Data Analyst *** 50/hr *** 1203
 MiDCOM Group Company  Pomona, CA, US

 Posted 6  Number of  67 



    14 applicants
    Entry level

Company

    5,001-10,000 employees
    Telecommunications

Connections
You have 0 connections at this company.
Add >
Five or more years of experience with data preparation, data extraction using large structured and unstructured datasets and tool.
Three or more years? experience ing with ERP (such as SAP or Oracle), or analytical  such as SAS or Visual Analytics (VA).
Three or more years of experience using Excel functions such as V-lookups, Pivot Tables, and creating charts and graphs.

 Duties

This  will support various project managers and CD&FA RPPM  for reporting, analyzing and data cleanup efforts.
Responsible for CD&FA analytics, data interpretation, data extraction and modeling.
Develops, and assists or makes presentation to supervision, senior management, department and division management on initiatives supporting CD&FA stgies and goals.
Acts as key resource or consultant to department and division management and partner organizations on specific project, program, functional, or stgic issues with mode to broad impact where subject matter expertise goes beyond established procedures and practices and requires analysis and decision making.
Makes stgic recommendations on data collection, extraction, preparation, and integration all while maintaining data integrity.
Ensures optimal resource al and completion of .

MIDCOM is an innovative provider of IT, Technical, Engineering and Professional Services. For over 37 years, our diverse and expert service, talent, and advanced  have allowed us and our partners to win, deliver, and complete mission critical programs and initiatives within the Aerospace and Defense Sector.

MIDCOM has current long standing national contracts with the major Aerospace, Defense, High  and Cyber Security agencies and Prime Contractors. Aerospace has been the primary focus of MIDCOM?s business for over 37 years and we have an in depth understanding of the skill sets, experience, and controls needed to ope in this industry with unparalleled success. Our employees  with us over the years and through changing economies because we treat them fairly and with respect.

Midcom is proud to become a Cenergy company. To learn more about Cenergy, please visit www.cenergyintl.com
Seniority Level

Entry level
Industry

    Electrical & Electronic Manufacturing Information  & Services Defense & Space 

Employment Type

Full-time
 Functions

    Information  
",Data Analyst,,LinkedIn,Job post
"
Financial Systems/Data Analyst
 Danaher Corporation Company  Brea, CA, US

 Posted 1 week ago Number of  83 



    16 applicants
    Full-time

Company

    10001+ employees
    Medical Device

Connections
You have 0 connections at this company.
Add >
As a Financial Systems/Data Analyst within the Global Financial Planning & Analysis team in Brea HQ, you will focus on the implementation of business intelligence solutions, including the development and enhancement of financial analysis and reporting  that our finance business partners will utilize to deliver key insights and drive improved financial results.

You will also  on  related to database systems, data integration, ETL and report development, primarily in the Microsoft environment (Excel, Access, Power BI). Activities of the department are growing rapidly and we are looking to bolster our IT system capabilities. Team members are  based in Brea, California, and in Geneva, Switzerland.

You?ll be trained and mentored by the Manager of Global Financial Analytics and Reporting, and receive on-the- training potentially in Brazil, Switzerland or China.

Provide ongoing support for system enhancements for financial planning and reporting systems
Utilize your expert  in Excel and VBA programming
Implement Power BI  and modeling
Develop SQL-based  to support the development of new reporting
ETL (Extract, Transform, Load) as well as to troubleshoot existing financial reporting schedules and related processes.

 
Lead financial reporting and system enhancement initiatives by defining , designing solutions, and testing new functionality
Develop new functionalities in reporting and pursue continuous improvement where there are gaps in processes or where additional data is needed for the business
Develop and implement Power BI reporting to improve existing processes
Recommend and implement process improvement/re-engineering in the form of high-impact initiatives as well as more routine items such as existing VBA scripts (using VBA, SQL, PowerShell?)
Develop training materials for new and existing users, as needed
Bachelor?s degree (with 2+ years of direct  experience) or Master?s Degree preferably in Computer Science or Management of Information Systems
Minimum of two years of experience of demonstd acumen using Excel, VBA, and Power BI/Tableau or related
Second major or degree in Finance/Accounting, Business, or Economics is considered a plus
Experience ing in a Finance-related  or partnering with Finance staff is considered a plus
SQL / Power Shell / Python / Access acumen and experience is highly 
Analytical thinker with strong problem-solving and organizational skill s
Strong attention to detail and ability to deliver high quality  product
Able to  under pressure, meet deadlines and handle multiple  simultaneousl y
Demonsts intellectual curiosity and desire to understand root cause of a problem
Team player who is also able to  well independently
Strong communication (and/or demonstd desire to improve)


Danaher Corporation and all Danaher Companies are s that evaluate qualified applicants without regard to race, color, national origin, religion, sex, age, marital status, disability, veteran status, sexual orientation, gender identity, or other characteristics protected by law. The ?EEO is the Law? poster is available here .
Industry

    Medical Device 

Employment Type

Full-time
 Functions

    Information  
",Data Analyst,,LinkedIn,Job post
"
Data Analyst/Developer
 Harris Computer Company  Irvine, CA, US

 Posted 4  Number of  77 



    8 applicants
    Entry level

Company

    5,001-10,000 employees
    Computer Software

Connections
You have 0 connections at this company.
Add >
Connecture is  a Data Analyst/Developer to join our team!
As the Data Analyst/Developer you will aali and load client provided data for use with our web applications and
develop and enhance internal .NET application.

What Your Impact Will Be

    Support, refine and further develop existing ETL feeds
    Monitor and maintain all data ups
    Develop database objects and scripts to load data
    Provide technical documentation for data feeds
    Establish and maintain consistent data element definitions
    Identify and advance opportunities to improve data flows and supporting processes
    Assist in the architecture design and testing of new systems
    Develop reports for SQL reporting services as well as performing ad-hoc report request
    Develop various application  based on business needs
    Perform QA on application  to ensure they meet the business s

What We Are Looking For

    Bachelor's degree in a Business, Technical or other applicable discipline.
    3-5 Years experience in SQL programming
    Experience ing with SQL 2008/2012/2016
    Experience using ETL  and SSIS packages
    Experience ing with development using C# .NET
    Experience in supporting enterprise-class software (24 * 7 availability) is a big plus
    Background in the principles of software design and implementation
    Project management ; ability to multi-task
    Data analysis and analytical 
    Written and verbal communication 
    SQL query writing 

What Will Make You Stand Out

    Adept at finding fresh solutions as the new problems arise
    Taking the lead on cross-team 
    Understanding of computer science fundamentals, as well as proficiency in a broad array of  including operating systems, neting, databases, and application development
    Attention to detail with the ability to manage a project and ensure everything stays on track

What We Offer

    A competitive compensation package
    A casual  environment
    Full range of employee benefits 401(k), Health Insurance (medical, dental, vision, life, short and long-term disability, AD&D)
    Paid Vacation

About Us

For nearly two decades, Connecture's vast net of health plan data has brought carriers, FMOs and brokers together to simplify quoting and selling health plans in order to help millions of consumers find and enroll in their best fit Medicare plans every year.
Harris is an Equal Opportunity/Affirmative Action Employer. We consider applicants without regard to race, color, religion, age, national origin, ancestry, ethnicity, gender, gender identity, gender expression, sexual orientation, marital status, veteran status, disability, genetic information, citizenship status, or membership in any other group protected by federal, state or local law.
If you would like to contact us regarding the accessibility of our website or need assistance completing the application process, please contact us at (1)-613-226-5511 or at HarrisTalentAcquisition@harriscomputer.com. This contact information is for accommodation requests only and cannot be used to inquire about the status of applications.
Seniority Level

Entry level
Industry

    Information  & Services Computer Software Insurance 

Employment Type

Full-time
 Functions

    Information  
",Data Analyst,,LinkedIn,Job post
"
Data Analyst
 AAA Texas Company  Costa Mesa, CA, US

 Posted 3 weeks ago



    0 applicants
    Entry level

Company

    5,001-10,000 employees
    Insurance

Connections
You have 0 connections at this company.
Add >


Team. Integrity. Dedication. Together, we make a difference. If you are career-minded, service-driven professional looking to join a fast paced organization then you have come to the right place. AAA is a member service organization affiliated with the national AAA net. With offices across the U.S., we?re united by common mission and common values of excellent member service. With more than 14,000 employees in 21 states, we provide legendary service to 16 million loyal members. With a constantly growing membership, we are always welcoming dedicated professionals looking to challenge themselves and build a career within our dynamic organization. You will find that being part of a very successful team is extremely rewarding.

This  administers the force Management and eResource systems; Develops and builds schedules for Call Center staff based on forecast volume and AHT; Implements and manages achievement of stgic business plan; In addition, the  develops staffing deviation reports, volume reports, AHT reports, absenteeism reports and VTO reports, analyzing trends, making recommendations regarding staffing levels, schedules, productivity and costs. The  ensures agent s are accu and upd real-time, ensures known time off for meetings, vacations, holidays, etc. is pre-planned and replacement staffing is in place to meet Service Level goal. Additionally, the  processes schedule changes and quarterly vacation selections. The  is also responsible for making minute-by-minute decisions based on daily trending analysis to ensure resources are available to handle real time traffic loads while communicating with Operations Management and the Real Time Adherence agent in each Call Center regarding additional hour?s needs, VTO and the adjusting of agents? offline activities.



    Advanced Excel  (incl. VBA, macros, complex calculations, etc.)
    SQL and Tableau , with data mining knowledge
    Proficiency using Microsoft Office software, including Outlook, Word, PowerPoint, Excel and Access
    Experience with Genesys/Genesys WFM
    Demonstd knowledge of call center performance metrics (Average Handle Time, Average Hold Time, etc.)
    3+ years of progressive experience with analysis, identifying trends, business operations, or analytics
    Strong understanding of statistical methods, advance data analysis and validation
    Proven ability to produce recommendations and proposals and oversee  as directed
    Strong organizational and time management  with the ability to multi-task and meet project deadlines set by management
    Bachelor?s degree or equivalent combination of education and  experience
    Professional written, oral communication  and business relationship management 
    Benefits:
    Health Coverage for Medical, Dental, Vision
    Paid time of including Vacation, Illness and Holidays
    Life Insurance
    Disability Coverage
    Pension
    401K Savings Plan
    Employee Discounts
    Career opportunities across multiple business lines and states ?Through dedicated employees we proudly deliver legendary service and beneficial products that provide members peace of mind and value.? AAA is an 

Seniority Level

Entry level
Industry

    Information  & Services Insurance Financial Services 

Employment Type

Full-time
 Functions

    Information  
",Data Analyst,,LinkedIn,Job post
"Associate, Data & Analytics Modeler, Machine Learning
KPMG6,154 re - Irvine, CA 92618


Innovate. Collabo. Shine. Digital Lighthouse houses KPMG's specialized capabilities across the digital landscape: applied data science, AI, data engineering and insights, software engineering, automation, and big data. Here, you'll  with a sophisticated team of professionals to explore solutions for clients in a multiplatform environment. This means your ability to find answers is limited only by your creativity in leveraging a vast array of techniques and . Be a part of a high-energy, unique, fast-paced, and innovative culture that delivers with the agility of a tech startup and the backing of a leading global consulting firm. In this particular , you'll  specifically in the AI Analytics & Engineering Community within the Digital Lighthouse, on a wide range of . From applied AI to optimization to big data platform engineering, your analytical and  will drive real impact in the business world. So, bring your ingenuity and pioneering spirit to KPMG Digital Lighthouse.

KPMG is   an Associate to join our KPMG Lighthouse - Center of Excellence for Advanced Analytics.

:

     in multi-disciplinary and cross-functional teams to translate business s into artificial intelligence approaches and s; Strong aptitude for quickly learning business operational, process, delivery, and revenue models
     in a fast-paced and dynamic environment with both virtual and face-to-face interactions; utilize structured approaches to solving problems, managing risks, and documenting assumptions; Communicate results and educate others through insightful visualizations, reports, and presentations
    Assess, capture, and translate issues and  into structured analytics use case, including rapid learning of industry/domain/client dynamics and development of effective  stream plans
    Build ingestion processes to prepare, extract, and enrich a variety of structured and unstructured data sources such as social media, news, internal/external documents, images, video, voice, emails, financial data, and operational data
    Utilize a hypothesis-driven problem-solving approach to design, construct, and rapidly test/ite exploratory models that will reveal insight and opportunities for the client while proactively broadening and deepening client relationships
    Deliver on engagement milestones by following analytics processes to mitigate risks in data, modeling, validation, and delivery; Manage assumptions, and risks, and  with others to clear issues

:

    Minimum of one year of experience in the design and building of machine learning pipelines, including data extraction, feature engineering from structured and unstructured data; Experience with the entire software development lifecycle, knowledge of software development methodologies, frames, , and technical architecture
    Bachelor's or Master's degree in an applied quantitative discipline (Data Science, Analytics, Computer Science, Engineering, or Mathematics) from an accredited college or university with experience involving modeling (regression, machine learning, feature selection, dimension reduction, validation), data (extracting, preparing, munging, validating), and building analytics pipelines
    Ability to  artificial intelligence techniques to achieve concrete business goals; ability to  with the business to understand available resources and constraints around data (sources, integrity, and definitions), processing platforms, and security; provide assistance, and resolve problems, using solid problem-solving , verbal/written communication, and data visualizations
    Broad, versatile knowledge of analytics and data science landscape, combined with strong business consulting acumen, enabling the identification, design and deployment of optimal machine learning solutions
    Proficiency with sophisticated analytics  and programming languages (SAS, R, Python, Java, Spark, Hadoop, Alteryx, SQL); Proficiency with data visualization  such as Tableau and QlikView
    Ability to travel up to eighty percent of the time; Applicants must be  authorized to  in the United States without the need for visa sponsorship now or in the future

KPMG LLP (the U.S. member firm of KPMG International) offers a comprehensive compensation and benefits package. KPMG is an affirmative action-. KPMG complies with all applicable federal, state and local laws regarding recruitment and . All qualified applicants are considered for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, protected veteran status, or any other category protected by applicable federal, state or local laws. The attached link contains further information regarding the firm's compliance with federal, state and local recruitment and  laws. No phone calls or agencies please.
6 

	",Data Scientist,,indeed,Job post
"Data Analyst (SAS, statistics, SQL, Healthcare)
Kinetic Personnel Group Inc.11 re - Ontario, CA
Full-time, Contract
 Now


Kinetic Personnel Group is  recruiting for multiple Data Analyst s for a Public Health Agency (government entity) in the Ontario, California area. The salary range is $65,000 to $85,000/year, with a yearly bonus and excellent benefits and CalPERS retirement pension program,  compensation year one with salary and benefits is $85,000 - $115,000/year. This company is renowned for its  in the community and being a great place to !

Purpose

The Data Analyst is responsible for providing technical, analytical and reporting support for the company's Clinical and Provider departments, programs and quality activities. The Data Analyst must have strong understanding of research methods and statistical concepts to design reliable studies/analyses, and payment algorithms. In addition to technical statistical concepts, the Data Analyst must understand relational database concepts, table design and structure, joins, data normalization, and data types..

Duties:

    Develops and prepares reports to support company's clinical and provider units utilizing available analytics and data mining , including but not limited to: SAS, SQL, SSRS system-specific reporting  or other related reporting software.
    Exercises timely and accu data mining techniques and develops new or modifies existing reports that ensure consistent conclusions to data analysis.
    Use analytical  to determine solutions and resolve problems.
    Responsible for evaluating report requests to determine requestor?s needs and s, identifying correct methodology for extracting data including data sources and criteria, and ensuring the delivered report is accu, timely, and formatted appropriately.
    Responsible for understanding the various data sources available within the company for reporting and analysis, as well as a clear understanding of business operations to be able to appropriately guide the development of quality program activities, including analysis and reporting.
    Significant participation in analysis, interpretation, and translating of complex health plan data, issues, trends, and relationships into effective stgies and action plans.
    Responsible for managing assigned  and effectively communicating with the leadership team any project deadlines that may be in jeopardy of being met, well in advance of the due .
    Document all analysis / reporting  following established protocols, ensuring replication of results by an independent reviewer.

:

    Bachelor's degree ; Master's .
    Data analysis, data presentation, spread sheet and database capabilities.
    ing knowledge of data analysis, study methodology, and preparing professional documents. Basic understanding of statistical methods.
    SAS or other statistical applications
    Extensive knowledge TSQL/SQL development
    SSRS experience 
    Proficiency in Microsoft Excel, Word.

 Types: Full-time, Contract

Salary: $65,000.00 to $85,000.00 /hour

Experience:

    Statistical Analysis: 3 years ()
    SQL: 1 year ()

Education:

    Bachelor's ()

 :

    United States ()

Contract Length:

    5 - 6 months

Contract Renewal:

    Likely

Full Time Opportunity:

    Yes

 :

    One 

Benefits:

    Health insurance
    Dental insurance
    Vision insurance

7 
If you require alternative methods of application or screening, you must approach the employer directly to request this as Indeed is not responsible for the employer's application process.

	",Data Analyst,,indeed,Job post
"Senior IT Business Analyst
VeriCour - Irvine, CA
 Now

Senior IT Business Analyst

Excellent opportunity to join a dedicated, laid back team in a growing organization.

The IT Business Analyst will be responsible for defining technical and business  in order to guide the development teams through the software development lifecycle including the creation of SDLC artifacts, specifications and deliverables.

:

    Partner with business owners and impacted teams to understand, identify, analyze and document .
    Elicit business  through existing documentation and user inter.
    Use a variety of techniques to determine functional and non-functional business  to support a broader understanding of business process and system needs.
    Collabo with developers and functional teams to understand related system and data integration stgies, prioritize and accommo  and support development process to ensure that project deliverables are met according to specifications.

 Develop and maintain a repository of technical documents and publications that support systems including disaster recovery plans.

 Provide training to users and stakeholders as necessary.

:

 Bachelor?s degree in Business, IT or related field.

 5+ years? experience in a Business Analyst or related field.

    Extensive knowledge of data analysis and data profiling.

 Experience identifying and documenting business  at multiple levels across complex  impacting several systems and functional areas.

 Solid understanding of SQL, OLTP and OLAP architectures as well as database programming using SQL server.

 Background in IT processes and SDLC.

 Ability to  independently with minimal guidance in a fast paced, team environment.

Excellent company benefits including medical, dental, vision.

.

 Type: Full-time

Experience:

    SDLC: 3 years ()
    Mortgage/Finance/Banking industry: 1 year ()
    IT Business Analyst: 5 years ()
    SQL: 3 years ()

Education:

    Bachelor's ()

:

    Irvine, CA ()

 :

    United States ()

30+ 
If you require alternative methods of application or screening, you must approach the employer directly to request this as Indeed is not responsible for the employer's application process.

	",Data Analyst,,indeed,Job post
"Sr. Sales and Operations Planning Analyst
Niagara Bottling483 re - Diamond Bar, CA 91765


At Niagara, we?re looking for Team Members who want to be part of achieving our mission to provide our customers the highest quality most affordable bottled water.

Consider ing here, if you want to:

     in an entrepreneurial and dynamic environment with a chance to make an impact.
    Develop lasting relationships with great people.
    Have the opportunity to build a satisfying career.

We offer competitive compensation and benefits packages for our Team Members.

Sr. Sales and Operations Planning Analyst

The Sr. Supply Chain Planning Analyst - S&OP is a specialist who drives innovation and operational excellence by aligning the planning, operations, and sales teams within Niagara. He/she s across the full breadth and scale of Niagara?s supply chain, products, manufacturing and  to help create solutions that solve business and customer?s supply chain s. This  helps provide recommendations on capex planning and inventory and production stgy.

Essential Functions

    Stgic Planning (assist with the following by providing solutions to support):
        Master Planning
            Develop, drive, and implement aggregate master schedules that optimize product availability, batch stgy, and inventory policy
        Stgic Capacity planning
            Growth/expansion capacity planning (domestic + international)
            Net Planning
            Executing annual capacity expansion planning calendar
        Tactical Service Planning
            Review customer demand including promotional activities, planned manufacturing downtimes &  and develop net supply plan that meets customer forecast at targeted service levels
            Collabo with planning, sales, logistics and manufacturing to successfully execute the designed supply plan
            Lead and drive periodic Tactical S&OP Meetings across departments to align on goals and ensure operational execution
    Internal Project Management
         with various departments at Niagara to discuss customer supply chain challenges and create a project plan to recommend a solution
        Get all internal departments on board to execute the proposed solution
        Manage and monitor progress of the project to ensure successfully execution
    Technical ? Good with systems, and a strong database et (Access/SQL) and Excel background is . Understanding  and IT limitations is critical in managing improvement .
        Ability to learn existing IT  at Niagara quickly
        Good knowledge of leveraging technical information for data collection
        Basic understanding of supply chain optimization including Net flow optimization, Inventory Optimization and Facility Planning
        Demand & Supply Planning 
            Setup, customization & solution validation of various modules
            Continuous improvement and automation to meet business rules/needs
             with Solution Architect to setup data integrity policies & procedures
    Inter-Departmental Collaboration
        Transportation and Logistics supplier bid event support
        Net design & stgic inventory build design
    Special 
        Long term capacity planning and capex recommendations
            Review new plants/new lines  for subsequent years and run model/scenarios for comparative analysis
    Travels approximately 10%
    Please note this   is not designed to contain a comprehensive list of activities, duties or  that are  of the employee for this . Duties,  and activities may change at any time with or without prior notice.



    Minimum :
        4 years ? Experience in Planning, MRP, Procurement, Logistics, Finance or similar function

*experience may include a combination of  experience and education

     :
        6 years ? Experience in Planning, MRP, Procurement, Logistics, Finance or similar function

    experience may include a combination of  experience and education

    Travels approximately 10%

Competencies

    Resourceful in defining systems stgy, developing systems , designing and prototyping, testing, training, defining.
    Experience with Supply chain planning  like JDA, Logility, Oracle ASCP, etc
    Experience with Supply chain optimization  such as LogicNetPlus, Supply Chain Guru, IPFA, etc.
    Ability to Multi-task and Prioritize
    Strong analytical .
    Excellent Communication . Able to effectively communicate complex technical subjects.
    Access/SQL database 
    Ability to solve practical problems and deal with a variety of concrete variables in situations where limited standardization exists.
    Strong verbal, interpersonal and organizational .
    Ability to prioritize  and meet deadlines
    Must be able to  well under pressure in a very fast paced environment.

This  also embodies the values of Niagara?s LIFE competency model, focusing on the following key drivers of success:

    Lead Like an Owner
        Makes safety the number one priority
        Keeps alert for safety issues and escalates immediately
        Effectively prioritizes tasks based on department goals
        Shows respect to others and confronts interpersonal issues directly
        Prioritizes resolution of customer issues effectively
        Responds promptly and honors commitments to internal and external customers
    InnovACT
        Makes recommendations to continuously improve policies, methods, procedures, and/or products
        Demonsts adaptability by reacting appropriately to unexpected changes in situations or circumstances
        Increases performance through greater efficiency
    Find a Way
        Seeks to develop technical knowledge through learning from other experts
        Understands interdepartmental impact of individual decisions and actions
        Seeks solutions rather than placing blame
    Empowered to be Great
        Consistently looks for ways to improve one?s self through growth and development opportunities
        Communicates clearly and promptly up, down, and across
        Communicates effectively to manage expectation

Education

    Minimum :
        Bachelor's Degree in Economics, Engineering, Mathematics, Supply Chain or other science / math related field, or equivalent  experience

    :
        Master's Degree in Economics, Engineering, Mathematics, Supply Chain or other science / math related field, or equivalent  experience

Certification/License:

    : None 
    : APICS ? CSCP, CPIM, CSCMP - SCPro

Foreign Language

    : None 
    : None

Any employment agency, person or entity that submits a rsum into this career site or to a  manager does so with the understanding that the applicant's rsum will become the property of Niagara Bottling, LLC. Niagara Bottling, LLC will have the right to hire that applicant at its discretion without any fee owed to the submitting employment agency, person or entity.

Employment agencies that have fee agreements with Niagara Bottling, LLC and have been engaged on a search shall submit rsum to the designated Niagara Bottling, LLC recruiter or, upon , submit rsum into this career site to be eligible for placement fees.
30+ 

	",Data Analyst,,indeed,Job post
"Business Analyst
CBD College7 re - United States
 Now
ly 

CBD College is looking for a Business Analyst/Tableau Developer. Rather than develop reports which only tell what happened, we seek to create stories with client data. Our ideal candi not only understands the technical side of Tableau, but is passionate about the field of analytics where they can use their creativity to push past line graphs, bar charts, and tables.

Tableau Developers interact with our business users, to uncover the ?Why? for the analysis. They will know how to manage small  and mentor Tableau users. From a Tableau perspective, the individual should have in-depth knowledge and experience with ing with large data sets and how to design high performing dashboards.

Main Duties/Functions

    Partner with business stakeholders to define their business intelligence needs and develop solutions for delivering against them

    Design, build, maintain, and optimize data visualizations and other self-service  to provide actionable insights that enhance business decision making

    Set up  and processes for effective data management.

    Shape best practices in visualization to deliver an excellent end-user experience

    Leverage the full range of Tableau platform  to design and implement production-ready solutions and create advanced BI visualizations.

    Develop and provides documentation, training, and support for clients.

    Champion utilization of business intelligence  across functional s throughout the organization

    Manage a portfolio of business intelligence  to ensure delivery

    Perform data analysis and identify business trends that can be turned into actionable insights

/:

    3+ years of business intelligence experience.

    3+ years ing with Tableau Software; up to  on the latest version and product capabilities, familiar with Tableau's JavaScript API.

    3+ years of experience creating custom SQL queries, familiarity with PostgreSQL a plus

    Experience with varied data sources, including Salesforce, Moodle, and Excel.

    A strong spirit of innovation, self-starter, independent, and with the ability to come up with solutions to meet business problems

    Strong desire to learn new things and pass knowledge acquired onto others

    Experience in implementation of end-to-end BI life-cycle  including dimensional modeling, ETL/integration processes, and metadata modeling

    Must have experience in Agile reports delivery methodology

    Keen attention to detail with deep quantitative and analytical  and demonstd capacity to solve problems and enhance reporting

    Strong relationship  and ability to build alignment among key stakeholders with demonstd experience influencing without authority

    Self-starter with ability to solve ambiguous/ open-ended business challenges

    Ability to  effectively in a fast-paced, deadline-driven, highly dynamic atmosphere and react quickly to changes and shifting priorities.

    Data interfaces such as Web Services, REST API and XML

To , please send a resume to onlinecareers AT cbd.edu. We will not accept resumes from other avenues.

 Type: Full-time

Experience:

    Tableau: 3 years ()
    Business Intelligence: 3 years ()

 :

    Fully Remote

Benefits:

    Dental insurance
    Health insurance
    Vision insurance
    Retirement plan

Company's website:

    www.cbd.edu

30+ 
If you require alternative methods of application or screening, you must approach the employer directly to request this as Indeed is not responsible for the employer's application process.

	",Data Analyst,,indeed,Job post
"Financial Analyst, Capital Markets
Ecom Mortgage, Inc. - Covina, CA 91724
$60,000 - $80,000 a year
 Now

 Purpose:

Reports financial status by preparing and analyzing financial plans, forecasts, and reports.

Duties:

    Perform advanced financial modeling and ad hoc analysis reporting
    Research and discuss hedging stgy with management and secondary marketing department
    Monitor and track key performance metrics against the company's stgic initiatives
    Analyze financial data and trends on companies in the mortgage banking industries
    Develop and present mortgage product offerings to achieve company's target conversion 
    Participate in civic activities to promote growth and development of the community and a positive company image
    Gather and analyze data on real estate markets including comparable lease and sale transactions
    Assist with due diligence processes, including analysis of pertinent real estate market, demographic, and economic data

/:

    Master's Degree in finance, accounting, or other related fields
    Minimum 1 year mortgage banking or industry-related experience in reporting, financial analysis, and accounting
    ing knowledge of AMB system
    Excellent communication , verbal and written
    Demonst excellent analytical  and attention to detail

 Type: Full-time

Salary: $60,000.00 to $80,000.00 /year

Experience:

    Secondary Marketing: 2 years ()
    Financial Reporting: 2 years ()
    Financial Modeling: 2 years ()
    Financial Analysis: 2 years ()

Education:

    Master's ()

:

    Covina, CA 91724 ()

 :

    United States ()

30+ 
If you require alternative methods of application or screening, you must approach the employer directly to request this as Indeed is not responsible for the employer's application process.

	",Data Analyst,,indeed,Job post
"Health Data Analyst IV
Kaiser Permanente10,192 re - Corona, CA 92879


    Provide quantitative measurement and data
    analysis expertise for the Utility for Care Data Analysis. Responsible for implementing
    established, as well as developing and ing innovative techniques for
    measuring, analyzing, and reporting on healthcare processes, health outcomes,
    and health costs.

    Essential Functions:

        Independently determines and implements appropriate measurement and
    analytical methods to achieve specified analytical project or tasks s.

        Exercises independent professional judgment regarding analysis assumptions
    and data quality. Identifies and resolves data quality issues, as .

        Coordinates the  of other project team members (UCDA staff, regional
    analytic staff, and contractors) to accomplish specified analytical project or
    task s.

        Determines the most informative approaches to summarizing data and
    communicating analytical results. Develops, as , innovative or
    customized templates and formats.

        Implements specified quality assurance procedures to ensure accuracy of
    project data, results, written reports, and presentation materials.

        Designs and creates written reports and presentation materials describing
    project s, methods, data, and results.

        Given project or task data , determines the most efficient
    approach to data collection and validation and s independently to obtain
    needed data from KP HealthConnect information systems (e.g. Clarity).

        Trains and coaches less-experienced healthcare data analysts on data
    retrieval techniques, programming practices, and statistical analysis and data
    visualization methods in the context of specific tasks.

        Specifies project quality assurance re and tests based on project
    characteristics and . Provides guidance to other healthcare data
    analysts regarding the implementation of such QA re and tests.

        Develops detailed measurement and reporting specifications based on project
    characteristics and  provide by project leadership. Provides
    guidance to other healthcare data analysts regarding the implementation of such
    specifications.

        Applies analytic knowledge,  and experience to perform project-related
    , complete specific project tasks, and create project deliverables.

    Gathers,
    researches, and analyzes data and facts from dispa sources and systems to
    identify and solve a wide range of problems within area of expertise. Prepares
    data in formats such as abstracts, graphs, or summaries.

    Experience
    with relational database concepts (SQL and HiveQL ) .
    Substantial experience with Microsoft Excel and knowledge of fundamental
    statistical concepts . Experience with Microsoft Power BI, Power Query,
    and DAX a plus.


Basic :
Experience

    Minimum five (5) related years of  experience with master's degree.

Education

    Master's degree OR a doctoral degree.
    Disciplinary training in advanced quantitative measurement methods (mathematics or statistics), ideally in the context of a quantitative social science or health services research.

License, Certification, Registration

    N/A


Additional :

    Demonstd effectiveness in written and verbal communication of technical material.
    Demonstd effectiveness ing as a member of a technical project team on complex .
    Experience with analysis of large administrative databases and computer-intensive statistical analysis (preferably using SAS), multivariate statistical methods, general research methodology.
    General knowledge or  experience in one (1) or more of the following areas: disease management, survey design, health status measurement, case-mix or health risk adjustment methods, research methods, actuarial methods, cost-benefit or cost-effectiveness analysis.
     in analytical methods including problem and model formulation, algorithm selection, and development of overall solution stgy.
    Creativity, critical thinking, and excellent problem-solving .
    Outstanding written and verbal communication and effective interpersonal .
    Ability to train and coach less-experienced team members on technical tasks.
    Considered functional expert.
    Must be able to  in a Labor/Management Partnership environment.


 :

    Minimum five (5) related years of  experience (i.e. in an analytical environment, preferably in healthcare) with master's degree.
    Knowledge of healthcare industry, especially healthcare analytics .
    Familiarity with Kaiser Permanente healthcare system .


    Analytic and critical thinking , writing , communication , and ability to  within a team.


    Proficiency in SQL, knowledge of healthcare industry, risk adjustment, and/or familiarity with KP healthcare system are a plus.


    Expertise with Microsoft Excel and knowledge of fundamental statistical concepts .


    Experience with relational database concepts (SQL ), Power BI, and Power Query a plus.


Primary : California,Oakland,1800 Harrison 1800 Harrison
HoursPerWeek : 40
Shift : Day
days : Mon, Tue, Wed, Thu, Fri
ingHoursStart : 9:00 AM
ingHoursEnd : 5:00 PM
 Schedule : Full-time
 Type : Standard
Employee Status : Regular
Employee Group/Union Affiliation : Salaried, Non-Union, Exempt
 Level : Individual Contributor
 Category : Accounting, Finance and Actuarial Services
Department : Risk Adjustment Data & Analytics (RADA)
Travel : Yes, 5 % of the Time
Kaiser Permanente is an  committed to a diverse and inclusive force. Applicants will receive consideration for employment without regard to race, color, religion, sex (including pregnancy), age, sexual orientation, national origin, marital status, parental status, ancestry, disability, gender identity, veteran status, genetic information, other distinguishing characteristics of diversity and inclusion, or any other protected status.

External hires must pass a background check/drug screen. Qualified applicants with arrest and/or conviction records will be considered for employment in a manner consistent with federal and state laws, as well as applicable local ordinances, including but not limited to the San Francisco and Los Angeles Fair Chance Ordinances.
30+ 

	",Data Analyst,,indeed,Job post
"Encounter Data Analyst
Prospect Medical Systems73 re - Orange, CA 92868
 Now
ly 

We are hospitals and affiliated medical groups, ing closely together for the benefit of every person who comes to us for care. We build comprehensive nets of quality healthcare services that are designed to offer our patients highly coordinated, personalized care and help them live healthier lives. Through collaboration, we strive to provide all of our patients and medical group members with the quality, affordable healthcare they need and deserve.

The Encounter Analyst is responsible for monitoring and oversight of the end-to-end encounter management flow. This  analyzes complex encounter inbound/outbound process issues, using data from internal and external sources to provide insight to decision-makers.
 /Duties

    Responsible for documenting, monitoring and analyzing the end-to-end encounter life cycle, inbound and outbound encounter process.
    Identifies and interprets encounter data, submission  and performance metrics per the regulatory and health plan guidelines.
    Researches and documents all encounter errors in established systems(s)/database(s) with appropriate statistical and trend analysis
    Performs root cause analysis of claims/encounters processing and submission issues; develops recommendations based on data and industry standards.
    Collabos across various departments to design and implement any business process and/or systems changes to meet encounter data processing and submission goals.
    Communicates with and provides clear, detailed, effective documentation to other departments within the organization on issues causing encounter pends/denials and potential solutions.
    Collabos with the Health Plans, clearinghouses and product Vendors on any encounter related issues
    Communicates regularly with management on issues discovered through research efforts
    Develops various encounter related reports (i.e. exception reports, performance reports, root cause analysis outcome reports, etc.) and distributes them to appropriate departments for error resolution, follow up and performance monitoring.
    Participates in the group to resolve encounter data and process issues.
    Owns and maintains the desktop procedures, flows and policy and procedure documents for encounters
    s to optimize business performance by recommending balanced approaches to system design and operational/process oriented solutions
    Review and research inquiries from regulatory bodies and/or the Health Plans related to submission data including the score cards from the Health Plans
    s with the Encounter/ EDI team for rolling out the best practices around encounter management
    Proactively monitors the performance of encounters metrics and recommend system/business process adjustments as applicable
    Other duties as assigned

Minimum Education: Bachelor?s Degree in a healthcare discipline, or combination of AA Degree plus 5 years of progressive managed care Encounter analysis experience, or seven (7) years of progressive managed care Encounter analysis experience .

Minimum Experience: Three to five (3-5) years ing in the health payer industry with proficient level understanding of general business practices and health plan operations (Membership, Provider, Claims, Customer Service, Care Management, etc.) . Three to five (3-5) years of experience as a Business Analyst or Operations Analyst (must have some experience writing business  and ing with technical teams to implement custom solutions) . Three to five (3-5) years of experience ing with ANSI x12 EDI standards for health care . Solid analytical  with ability to compile data from many sources and formulate plans and recommendations. Excellent data manipulation, communication, analytical and statistical  are . Must be comfortable with SQL, MS Excel, MS Access, MS Word, MS Visio and MS PowerPoint. Must have excellent time management and organizational  with the ability to handle multiple tasks in a timely and accu manner. Ability to  under pressure, adhere to deadlines and know when to escalate information/issues. Must have a high level of self-motivation and with little guidance/supervision. Must be able to  both independently as well as a team participant. Must have outstanding verbal and written communications  with the ability to communicate clearly to all levels of an organization. Must have strong interpersonal . Three (3) plus years health plan operations analysis or related  . Experience with multiple health plan operation functional areas. Knowledge of Medicare, Medicaid, TPA business  is a plus. Experience executing user acceptance testing and triaging test issues. Prior class room or on-the- training experience. Experience mapping business processes. Experience ing on at least one (1) enterprise-wide system implementation (from start to finish).

Req. Certification/Licensure: None.
Employee Value Pro
Prospect Medical Holdings, Inc., is guided by a diverse and highly experienced leadership core. This group maintains the vision that has made Prospect a needed difference-maker in the lives of so many patients today, and many executives contribute to our continued efforts. As a member of our highly effective team of professionals you will receive:

    Company 401K
    Medical, dental, vision insurance
    Paid time-off
    Life insurance

Prospect Medical Systems has a zero-tolerance policy regarding the use of drugs and alcohol. Our Company is committed to maintaining a productive, drug free place that keeps employees and patients safe from harm. For this reason, we require applicants to pass a screening for drug use as a condition of employment. This includes: alcohol, marijuana, cocaine, opiates and methamphetamines.

 Type: Full-time

Experience:

    837 Guidelines: 3 years ()
    Encounter: 3 years ()
    SQL: 3 years ()

Education:

    Bachelor's ()

30+ 
If you require alternative methods of application or screening, you must approach the employer directly to request this as Indeed is not responsible for the employer's application process.

	",Data Analyst,,indeed,Job post
"Systems Data Analyst
DISA Global Solutions59 re - Tustin, CA
 Now
DISA Global Solutions is a fast-paced, growing company focused on providing safety and compliance solutions, which include drug and alcohol testing, background screening, occupational health screening, transportation compliance, and safety and substance abuse training. We are headquartered in Houston, Texas with over 800 employees. DISA has specialized in promoting place health and safety for over 25 years and has a strong market presence in the energy and transportation sector. We are IT innovators and developed one of the most advanced platforms to support our clients? access to data. DISA is highly committed to quality and service excellence.

DISA employees are offered a competitive pay and benefits package and a fun and exciting place to . We have a collaborative and team-oriented culture with numerous opportunities of career advancement. We offer monthly appreciation events and strive to provide a learning environment where each employee is encouraged to participate in continuing education and training.

 Overview: The System Data Analyst interacts with users of AWSI business systems providing support for data needs and mapping out system enhancements based on user feedback. Other duties include data reporting, data importing, and exporting to meet business and customer needs.

A Day in the Life:

    You will support AWSI clients and users in the operation of AWSI business systems.
    You will analyze and document system data of existing or new business partner applications moving to AWSI applications.
    You will complete data reporting, data importing and exporting.
    You may perform quality assurance testing and validation of data.
    You will attend internal and external meetings regarding AWSI business applications, when needed.
    You will participate in -gathering and documentation processes for new system modules and enhancements when needed.
    You will perform pre-release and post-release testing of new software releases.
    You may assist, facilitate, and participate in Integration Testing and User Acceptance Testing (UAT) and Functionality Testing.

Our Ideal Candi:

    Has experience ing with programming team on systems implementations.
    Has experience with Microsoft SQL database and writing SQL database queries.
    Has strong computer  in the areas data analytics and data management.
    Has expertise in Microsoft Excel.
    Has experience in creating/Pulling Reports related to Sales, Operations, Product, etc.
    Has experience with  gathering and testing for new software implementations.
    Has the ability to communicate new concepts and train users in operating new software.
    Has exposure to software and hardware design principles.
    Has superior attention to detail and accuracy.
    Has the ability to think in a clear, logical and organized manner and  under minimal supervision.
    Has strong organizational and time management .
    Has a high school diploma.
    Has 3 years? experience in data analysis and reporting.
    Has a Bachelor?s Degree and 3 years? experience in in data analysis and reporting.

DISA is an equal opportunity at will employer and does not discriminate against any employee or applicant for employment because of age, race, religion, color, disability, sex, sexual orientation or national origin.
9 
If you require alternative methods of application or screening, you must approach the employer directly to request this as Indeed is not responsible for the employer's application process.

	",Data Analyst,,indeed,Job post
"Health Data Analyst - (Behavioral Health)
Windstone Health Services10 re - Santa Ana, CA 92704
$24 - $25 an hour
 Now

BUILDING RELATIONSHIPS THAT MATTER

(www.windstonehealth.com)

Windstone Health Services, Inc. (WHS) provides managed mental health and substance abuse treatment services through contracts with managed care organizations, HMOs, behavioral health carve-out companies, medical groups and public sector agencies.

Windstone has immediate opening for Health Data Analyst.

Summary/:

    The quality assurance / data analyst is responsible for participating in quality assurance and data analytics related to internal metrics and audits in adherence with internal and external standards and regulations.
    Consults with programs to resolve quality, data, and efficiency problems.
    Functions as an information source to programs and departments when special and critical quality issues occur.
    Function in conjunction with managers and quality assurance services, on special department .

 Type/Expected Hours of 

    Full-time; $24-$25/hour
    Monday-Friday; 8:30 - 5:00 or as dictated by supervisor and as business dictates.
    Santa Ana office
    Benefit include: healthcare, dental, vision, 2-week annual accrued vacation, 8 basic holidays

Essential Functions:

    Develop data analytic reports for programs and departments.
    Confirm that internal standards are properly executed.
    Determine overall program quality by conducting and evaluating various data audits; perform QA activities.
    Address nonconformities from previous internal and external audits
    Resolve major quality problems areas as .
    Develop actionable reports when necessary with regard to deficiencies in standards.
    Educate and instruct personnel in recommended standards and practices.
    Consult with program and department management on quality improvement to improve overall efficiency and efficacy.
    Discuss program evaluation with management, make appropriate recommendations and set timetable for improvement and corrections.
    Function as an information source for programs and departments when critical quality issues arise.
    Prepare evaluation reports and submit them to manager, quality assurance services.
    Assist in development of special department  and  in conjunction with manager, quality assurance services.
    Conducting a management review of the quality system.
    Following-up on pending preventive and corrective action plans.
    Insure organizational systems and training is in place.
    Ensuring documentation of data and quality are accu.
    Develop reports in an efficient and meaningful manner.
    Deliver reports on a regular basis.
    Oversee established calibration and testing program.
    Assist the management team and provide QA support as necessary.
    Maintain QA activity files and history.
    Other duties as assigned by CEO

Please note this   is not designed to cover or contain a comprehensive listing of activities, duties or  that are  of the employee for this . Duties,  and activities may change at any time with or without notice.

:

    Bachelor?s degree in a related field, or equivalent.
     in Excel spreadsheets (pivot tables, macros, formulas), or equivalent.
    Experience in quality assurance or equivalent experience in supporting improvement , office support, audits, etc.
    Experience in healthcare, mental health or equivalent.
    A crucial skill will be the ability to ope all of the companies' software systems reliably.
    The ability to  within a team culture.
    Solid computer and software  are  and will be tested.

 Type: Full-time

Salary: $24.00 to $25.00 /hour

Experience:

    data analytics: 2 years ()
    healthcare: 2 years ()
    technical proficiency (Microsoft Office): 3 years ()

Education:

    Bachelor's ()

 :

    One 

Benefits:

    Health insurance
    Dental insurance
    Vision insurance
    Paid time off

This Company Describes Its Culture as:

    Detail-oriented -- quality and precision-focused
    Outcome-oriented -- results-focused with strong performance culture
    Team-oriented -- cooperative and collaborative

Today
If you require alternative methods of application or screening, you must approach the employer directly to request this as Indeed is not responsible for the employer's application process.

	",Data Analyst,,indeed,Job post
"Director of Data Analytics
MobilityWare - Irvine, CA 92602
 Now
Company Summary

Looking for an innovative, creative, passionate, and fun company in OC? Then, let us introduce ourselves. We are MobilityWare and we make fun for a living! Our mobile games have been rocking the app store since its inception and we regularly show up in top lists for most popular games. We recently reached over 350 million downloads across our portfolio of games!

We've been voted 2015, 2016, 2017 & 2018 Best Places to  in Orange County by the Orange County Business Journal and OC Register! Headquartered in Irvine, CA., our flagship game, Solitaire, was released on the day the App Store opened in 2008. Other s include Blackjack, FreeCell, Destination Solitaire and Spider Solitaire, to name a few. Our vision is to create the most exciting mobile games and have fun doing it.

 Summary

MobilityWare is looking for a Director of Data Analytics that will oversee in-game, marketing and monetization analysis for the studio. Data is at the core of our decision making at MobilityWare and in this  you would be a key contributor to the future success of our games. You will be responsible for providing quantitative guidance and leadership to the studio. You will  cross-functionally with other studio leaders to communicate a holistic view into the health of the games and provide predictive insights to continually improve the product. The ideal candi will have significant successful experience building and leading an analytics team, ing with large data sets and making data-driven decisions.

 

    Manage a team of highly talented and inquisitive data analysts helping them to create actionable insights and business intelligent for our rich portfolio of games
    Collabo with peers, engineers, database managers, and business analysts to design and implement stgies to test, vali and communicate data-driven insights
    Set the stgy and roadmap for the Analytics team. Assess high-impact opportunities for the business and partner with other leaders across the Studio to understand their analytical needs and then develop reports/dashboards that meet those needs.
    Create models that predict consumer behaviors and develop segmentation models for driving gaming s and KPIs
     proven methods in ing with divergent data types to explore and extrapolate data-driven insights using advanced, predictive statistical modeling and testing applied to data acquired and cleansed from a range of sources
    Provide guidance and support to business leaders and stakeholders on how best to harness available data in support of critical business needs and goals
    Design experiments, evaluate the quality of assessments and develop continuous improvement roadmap for gaming s
    Partner with our Data Integration Manager to determine best BI tool set for querying, storage and visualization of high-volume data



    5+ Years  Experience in Mobile Gaming including experience with building predictive models
    Advanced degree (or equivalent) in a quantitative field such as Statistics, Math, Economics or in Computer Science with Modeling/Data Science course
    Ability to manage, recruit and scale highly productive analytics teams including managers
    Experience ing with Senior and Executive leadership as a quantitative owner
    Familiarity with relational databases, MS SQL, data transformation, data mining, and ad hoc analysis
    Experience with big data software and techniques a big plus
    Skilled in analyzing data and drawing appropriate conclusions. Able to effectively translate statistical findings to business stgies that positively impact the business and customer.
    Can  with Database and Statistical applications; can automate routine procedures and create  that increase productivity and efficiency.
    Ability to  productively with others in a highly collaborative manner
    High degree of Initiative, Integrity & Curiosity for understanding patterns and behaviors through the use of data
    Advanced MS SQL  ing experience with SQL Server Analysis Services 

Why you should  for us:

    Awesome Perks: free lunch, free snacks, free drink, free Annual Company trip
    Casual place - yes, flip flops and shorts are pretty much an everyday thing
    Growth, growth, growth - we are growing and we would love for you to join us on the ride
    We also want to take care of our team so we've got profit sharing incentives, medical, dental (100% paid), vision (100% paid), Unlimited PTO/Vacation, life insurance (100% paid), and 401K
    Awesome office smack dab in the middle of OC and 2 minutes off the 5 freeway
    Amazing talent: We've got some heavy hitters from the industry who embody both brilliance AND humility

30+ 
If you require alternative methods of application or screening, you must approach the employer directly to request this as Indeed is not responsible for the employer's application process.

	",Data Analyst,,indeed,Job post
"Business Analyst - Retail Channels
BDS Marketing LLC213 re - Irvine, CA
Responded to 75% or more applications in the past 30 days, typically within 2 days.
 Now
ly 

Announcing a rare opportunity to be part of a team pioneering a NEW program with Facebook at BDS Marketing! Partner directly with Facebook to build and analyze AR/VR/Portal retail field marketing data. Help us build the frame and align key retail measurements with brand stgy. The program will consist of a core team who will  directly with Facebook to build and implement various tactics across the globe!

ESSENTIAL FUNCTIONS:

    Develop analytics, reporting, stgic insights, and tactical learnings in support of the client?s products and retail plans
    Mine data and evaluate market intelligence from the field team
    Produce account-specific research deliverables and data analysis
    Summarize results in clear and concise deliverables with effective visualizations of the data
    Maintain and develop metrics-driven dashboards
     closely with cross-functional departments and team members
    Adhere to all  timelines
    Manage and deliver on ad hoc  as needed
    Other tasks as requested by management

EDUCATION AND EXPERIENCE:

    Bachelor?s Degree in a quantitative discipline such as Mathematics, Economics or Operations Research
    3-5 years of relevant  experience leading and conducting research and data analytics 
    Familiarity with retail channels and channel marketing analytics
    Experience in consumer electronics and/or wireless industries 
    Experience interfacing with external clients and interacting with cross-functional teams in multiple geographic areas
    Experience ing with retail POS data to analyze category, brand, or product performance

 AND :

    Knowledge of one of more BI and Reporting  (Tableau, Power BI, etc.)
    Advanced MS Excel  (Analysis ToolPak, Macros, PowerPivot, etc.)
    Proficiency in Microsoft Word and PowerPoint
    Highly analytical and detail-oriented with strong written and verbal communication 
    Ability to anticipate client?s needs and proactively develop solutions to meet them
    Must be highly detail-oriented and organized with excellent analytic and problem-solving abilities
    Ability to effectively manage own time while multitasking between multiple project requests in a dynamic business environment with a variety of internal stakeholders, partners and customers
    Travel up to 2x per month (will vary)

PHYSICAL :

The physical demands described here are representative of those that must be met by an employee to successfully perform the essential functions of this . Reasonable accommodations may be made to enable individuals with disabilities to perform the essential functions

While performing the duties of this , the employee is  to:

    Regularly sit, stand, walk, grasp, talk and/or hear, and drive
    Occasionally carrying and lifting something up to 10 lbs
    Continuous hand/eye coordination and fine manipulation

We will consider for employment all qualified Applicants, including those with Criminal Histories, in a manner consistent with the  of applicable state and local laws, including the City of Los Angeles? Fair Chance Initiative for  Ordinance.

 Type: Full-time

Salary: $70,000.00 to $750,000.00 /year

Experience:

    business analysis: 3 years ()

Education:

    Bachelor's ()

 :

    United States ()

 travel:

    25% ()

Additional Compensation:

    Bonuses

 :

    One 
    Multiple s

Benefits:

    Health insurance
    Dental insurance
    Vision insurance
    Retirement plan
    Paid time off

3 
If you require alternative methods of application or screening, you must approach the employer directly to request this as Indeed is not responsible for the employer's application process.

	",Data Analyst,,indeed,Job post
"Data Analyst
Healthpeak Properties, Inc. - Irvine, CA 92614
 Now

 

Our new Stgic Analytics and Intelligence team is  a hardcore Data Analyst to add value to Healthpeak. The duties include but are not limited to:

    Develop analytical and optimization models/ to solve business challenges and improve efficiency using statistical and machine learning methods
    Perform data analysis, coding and causation/attribution/correlation identifying key insights to help decisions and business stgy
    Understand data from both internal and external sources, and  with functional teams to build/improve data platform and systems
    Design and implement a robust data pipeline, from sourcing, storing, and validating to performing analysis
    Create customized reports summarize key takeaways, make recommendations and propose specific action plans
    Design effective user interface/visualization that translate critical and complex data to a concise and useful format
    Summarize key findings for data providers' (NIC, StratoDem, etc.) monthly/quarterly Webinar and reporting.
    Perform research and ad hoc analysis that impact segments' performance, such as legislation, regulation, flu, etc.
    Stay current with the latest  trends and seek innovation
    Perform all other -related duties as requested

 

    Bachelor's or Master's in Business, Finance, Economics, Statistics, Applied Mathematics, Computer Science, Engineering, or other relevant field
    1+ year full-time or internship experience in analytics, modeling, optimization, and machine learning algorithms
    1+ year full-time or internship experience with data mining and strong knowledge of regression and linear analysis
    Experience with BI / data visualization  and third-party API integration
    Experience in developing, implementing and maintaining database and programs to manage data analysis efforts
    Ability to  in a fast-paced and dynamic environment, and deliver results both quickly and acculy
    Excellent team player with a collaborative, forward-thinking and positive mindset that fosters relationships with various stakeholders
    Effective written & oral communication  and ability to concisely explain methods/reasoning/ conclusions
    Passionate in using  and science to build the next-generation capabilities and platforms
    Interest in ing in real estate, healthcare, and/or investment

sWJVqnofGm
30+ 
If you require alternative methods of application or screening, you must approach the employer directly to request this as Indeed is not responsible for the employer's application process.

	",Data Analyst,,indeed,Job post
"Data Analyst
SEB Professional North America - Tustin, CA 92780
 Now
:
At SEB Professional your big ideas, new thinking and passion for coffee fuel millions of people as they take on their day. We roll up our sleeves to bring joy to coffee drinkers everywhere. Here, it?s not just a cup of coffee, it is countless hands building, thinking, and creating that sparks passion around the world. Join us.

SEB Professional North America is the market leader in automated coffee and espresso solutions. We ope under three brands; Schaerer, WMF, and Wilbur-Curtis, to support the commercial and non-commercial foodservice industry, as well as the grocery and convenience retailing sectors among others. Our customers include Dunkin Donuts, Starbucks and Tim Hortons, to name a few. Our products can be found virtually anywhere coffee can be consumed out of the home.

Duties & :

    Process confidential data and information
    Gene reports from a multiple systems environment.
    Support initiatives for data integrity and normalization in close collaboration with the headquarters.
    Troubleshoot the reporting database environment and reports.
    Partner with SAP team to determine and utilize standard data exports for visual reporting and dashboards.
    Train end users on new reports and dashboards.
    Provide technical expertise on data storage structure, data mining and data cleansing.
    Identify and remediate data quality issues.
    Other tasks as assigned.

:

    Ability to  with stakeholders to assess potential risks
    Ability to translate business  into non-technical, lay terms.
    Demonstd experience in handling large data sets and relational databases.
    Understanding of addressing and metadata standards.
    Experience with Power BI.
    Prior experience in data analysis or related field.
    Bachelor?s degree in data science or related field a plus.

Physical Demands:

    Sitting or standing at a desk while using the phone and computer.
    Some weekend  may be .

 Environment:

    Office environment.
    Ability to travel up to 15% of time.

Compensation & Benefits:
SEB Professional North America provides a competitive compensation package including health and welfare benefits, paid time off and a retirement savings program.
30+ 
If you require alternative methods of application or screening, you must approach the employer directly to request this as Indeed is not responsible for the employer's application process.

	",Data Analyst,,indeed,Job post
"Health Data Analyst
Double M Resources - Brea, CA
 Now

Permanent 
Brea, CA

Summary:
The Healthcare Analyst provides clinical and financial analysis and reporting for existing clients and provides support for pricing new business.

Essential Duties and :

    Acts with honor and integrity, serving as a  model for the company.
    Analyzes prospective and current client utilization data to identify utilization trends, calculate cost of healthcare and identify opportunities to add value to customers.
    s with internal business customers to define report and dashboard  for use in managing day to day operations more efficiently.
    Assists with the QC/Validation of the data  being developed into the data warehouse.
    Valis the completeness of potential client data files using various techniques.
    Extracts and compiles both internal and external data elements used in all aspects of the business from client pricing, medical cost management, operational reporting and general financial analysis as .
    Populates existing pricing model with potential client information and makes suggestions on potential improvements to the model or corresponding presentation/reporting.
    Participates in or conducts presentations to prospective clients regarding pricing methodologies.
    Reconciles capitation payments to eligibility data to ensure monthly payments received are accu.
    Calculates and analyzes monthly capitation payments made to net providers; identifies and investigates unusual fluctuations and determines root cause of potential problems.
    Conducts recurring and ad hoc provider utilization analyses; prepares meaningful, professional reports for health plans.
    Respects and maintains HIPAA confidentiality guidelines.
    Acts as an interdepartmental liaison between offices, departments, and committees.
    Performs other duties as assigned.

Project Management and Execution:

    Coordinate with other departments across the AO such as IM governance & EDW and support conceptualization and development of reporting/analytical methodologies, solutions and initiatives
    Aid in the development of project plans and stgies as well as execute assigned project plans to deliver solutions in a timely manner
    Support collaboration on roadmaps, dependencies, and execution plans across analytical units and with applicable teams in the organization
    Manage the intake requests across the business units; track and monitor the  to ensure critical milestones are met, identify and report/escalate risks to management appropriately; manage stakeholder relations
    Obtain data, direct/execute reporting/analysis, perform interpretations and conclusions, and prepare recommendations.
    Assist in determining business needs by effectively conducting fact-finding inter and leveraging various  and analytical methods and then summarize findings in a coherent manner to develop and propose appropriate solution
    Conduct research studies that include collecting data from various sources, analyzing, trending and presenting the findings and recommendations to management
    Support translation of business  and unstructured business issues into data analytic problems
    Perform overall program evaluations (i.e., planning-implementation-completion/analysis-reporting) and other assessments such as change management assessments, flow optimization assessments, etc.

QUALIFICATION :
EDUCATION and/or EXPERIENCE:

At least a Bachelor's degree with emphasis in Economics, Computer Science, Statistics, Math or Business, or equivalent  experience.

    Minimum of 4-6 years?  experience in an analytical 
    Master degree in a quantitative field .
     experience with line item healthcare claims and PBM information is . OTHER  and ABILITIES:
    An understanding of CPT, ICD-9, Revenue and DRG codes is .
    Must have expensive experience and proficiency leveraging SQL, including loading data, writing up and select statements, and joining tables.
    Experience with Excel spreadsheets and other office productivity applications to  and present analytic results.
    Experience with SAS.
    Experience with statistical analysis  such as SAS Office Analytics desired.
    Experience with business Intelligence  such as SAS Visual Analytics desired.
    Ability to understand and  with a variety of data formats to create charts, graphs, tables, and other visual means of representing key aspects of the data.
    Strong written and verbal communications, including the ability to summarize the results of data analysis for technical and non-technical audiences. Must also be comfortable presenting this information via in person meetings as well as webinar .
    Experience with technical writing .
    Must have strong analytical  with the ability to develop advanced analytic models (e.g. forecasting).
    Must be skilled at prioritizing assignments and ing to ensure all mission critical timeframes and deadlines are met.
    Must be able to  independently and demonst initiative and problem solving  ing within a fast paced start up environment

30+ 
If you require alternative methods of application or screening, you must approach the employer directly to request this as Indeed is not responsible for the employer's application process.

	",Data Analyst,,indeed,Job post
"Business Analyst
Smile Brands459 re - Irvine, CA 92618
 Now
Smile Brands, a Glassdoor Top 100 Best Place to , is  a Business Analyst to join our team. The Business Analyst?s  will be to gathering business , perform data extractions, design and produce reporting and analysis to facilitate decision making, and turn complex data into visualizations and actionable stgies.

Essential Duties & 

    Performing complex data extractions and manipulations, which culminate in the design and production of analysis and reports for company leadership
    Performing data validation to ensure accuracy and appropriateness of analytic models
    Act as a liaison to  across departments to gather data and perform analysis
    Partner with company executive management to define technical data specifications and 
    Preparing executive summary reports and present to management
    May be  to take the lead on some  and analysis and provide guidance and direction to less senior members of the team when necessary
    Able to  independently with minimal supervision

 :

Competencies & 

    Intellectual curiosity: Strong desire to understand ?why? and the ability to conduct analysis to uncover the answer.
    Ability to convert complex analysis into actionable insights: Skilled at making complex information easily understood by business leaders through ?data storytelling? and visualization.
    Attention to detail: Ensure analysis, reports and recommendations are based on accu data.
    Operational & fiscal acumen: Deep understanding of business metrics & drivers, ability to develop forecasts and projections, and demonstd operational and fiscal responsibility.
    Cross-functional collaboration: Demonstd ability to  well with others and collabo across departments.
    Communication: Ability to influence others with data and arguments supported by facts and research.
    Integrity & trust: Ability to earn the confidence and trust of colleagues at all levels within the company.

Education & Experience

    Minimum of Bachelor's Degree, advanced degree 
    3+ years of experience as a business analyst, with emphasis on quantitative reasoning, statistical and analytical modeling, and operations reporting
    3+ years of experience in performing analysis using MS SQL and knowledge of relational databases
    Experience with Tableau or other analysis  is a plus
    Strong Excel  and proficient with MS Office Suite

What We?ll Offer You

    Unparalleled support to grow your career
    A culture that celebs success and diversity
    Benefits and perks for you, your family and even your pets!
    Medical, Dental, Vision, 401K and paid time off for full-time s

Smile Brands Inc. and all relevant Affiliates are s.


DISCLAIMER: Please be aware of suspicious recruiting emails and text messages scams that attempt to collect your personal information. You will never be asked for your driver license or Social Security number during an online interview. Additionally, we do not attempt to collect money, gift cards or request you to purchase equipment on behalf of the company.
30+ 
If you require alternative methods of application or screening, you must approach the employer directly to request this as Indeed is not responsible for the employer's application process.

	",Data Analyst,,indeed,Job post
"Sr Stgic Business Analyst
Niagara Bottling483 re - Diamond Bar, CA 91765


At Niagara, we?re looking for Team Members who want to be part of achieving our mission to provide our customers the highest quality most affordable bottled water.

Consider ing here, if you want to:

     in an entrepreneurial and dynamic environment with a chance to make an impact.
    Develop lasting relationships with great people.
    Have the opportunity to build a satisfying career.

We offer competitive compensation and benefits packages for our Team Members.

Sr Stgic Business Analyst

The Contract Manufacturing team is a division dedicated to leading  that drive both domestic and international expansion of the company. We strive on our strong analytical competencies, detailed knowledge of the product and market, our ability to build and maintain positive relationships and our knowledge of Niagara?s core competencies to help us achieve our goals of identifying the right opportunities for the business.

The Contract Manufacturing Associate Manager is responsible for ing with customers (both internal and external) in building robust financial models, statistical datamining, providing unbiased presentation of data, performing extensive data analysis, and providing analytical insight regarding various opportunities to internal and external holders to help guide stgic decisions on expansion.

    Overall:
        Understands the stgic goals of the team and organization ? ensuring and aligns all  with those goals
        Determines and reports the success or failure of implemented improvements, including the definition of metrics and indicators, and analyzing the results of completed 
        Responsible for developing new and complex product oriented models and stgies
        Building and tailoring models and providing analysis on a bespoke basis to understand how they best  to each situation.
        Thorough understanding of company-wide business operations including, but not limited to manufacturing, engineering, product development, supply chain, finance, quality, etc.
        Extensive interaction with internal and external customers ? developing relationships and ing cohesively in information/data gathering to ensure appropriate details of due diligence in order to properly analyze each opportunity
    Building of Financial Models
    Develop financial and data models to aid in better understanding overall financial performance and value creation with a particular focus on forward looking analyses in the areas of expansion, new product development, special  and / or M&A activities such as analyzing potential target segment/customers, gathering information on growth, competitors, market  opportunities and presenting findings to executive leadership team
    Provide financial expertise, analysis, and guidance in direct support of the company?s leadership team collaborating with team members at all levels of the company.
    Responsible for complex and comprehensive financial activities or functions including ownership, oversight, and support of profitability analysis, business plan development, and other areas relating to financial or managerial accounting and analysis.
    Investigate and explain variances or unexpected results.

    Presentation
        Ability to take complex financial models and deliver findings in a succinct, yet thorough method to all levels throughout the organization
        Create and/or gene necessary reporting and analyses for internal & external business segments, brands, and products in support of stgic decision making
        Design, develop, implement, and maintain daily, weekly, and monthly financial reports that provide management with information used in the decision-making process of various initiatives/.

    Please note this   is not designed to contain a comprehensive list of activities, duties or  that are  of the employee for this . Duties,  and activities may change at any time with or without prior notice.



     :
        6 Years ? Experience in Operations/Business Development Field
        6 Years ? Experience in 
        6 Years ? Experience managing 

     :
        10+ Years? Experience in Operations/Business Development Field
        10+ Years ? Experience ing in 
        10+ Years ? Experience managing 
        Previous experience in M&A?analyzing potential target segment/customers, gathering information on growth, competitors, market  opportunities and presenting findings to executive leadership team(s)
        Advanced Excel 

Education

    Minimum :
        Bachelor's Degree in Business, Operations, Finance, and other related fields or equivalent experience

    :
        MBA in Operations, Finance, Stgy

Any employment agency, person or entity that submits a rsum into this career site or to a  manager does so with the understanding that the applicant's rsum will become the property of Niagara Bottling, LLC. Niagara Bottling, LLC will have the right to hire that applicant at its discretion without any fee owed to the submitting employment agency, person or entity.

Employment agencies that have fee agreements with Niagara Bottling, LLC and have been engaged on a search shall submit rsum to the designated Niagara Bottling, LLC recruiter or, upon , submit rsum into this career site to be eligible for placement fees.
30+ 

	",Data Analyst,,indeed,Job post
"Senior Sales Operations Analyst
Apria Healthcare1,853 re - Lake Forest, CA 92630

With over 300 s across the US, Apria Healthcare?s mission is to improve the quality of life for our 1.8 million patients at home by providing home respiratory services and select medical equipment to help them sleep better, breathe better, heal faster, and thrive longer. Additional information can be found at www.apria.com.

 SUMMARY

This  will be responsible for one or multiple areas related to identifying opportunities, new programs/products/services or expanding core business, assisting in developing sales growth opportunities, monitoring sales trends, analyzing sales data, ing on sales compensation, CRM or territory deployment and/or developing  and improving system efficiencies within the Sales Operations department. This is a more advanced  than an analyst the big difference is ing independently with only some guidance and helping other associates and analyst in their . An ideal candi is highly motivated, problem solver with great , and ability to succinctly communicate results with senior leadership. They are expected to  independently and/or with other associates & analysts in the department.

ESSENTIAL DUTIES AND 

The Sr. Analyst handles more complex process, data and/or  compared to an associate or analyst

Responsible for one or more of the following:
Handle territory optimization & sales deployment utilizing appropriate software . Track, review, and make recommendations regarding territory and quota change requests.  with field leadership to implement changes
 to administer an incentives program (such as calculate payout or rank contest winners) and provide feedback to ensure alignment with company goals. Troubleshooting issues, identifying opportunities for improvement and  with I.T. to up internal sales incentive compensation system
Develop quota-setting methodology/associated  and manage quarterly quota al process
Maintain or up CRM system and collect and analyze data to track sales performance. Evaluate performance metrics and/or make recommendations on metrics to be tracked
Manage the design and production of management reports. Analyze and evaluate data to identify business implications of trends or sales efficiencies
Analyze sales by product, territory or other geographic entities.  with field executives to analyze sales growth opportunities, and to design and implement growth improvement plans
Develop and execute methodology for figuring business potential for new programs/product/service introduction or core business expansion and quantify the potential revenue and earnings contribution to the Company. Use available data and/or research and suggest data sources to help with the analysis
Perform ?drill down? analyses for Sales Operations management and field sales VPs to determine improvement opportunities in one or more of several areas - incentives, CRM metrics, territory deployment, product growth, sales turnover etc. Understand results and implication and communicate to senior leadership
Identify current processes, understand gaps and develop improvement ideas and design business processes for Sales Operations efficiency improvement and/or company-wide implementation
Analyze sales data, external market sizing data, and market  data to evaluate sales staffing efficiency, turnover and/or attainment to goals
Respond to field manager?s requests for information and ensure that reports meet the ongoing business needs of management
Audit reports for accuracy, consistency and completeness
Assist less experienced Sales Operations Associates and Analysts
Performs other duties as .

MINIMUM  

Education and/or Experience

Four-year college degree or equivalent experience is 
Five years of related  experience is .
Experience in finance and/or sales operations is essential.

, KNOWLEDGE AND ABILITIES

Should have passion to learn and  with multiple databases and build models to perform complex analyses
Great problem solving  and ability to take a business problem, understand issues/gaps, break it down into smaller pieces and  on modeling
High sense of ownership & customer service both internally and externally
Natural curiosity and motivation to dig into the details
An eye for continual process improvement
Good communication 
Great quality control

Computer 

Microsoft Excel proficiency 
SQL or similar database experience 

Language 

English (reading, writing, verbal)

Mathematical 

College level mathematical proficiency, with a strong ability to understand, interpret and develop spreadsheet data

 

Education and/or Experience

Advanced degree 
VBA proficiency 

PHYSICAL DEMANDS

This is a stationary  that requires frequent sitting or standing, repetitive wrist motions, grasping, speaking, listening, close vision, color vision, and the ability to adjust focus. It also may require occasional lifting, carrying, walking, climbing, kneeling, bending/stooping, twisting, pulling/pushing, walking, bending, stooping, and reaching above the shoulder. Employees in this  must be physically able to efficiently perform the essential functions of the . Reasonable accommodations will be provided to assist or enable qualified individuals with disabilities to perform the essential functions of the , upon request.

 ENVIRONMENT

 is performed in an office setting with exposure to mode noise.

TRAVEL

Occasional travel as .

OTHER INFORMATION

The essential duties and , physical , and  environment described above are representative of those typically  for this  but may vary depending on staffing and business needs at specific s. The inclusion or omission of a specific duty or physical requirement is, therefore, not determinative of whether that function is essential to a specific individual?s .

Join the Apria team for a rewarding opportunity in healthcare! Competitive pay s with cash incentives and full benefit packages available, including health, dental, vision, disability and 401(k).

Apria Healthcare is committed to  veterans and military spouses.

As an EOE/AA employer, Apria Healthcare is committed to providing all applicants and employees with equal access to employment opportunities, regardless of sex, race, age, color, national origin, disability, pregnancy, religion, genetic information, sexual orientation, transgender status, gender identity, marital status, veteran status, or any other characteristic protected by federal, state, or local law. Apria Healthcare shall abide by the  of 41 CFR 60-300.5(a) and 60-741.5(a). These regulations prohibit discrimination against qualified individuals on the basis of protected veteran status or disability, and require affirmative action by covered prime contractors and subcontractors to employ and advance in employment qualified protected veterans and individuals with disabilities. AA/EOE, M/F/Disability and Vet.
9 

	",Data Analyst,,indeed,Job post
"Business Analyst
Horizontal - Irvine, CA 92618
Contract


:

    The EHR Implementation Specialist will assist in the rollout of EHR and Practice Management  and may function in one or more of the following capacities: training, support for system implementation, flow re-engineering, change management and go live support.
    In partnership with team members and in line with the project plan, the Implementation Specialist will interact directly with providers, managers and office staff members and serve as the main point of contact.
    Monarch HealthCare is led by doctors who are committed to offering patients the most extensive choices and access to Orange County?s leading physicians, hospitals, labs and  care centres. This allows patients to get the perfect care in support of their good health.
    Monarch HealthCare?s independent, private-practice physicians are part of a coordinated net that s together to promote excellence in patient care.

Primary :

    Develop stgic and focused implementation assessment in accordance to assigned 
    Participate as a key member of the core implementation team charged with successfully implementing and supporting an EHR system by becoming a Next Gen Certified Professional and subject matter expert in the functionality of the electronic medical records and electronic practice management systems
    Serve as main point of contact and application subject matter expert for the Physician offices during the implementation process and post implementation process; on site go-live support and post implementation phase, including optimization
    Analyse and document current and proposed clinical flows, as they pertain to the implementation of electronic practice management and electronic medical records software
    Analyse, evaluate and make recommendations and/or alternatives that address existing and potential areas of improvements in physician office operations
    Facilitate the management of end user expectations
    Redesign existing flows to leverage the NextGen application and document, review and train future state flows
    Vali and implement customized training curriculum as necessary Document implementation decisions and configuration approach
    Directly and remotely support the physicians and physician office staff in the use of the software
    Play a key  in the planning, design, development and deployment of new application modules and enhancements to existing applications
    Perform and/or coordinate in-depth tests, including unit and integration testing
    Utilize the ticketing software provided to adequately document issues
    Correctly status and prioritize issues
    Resolve issues directly and remotely utilizing training and provided documentation
    Follow established escalation and notification procedures Provide regular status ups to Director of EHR Implementation and/or project manager for all assigned tasks

 :

    Undergraduate degree or equivalent experience
    4+ years of professional experience ing in a healthcare environment
    2+ years of professional experience ing in a NextGen EPM or EHR implementation 
    2+ years of professional experience with electronic physician's office billing systems
    2+ years of professional experience providing training for adult learners
    2+ year of professional experience ing in an EPM / EHR troubleshooting 
    Experience with issue resolution and / or issue tracking software

Diversity creates a healthier atmosphere:

    Equal Employment Opportunity/Affirmative Action employer and all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, age, national origin, protected veteran status, disability status, sexual orientation, gender identity or expression, marital status, genetic information, or any other characteristic protected by law.
    Candis are  to pass a drug test before beginning employment

#LI-SP1,#LI-HA1

Horizontal is proud to be an Equal Opportunity and Affirmative Action Employer. We seek to provide employment opportunities to talented, qualified candis regardless of race, color, sex/gender including gender identity and/or expression, national origin, religion, sexual orientation, disability, marital status, citizen status, veteran status, or any other protected classification under federal, state or local law.

In addition, Horizontal will provide reasonable accommodations for qualified individuals with disabilities. If you need to request a reasonable accommodation in order to complete the application or interview process, please contact hr@horizontal.com.

All applicants ing must be legally authorized to  in the country of employment.
15 

	",Data Analyst,,indeed,Job post
"Business Analyst / Business Intelligence Analyst (COBOL)
Parker Hannifin Corporation2,542 re - Irvine, CA 92606


    Parker Hannifin
    Founded in 1917, Parker Hannifin Corporation is a $14.3 billion, global company. With annual sales of $14.3 billion in fiscal year 2019, Parker Hannifin is the world's leading diversified manufacturer of motion and control  and systems, providing precision-engineered solutions for a wide variety of mobile, industrial and aerospace markets. The company has operations in 50 countries around the world. Parker has increased its annual dividends paid to holders for 63 consecutive fiscal years, among the top five longest-running dividend-increase records in the S&P 500 index. Parker?s engineering expertise and broad range of core  uniquely s the company to solve some of the world?s greatest engineering challenges. By partnering with customers, Parker improves their productivity and profitability and seeks new ways to solve humanity's biggest challenges.

    Department Marketing Statement
    Parker Aerospace is a global leader in the research, design, integration, manufacture, certification, and lifetime service of flight control, hydraulic, fuel and inerting, fluid conveyance, thermal management, lubrication, and pneumatic systems and components for aerospace and other high- markets. The company supports the world?s aircraft manufacturers, providing a century of experience and innovation for commercial and military aircraft.

    Major Duties
     Summary:
    The key candi will have strong analytical  and currency in mainframe COBOL related system . This COBOL Analyst  will be responsible for support and enhancements to an Aerospace business system that encompasses order entry, manufacturing, shipping, and invoicing in a fast-paced team environment. This  will also provide technical support to the business process owners and cross IT support teams. Specifically, the candi will provide development in COBOL, DB2, JCL, and CICS in an enterprise class IBM environment. Knowledge, experience or background with Manufacturing is a plus. Experience with interacting with IBM MQ series or similar enterprise bus architectures is a plus.
    Experience in Telon is a plus.

    
    Logical analysis of simple to large complex queries and procedures in an IBM COBOL environment
    Support and resolution of complex business system issues
    Team with regional programmers and analyst to understand and resolve regional data definitions and constructs

    
    B.A. /B.S. degree in a related technical discipline or equivalent professional level experience of 10 or more years
    3-5 years of COBOL Mainframe development including recent ing experience with JCL, DB2, and CICS.
    Knowledge of manufacturing processes, finance, information systems, and general business practices.
    Demonstd team in producing results and meeting organizational s
    Able to create and maintain enthusiasm for new and challenging goals
    U.S. Citizen or U.S. Permanent Resident Status  Education and Experience:
    Bachelors degree (BA/BS) in Information Systems, Computer Science, Business Administration.
    Demonstd ability to perform the essential functions of the  typically acquired through ten or more years increasingly responsible related application/infrastructure experience utilizing the company's standard ets.
    Specific assignment may require recognized certification in area of expertise.
    Knowledge of manufacturing processes, finance, information systems, and general business practices.
    Demonstd team in producing results and meeting organizational s
    Able to create and maintain enthusiasm for new and challenging goals
    U.S. Citizen or U.S. Permanent Resident Status  Knowledge,  and Abilities:
    Advanced, specialized knowledge of programming, system design and analysis principles, industry practices, regulations, and overall business operations. Specific area of assignment may require knowledge in specific or multiple products and  such as: technical knowledge of software and hardware selection implementation and testing methodology, client/server application development, architecture, database-connectivity and database design methodology; and, advanced, specialized knowledge of manufacturing and support organizations, business and flow process, and quality methods. Ability to effectively assess, lead, and implement continuous improvement in critical and/or high-impact areas. Knowledge of project management standards, methods and techniques. Ability to  on complex problems where in-depth analysis and evaluation of situations or data frequently require complicated negotiations. Ability to solve a wide range of unique problems of considerable scope and complexity that may require new approaches or significant modification of standard procedures. Ability to establish goals and s to complete . Ability to develop improved methods and systems. Ability to communicate effectively with users, project teams, and management. Ability to analyze and interpret business periodicals and detailed technical manuals and procedures. Ability to respond to significant inquiries from users. Ability to define problems, collect data, establish facts, and draw valid conclusions. Ability to effectively demonst team member competencies and participate in goal setting, performance feedback, and self-development activities.

    Equal Employment Opportunity
    Parker is an Equal Opportunity and Affirmative Action Employer. Parker is committed to ensuring equal employment opportunities for all  applicants and employees. Employment decisions are based upon  related reasons regardless of race, ethnicity, color, religion, sex, sexual orientation, age, national origin, disability, gender identity, genetic information, veteran status, or any other status protected by law. U.S. Citizenship/Permanent Resident is  for most s. (?Minority/Female/Disability/Veteran/VEVRAA Federal Contractor?) If you would like more information about Equal Employment Opportunity as an applicant under the law, please go to http://www.eeoc.gov/employers/upload/eeoc_self_print_poster.pdf and http://www1.eeoc.gov/employers/upload/eeoc_gina_supplement.pdf #LI-JC1 #CB

30+ 

	",Data Analyst,,indeed,Job post
"Business Operations Analyst Sr
QTC Management, Inc.178 re - San Dimas, CA 91773


 Summary

s in business operations organization supporting program/product areas. Performs activities that drive efficient planning and execution of enterprise financial and resource investments and efforts across various functions, including but not limited to operations, provider recruitment, program management, sustainment, business finance, and/or contracts.

Essential Duties and 

    Compile and extract data from various sources to present to internal and external business partners
    Gene and distribute various operational reports on a daily, monthly, quarterly and annual basis
    Communicate analysis results and supporting data to Senior Management Team
    Assist with report enhancements and modifications, partnering with other departments as needed
    Provide training to external clients on QTC proprietary applications(s) as necessary
    Participate in user acceptance testing (UAT) in support of  systems/changes
    Perform other duties and  as assigned

Competencies

    Solid analytical, data analysis, problem-solving and presentation 
    Ability to utilize various project management methodologies and  to coordinate and manage 
    Ability to quickly foster relationships and credibility at all levels of the organization

Education and/or Experience (includes certificate & licenses)

    Advanced/Mastery experience with Excel and Tableau a plus
    Proficiency in MS CRM Dynamics is .

This   supersedes all prior  s and is intended to describe the general content and essential  for the  listed above. It is not to be construed as an exhaustive statement of , duties and . Management reserves the right to add or change the duties of this  as  at any time.

QTC Management Inc. is a VEVRAA Federal contractor and an . The company has an ongoing commitment to affirmative action and the creation of a place free of discrimination, harassment and retaliation. The company recruits, hires, trains, and promotes individuals in all  s without regard to race, color, creed, religion, ancestry, national origin, age, sex, sexual orientation, people with disabilities protected under law, and protected veteran status.

IND1

Experience


    6 - 10 years: Business Operations Analyst, or related

Education


    Bachelors or better




    Project Management
    MS Office
    Customer Service
    Sense of Urgency
    Verbal Communication
    Written Communication
    Excel
    Analytical

Behaviors


    Team Player: s well as a member of a group
    Dedicated: Devoted to a task or purpose with loyalty or integrity
    Detail Oriented: Capable of carrying out a given task with all details necessary to get the task done well

Motivations


    Flexibility: Inspired to perform well when granted the ability to set your own schedule and goals
    Goal Completion: Inspired to perform well by the completion of tasks
    Growth Opportunities: Inspired to perform well by the chance to take on more responsibility
    Self-Starter: Inspired to perform without outside help

/Protected Veterans/Individuals with Disabilities

The contractor will not discharge or in any other manner discriminate against employees or applicants because they have inquired about, discussed, or disclosed their own pay or the pay of another employee or applicant. However, employees who have access to the compensation information of other employees or applicants as a part of their essential  functions cannot disclose the pay of other employees or applicants to individuals who do not otherwise have access to compensation information, unless the disclosure is (a) in response to a formal complaint or charge, (b) in furtherance of an investigation, proceeding, hearing, or action, including an investigation conducted by the employer, or (c) consistent with the contractor?s legal duty to furnish information. 41 CFR 60-1.35(c)
30+ 

	",Data Analyst,,indeed,Job post
"Senior Data Analyst
Kelly Services14,506 re - Irvine, CA 92618


Kelly Outsourcing & Consulting Group (KellyOCG), a managed solution provider and business unit of Kelly Services, Inc., is   an Sr. Data Analyst for a long-term engagement at one of our Global clients in Irvine, CA.

This  is a full-time, fully benefitted .

As a KellyOCG employee you will be eligible for Medical, Dental, 401K and a variety of other benefits to choose from. You?ll also be eligible for paid time off, including holiday, vacation, and sick/personal time. All KellyOCG employees receive annual performance re.

 :

    With supervision,  is responsible for analyzing industry, customer, and competitive insights within the Global Business Insights team (GBII.
     is responsible for executing primary and secondary research, identifying key insights, and building executive level presentation slides to communicate insights.
     will maintain and refresh market and  reporting processes and will be responsible for quality control, data validation, data coding, IT relationship management, and enhancing process efficiency.
     will require broad cross-functional neting, proactive dashboard creation and development, and strong presentation .
    With supervision,  will  with the Business Unit teams to develop stgic market insights, including but not limited to opportunity identification, competitor pipeline, commercial stgy, tactical and stgic plans, and marketplace conditions and  scenarios.
    With supervision, design and execute primary research studies to deepen market understanding and drive business stgy.

 Duties and :

    With supervision, synthesize data from various sources to form recommendations and actionable insights
    Utilize Excel, Tableau to identify and analyze data to support key organizational stgies
    Integ and analyze large datasets and ensure high quality of data
    Develop and maintain MS Excel and Tableau dataset.
    Develop and maintain internal Point data libraries and communication materials
    With direction, demonst an ability to implement competitive insight and analysis  in support of assigned business unit / region.
    Develop and execute primary research studies

 Education and/or Experience:

    Bachelor?s degree and 2 years of experience in Analyst/Sr. Analyst or similar 
    Advanced Excel  are , and a competency test will be administered. Advanced knowledge of V-Lookup, Formulas and Formatting .
    Proficiency with database management  and presentation of data analysis and visualization tool ? Tableau software is 
    Strong quantitative and analytic abilities to analyze and vali data
    Excellent computer  (Word, PowerPoint, Point)
    Experience in the medical device, hospital specialty, or healthcare field is  but not 
    Ability to  in a matrix environment where a high degree of collaboration is needed
    Demonstd success in creative problem solving and team partnership is .
    Due to the nature of information reviewed in the , candi must demonst a high level of discretion and confidentiality
    Strong project management 
    Primary research experience  but not 
    Ability to organize and present information to internal business partner
    **Once candi has an established track record of performance, client is supportive of ing from home 1-2 days per week (or more) as needed

Important information: This  is recruited for by a remote Kelly office, not your local Kelly branch. Applicants must be legally permitted to  in the United States immediately and without employer sponsorship (F1 OPT/H1B cannot be considered for this ).



Why Kelly??

As a er today, it?s up to you to take charge of your career and look for opportunities to learn, grow, and achieve your potential. Helping you find what?s next is what we?re all about. We know what?s going on in the evolving world of ?just ask the nearly 500,000 people we employ each year. Connecting with us means getting the support, guidance, and opportunities needed to take your career where you may have never imagined.

About Kelly?

At Kelly, we?re always thinking about what?s next and advising  seekers on new ways of ing to reach their full potential. In fact, we?re a leading advocate for temporary/nontraditional styles, because we believe they allow flexibility and tremendous growth opportunities that enable a better way to  and live. Connecting great people with great companies is what we do best, and our employment opportunities span a wide variety of styles, skill levels, and industries around the world.

Kelly is an  committed to employing a diverse force, including, but not limited to, minorities, females, individuals with disabilities, protected veterans, sexual orientation, gender identity. Equal Employment Opportunity is The Law.
17 

	",Data Analyst,,indeed,Job post
"Sr Financial Analyst
Apria Healthcare1,853 re - Lake Forest, CA 92630

ABOUT THE COMPANY

Apria Healthcare?s mission is to improve the quality of life for our patients at home. We are looking for empathetic, thoughtful and compassionate people, to meet the needs of our patients. Already an industry leader in healthcare services, we provide home respiratory services and select medical equipment to help our patients sleep better, breathe better, heal faster, and thrive longer.

 SUMMARY

The Sr. Financial analyst creates and maintains various financial models and reports and manages all aspects of TM1 reports, versions and data integrity. The Sr. Financial analyst will have a sense of ownership of process and results, creativity, be well versed in SAAS reporting and visualization , and be extremely organized.

ESSENTIAL DUTIES AND 

Coordinates with all levels of management to gather  for TM1 reporting needs
Produce monthly reporting packages with stgic, forward-thinking insights for respective functional area
Conduct ?what-if? and sensitivity analysis to support modeling activities
Create a very organized structure around report change management, creation and validation
Report development and maintenance ownership. Create and maintain consistent online reporting environment
Create and maintain TM1 web user interfaces
Gather and define  around dimension changes/hierarchy/attributes/calculations as it relates to FP&A
 directly with TM1 developers to implement enhancements, data requests, and all other metadata changes
Manage archiving of versions, creating new versions (both static and dynamic) & managing version tracking and control
Manage the process around data loading/importing, data movement and data validation across cubes as it relates to budgeting and forecasting
Assist with annual budget and rolling monthly forecast process
Evaluate the efficiencies of current data loads and reporting processes
Develop complex user interfaces for budgeting and forecast
Oversee and manage the continued development of Forecasting and Modeling 
Enhance and expand the use of data at all levels within the organization through appropriate business intelligence and reporting systems
Ensure data accuracy and integrity
Performs other duties as 

SUPERVISORY 

N/A

MINIMUM  

Education and/or Experience

Four-year college degree is 
A major in Finance, Computer Science or Business administration 
4 years overall experience with business, finance, or analytics-focused environment; experience with TM1

, KNOWLEDGE AND ABILITIES

Cognos BI, Cognos TM1, Hyperion Essbase or comparable experience 
Understanding of Financial concepts including Income Statement structure, P&L terminology, and basic financial calculations. Strong mathematical capabilities. Strong Excel , especially the ability to  with complex formulas
Superior problem-solving 
Ability to adapt to changes in priorities, circumstances, and direction
Strong data analysis  . Superior financial modeling, Excel and PowerPoint  
Ability to tear apart a complex problem, understand what is going on, put it all back together and ultimately communicate succinctly with senior leadership
Ability to communicate and  cross-functionally with all areas of Finance
An eye for continual process improvement
Ability to see the big picture and see things from a stgic perspective
Ability to present information, analysis and financial data in a clear, concise manner to non-financial managers and senior leadership
Demonstd history of identifying and resolving a wide range of issues in imaginative and practical ways, as well as exercising good judgment in selecting methods and techniques to obtain solutions
Demonstd strength ing cross functionally to drive issue resolution, results driven, problem solver, self-starter
Self-starter with strong organizational  and attention to detail
Ability to  proactively, independently and in a team environment
High attention to detail with an eagerness to review and analyze data
Ability to handle multiple tasks under tight deadlines
Computer 

Proficiency with Microsoft Office, including advanced proficiency with MS Excel and preferably MS PowerPoint
Experience with budget/forecast systems (Hyperion, TM1)
Data visualization  like Domo, Tableau, Power BI a plus
Language 

English (reading, writing, verbal)

Mathematical 

College level mathematical proficiency, with a strong ability to understand, interpret and develop spreadsheet data
 

Certificates, Licenses, Registrations or Professional Designations

Designation such as MBA, CPA or CMA a plus
, KNOWLEDGE AND ABILITIES

Healthcare background a plus
Computer 

Computer savvy with: MS Excel, MS Office and a variety of business intelligence .
PHYSICAL DEMANDS

This is a stationary  that requires frequent sitting or standing, repetitive wrist motions, grasping, speaking, listening, close vision, color vision, and the ability to adjust focus. It also may require occasional lifting, carrying, walking, climbing, kneeling, bending/stooping, twisting, pulling/pushing, walking, bending, stooping, and reaching above the shoulder. Employees in this  must be physically able to efficiently perform the essential functions of the . Reasonable accommodations will be provided to assist or enable qualified individuals with disabilities to perform the essential functions of the , upon request.

 ENVIRONMENT

 is performed in an office setting with exposure to low noise.

OTHER INFORMATION

The essential duties and , physical , and  environment described above are representative of those typically  for this  but may vary depending on staffing and business needs at specific s. The inclusion or omission of a specific duty or physical requirement is, therefore, not determinative of whether that function is essential to a specific individual?s .

Join the Apria team for a rewarding opportunity in healthcare! Competitive pay s with cash incentives and full benefit packages available, including health, dental, vision, disability and 401(k).

Apria Healthcare is committed to  veterans and military spouses.

As an EOE/AA employer, Apria Healthcare is committed to providing all applicants and employees with equal access to employment opportunities, regardless of sex, race, age, color, national origin, disability, pregnancy, religion, genetic information, sexual orientation, transgender status, gender identity, marital status, veteran status, or any other characteristic protected by federal, state, or local law. Apria Healthcare shall abide by the  of 41 CFR 60-300.5(a) and 60-741.5(a). These regulations prohibit discrimination against qualified individuals on the basis of protected veteran status or disability, and require affirmative action by covered prime contractors and subcontractors to employ and advance in employment qualified protected veterans and individuals with disabilities. AA/EOE, M/F/Disability and Vet
30+ 

	",Data Analyst,,indeed,Job post
"Senior Healthcare Data Analyst
Advantmed98 re - Santa Ana, CA
 Now

 

The Data Analyst will be responsible for data interpretation, providing business and technical guidance and recommendations to internal and external stakeholders.  will be remote but does require travel to the corpo office in Orange County, CA.


Duties and 

    Perform data analytics to ensure data accuracy. (business and technical compliance).
    Performed analytics using government program or other internal/external data.
    Responsible to stage internal/external data and present any exceptions or findings to stakeholders.
    Responsible for Data Mining, Mapping, Analysis, Transforming, Cleaning, Manipulating and Reporting throughout all form of Healthcare data.
    Identify data trends and patterns and present findings.
    Produce summaries of key data populations.
    Responsible for producing final data mapping documentation and .
    Provide guidance to Data Operation Dept.
    Responsible for support and answer business questions and provide guidance to internal teams and/or customers.



Minimum of a BA/BS degree -  but not  in Mathematics, Computer Engineering, IT, or related fields. Comparable experience will be considered for qualified candis

    5+ years' of experience in Data Mining, Mapping, Analysis, Transforming, Cleaning, Manipulating and Reporting. 3+ years' of experience in Healthcare Data Analyst  including familiarity with Claims and Encounters, Rx-Claims, Eligibility, CMS data like MMR, MOR.
    A strong candi would be the one who can take raw healthcare data and convert it into a actionable business decision making information.
    3+ years of experience in HEDIS and Star Measures, Risk Adjustment Experience is .
    Client/Executive Facing Experience is .
    5+ years of experience in Microsoft SQL queries including ability to write complex queries to process and view data.
    The Candi must have strong attention to detail, excellent oral and written .
    The Candi must be able to collabo in the team effectively with direct and indirect reports.
    The Candi should be able to take initiative and proactively offer suggestions and resolutions.
    The Candi must be proficient in MS SQL Server platform, preferably in Management Studio, must have handson experience in transforming, cleaning, and manipulating raw data and have hands on experience with Microsoft SQL including ability to write complex queries to process and view data.
    Experience with Business Intelligence Tool will be a plus.

ing conditions

A qualified candi will be able to lead these efforts from any geographic  in the United States. The Advantmed corpo office is in Santa Ana, CA, easy access to this  will be important.

Compensation

    Dependent on experience
    Bonus Opportunity is 10% annually
    Health, Dental, Vision, benefits
    Optional 401k Profit Sharing Plan.

 Type: Full-time

1eaz0d0sg0
30+ 
If you require alternative methods of application or screening, you must approach the employer directly to request this as Indeed is not responsible for the employer's application process.

	",Data Analyst,,indeed,Job post
"Lead Data Analyst
Auction.com115 re - Irvine, CA 92618
 Now
Auction.com is the nation?s leading online real estate marketplace focused exclusively on the sale of residential bank-owned and foreclosure properties via online auctions and live trustee sale events. By offering access to exclusive properties and  designed to seamlessly connect buyers and sellers, Auction.com empowers residential real estate investors and financial institutions to achieve optimal, mutually beneficial results ? to go beyond the bid.

The Lead Data Analyst will conduct complex performance and opportunity assessments to inform decision-making, policy development, system development, and management. This  will partner closely with executive leadership to facilitate financial modeling, root cause analysis, ad hoc, and what-if scenarios.  includes but is not limited to revenue forecasting, market sizing, return on investment analysis, operational performance re and effectively translate data to non-technical business partners. Influence, provide leadership, and guidance over a team of Data Analysts.

/Duties:

    Analyzes complex business problems and issues using data from internal and external sources to provide insight to decision-makers.
    Identifies and interprets trends and patterns in datasets to locate influences.
    Constructs market assessments, forecasts, recommendations and stgic/tactical plans based on business data and market knowledge.
    Manage multiple simultaneous tasks and priorities.
    Build presentations for audiences including executive leaders.
    Deliver presentations to executive team.
    Provide consultation to users and lead cross-functional teams to address business issues.
    May directly produce data sets and reports for analysis using system reporting 
    Implement and utilize predictive and statistical models to measure and inform on Auction.com?s marketplace.Day-to-day coordination, engagement and dotted line responsibility of Data Analysts.

Knowledge,  and Abilities:

    Advanced problem solving , must demonst analytical and critical thinking.
    Advanced time management .
    Strong ability to  in a fluid and changing environment.
    Ability to thrive in a fast paced environment while managing multiple, complex issues and competing priorities simultaneously, and collabo cross-functionally.
    Proven communicator, ability to present to executive level leaders in all communication forms.
    Proficient using the Microsoft Office Suite (Advanced MS-Excel ).
    Seasoned with Structured Query Language (SQL), preferably Microsoft SQL.
    Strong finance and accounting knowledge.
    Strong math , solid statistics background a definite plus.
    Strong stakeholder relationship management .
    Experience ing with and wrangling data from multiple data sources to develop a single process.

Education/Experience:
Bachelor?s degree in Finance, Economics, Computer Science, Information , a quantitative discipline or equivalent  experience
Minimum 4 to 7 years' experience in Analytics measuring and communicating business performance.
Advanced  with SQL, Data Visualization  such as Tableau or Power BI, Microsoft Office Suite, Statistical Programming software such as R or SAS.

 :
Additional programming experience with Hadoop, Big Query, Python
Experience ing in Real Estate analytics, valuation modeling, marketplace analytics, behavioral analytics, and/or auction & game theory.

To all recruitment agencies: Auction.com does not accept agency resumes unless you are part of our  partner net. Please do not forward resumes to our s alias, Auction.com employees or any other company . Auction.com is not responsible for any fees related to unsolicited resumes.
16 
If you require alternative methods of application or screening, you must approach the employer directly to request this as Indeed is not responsible for the employer's application process.

	",Data Analyst,,indeed,Job post
"Senior Financial Data Analyst
Alignment Healthcare64 re - Orange, CA 92868
 Now

Sr. Financial Data Analyst

The Sr. Financial Data Analyst will review, analyze and report on data from the Alignment Provider Management Report (APMR), which includes but is not limited to medical claims data, utilization data, pharmacy data and other sources such as general ledger postings, accounting data, etc. This will include the design and development of ad hoc reports to support Provider Contracting and Net Development, analysis of product line profitability, gross margin, and expansion market analytic support.

Another focus area will be creating, managing and distributing monthly, quarterly and annual risk pool exhibits to delegated entities/IPAs. This involves cross functional  with departments such as net management, claims, utilization management, provider configuration, IT, etc. in interpreting contracts, updating claims data to factor in exclusions for out of area services, transplants, etc. and assisting delegated entities as well as internal stakeholders in risk pool changes over time (identifying key drivers of risk pool surplus or deficits).

General Duties/:

(May include but are not limited to)

    Develop and manage Alignment Provider Management Report (APMR) for current and expansion markets.
     with Alignment IT department to assist in the programming and mapping of various financial and key performance indicators within APMR
     with Alignment Accounting staff to reconcile and verify the integrity of APMR yearly and monthly financial data to the General Ledger
     with Alignment FP&A team to incorpo budget data and assumptions into APMR.
     with Alignment Claims department to vali the integrity of claim payment within APMR
     with Alignment Provider Contracting and Net Development staff to analyze contract  trending impact and anomalies.
     with Alignment UM and Clinical operations department to vali and reconcile key  admit and bed day metrics to actual paid claim data.
    Support Regional VP?s in providing analysis of monthly APMR data and ad hoc reporting of key performance indicators.
    Support finance and operations in developing predictive KPI?s to better forecast and manage the business of Alignment.
     with all levels of staff to direct, assist, and explain the APMR process as needed.
    Reading, interpreting and clarifying delegated entity/IPA contracts with regards to risk pool definitions for revenue, expense, carve outs and calculations
    Making ups to risk pool calculations and definitions for new delegated entities, as well as during contract amendments for existing entities
    Create, up, analyze and distribute monthly, quarterly and annual risk pool exhibits to internal stakeholders as well as delegated entities [FTP/SFTP]
    Analyze key drivers of risk pool surplus or deficits; communicate these drivers to internal stakeholders
    Demonst commitment and attendance sufficient to complete the duties of the  as .
    Perform related duties similar to the above in scope and function as .

:

    Minimum Experience:

    Experience in financial analysis and ing with data, preferably ing with healthcare data, either from educational study or professional experience
    At least a foundational level of knowledge of Access, SQL and/or other database programs
    At least an intermediate level of proficiency with MS Excel (Pivot tables, VLOOKUP, etc.)

    Education/Licensure:

    Bachelor of Science  * Other:

    Healthcare experience .
    Knowledge of risk pools  (hospital funding, claims exclusions based on DOFRs, surplus and deficit carry-overs, etc.) .
    Knowledge of principles and practices of finance administration in accounting, budgeting, and auditing.
    Understanding of CMS Premium payment methodologies to MA Health Plans. Understanding MMR documentation, premium payment calculations, Risk adjustment Factors (RAF), other premium adjustments, and file layout.
    Understanding of CMS provider payment methodologies (DRG, RBRVS, etc.)
    Understanding of operations, services and activities within a Data Warehouse environment (Claim Data, Membership, Eligibility, and Revenue Data, etc.)
    Understanding provider capitation contracts, development of capitation s, contract carve outs, performed comparative analysis to Fee For Service.
    Exposure and ability to use Actuarial studies related to IBNR and LDS
    Understanding of provider contracts for medical groups, independent physicians, and hospitals.
    Understanding of payment methodologies for Hospitals (DRG, Per Diem, % of Billed, etc.) and Physicians (RBRVS, FFS, Capitation, etc.) and other ancillary providers.
    Develop new analysis and approaches to the use of data that allow fresh insights into the business of the company
    Be able to  to goals with low level of supervision
    Communicate clearly and concisely, both orally and in writing.
    Establish and maintain effective ing relationships with those contacted in the course of .
    Maintain effective audio-visual discrimination and perception needed for making observations; communicating with others; reading and writing; and operating assigned equipment.
    Maintain mental capacity which allows the capability of making sound decisions and demonstrating intellectual capabilities.

Essential Physical Functions:

The physical demands described here are representative of those that must be met by an employee to successfully perform the essential functions of this . Reasonable accommodations may be made to enable individuals with disabilities to perform the essential functions.

    While performing the duties of this , the employee is regularly  to talk or hear. The employee regularly is  to stand, walk, sit, use hand to finger, handle or feel objects, , or controls; and reach with hands and arms.
    The employee frequently lifts and/or moves up to 10 pounds. Specific vision abilities  by this  include close vision and the ability to adjust focus.

Internal :

Sr. Financial Data Analyst

The Sr. Financial Data Analyst will review, analyze and report on data from the Alignment Provider Management Report (APMR), which includes but is not limited to medical claims data, utilization data, pharmacy data and other sources such as general ledger postings, accounting data, etc. This will include the design and development of ad hoc reports to support Provider Contracting and Net Development, analysis of product line profitability, gross margin, and expansion market analytic support.

Another focus area will be creating, managing and distributing monthly, quarterly and annual risk pool exhibits to delegated entities/IPAs. This involves cross functional  with departments such as net management, claims, utilization management, provider configuration, IT, etc. in interpreting contracts, updating claims data to factor in exclusions for out of area services, transplants, etc. and assisting delegated entities as well as internal stakeholders in risk pool changes over time (identifying key drivers of risk pool surplus or deficits).

General Duties/:

(May include but are not limited to)

    Develop and manage Alignment Provider Management Report (APMR) for current and expansion markets.
     with Alignment IT department to assist in the programming and mapping of various financial and key performance indicators within APMR
     with Alignment Accounting staff to reconcile and verify the integrity of APMR yearly and monthly financial data to the General Ledger
     with Alignment FP&A team to incorpo budget data and assumptions into APMR.
     with Alignment Claims department to vali the integrity of claim payment within APMR
     with Alignment Provider Contracting and Net Development staff to analyze contract  trending impact and anomalies.
     with Alignment UM and Clinical operations department to vali and reconcile key  admit and bed day metrics to actual paid claim data.
    Support Regional VP?s in providing analysis of monthly APMR data and ad hoc reporting of key performance indicators.
    Support finance and operations in developing predictive KPI?s to better forecast and manage the business of Alignment.
     with all levels of staff to direct, assist, and explain the APMR process as needed.
    Reading, interpreting and clarifying delegated entity/IPA contracts with regards to risk pool definitions for revenue, expense, carve outs and calculations
    Making ups to risk pool calculations and definitions for new delegated entities, as well as during contract amendments for existing entities
    Create, up, analyze and distribute monthly, quarterly and annual risk pool exhibits to internal stakeholders as well as delegated entities [FTP/SFTP]
    Analyze key drivers of risk pool surplus or deficits; communicate these drivers to internal stakeholders
    Demonst commitment and attendance sufficient to complete the duties of the  as .
    Perform related duties similar to the above in scope and function as .

:

    Minimum Experience:

    Experience in financial analysis and ing with data, preferably ing with healthcare data, either from educational study or professional experience
    At least a foundational level of knowledge of Access, SQL and/or other database programs
    At least an intermediate level of proficiency with MS Excel (Pivot tables, VLOOKUP, etc.)

    Education/Licensure:

    Bachelor of Science  * Other:

    Healthcare experience .
    Knowledge of risk pools  (hospital funding, claims exclusions based on DOFRs, surplus and deficit carry-overs, etc.) .
    Knowledge of principles and practices of finance administration in accounting, budgeting, and auditing.
    Understanding of CMS Premium payment methodologies to MA Health Plans. Understanding MMR documentation, premium payment calculations, Risk adjustment Factors (RAF), other premium adjustments, and file layout.
    Understanding of CMS provider payment methodologies (DRG, RBRVS, etc.)
    Understanding of operations, services and activities within a Data Warehouse environment (Claim Data, Membership, Eligibility, and Revenue Data, etc.)
    Understanding provider capitation contracts, development of capitation s, contract carve outs, performed comparative analysis to Fee For Service.
    Exposure and ability to use Actuarial studies related to IBNR and LDS
    Understanding of provider contracts for medical groups, independent physicians, and hospitals.
    Understanding of payment methodologies for Hospitals (DRG, Per Diem, % of Billed, etc.) and Physicians (RBRVS, FFS, Capitation, etc.) and other ancillary providers.
    Develop new analysis and approaches to the use of data that allow fresh insights into the business of the company
    Be able to  to goals with low level of supervision
    Communicate clearly and concisely, both orally and in writing.
    Establish and maintain effective ing relationships with those contacted in the course of .
    Maintain effective audio-visual discrimination and perception needed for making observations; communicating with others; reading and writing; and operating assigned equipment.
    Maintain mental capacity which allows the capability of making sound decisions and demonstrating intellectual capabilities.

Essential Physical Functions:

The physical demands described here are representative of those that must be met by an employee to successfully perform the essential functions of this . Reasonable accommodations may be made to enable individuals with disabilities to perform the essential functions.

    While performing the duties of this , the employee is regularly  to talk or hear. The employee regularly is  to stand, walk, sit, use hand to finger, handle or feel objects, , or controls; and reach with hands and arms.
    The employee frequently lifts and/or moves up to 10 pounds. Specific vision abilities  by this  include close vision and the ability to adjust focus.

 Type: Full-time
20 days ago
If you require alternative methods of application or screening, you must approach the employer directly to request this as Indeed is not responsible for the employer's application process.
report 
	",Data Analyst,,indeed,Job post
"Data Analyst
Nexlogica - Irvine, CA 92618
 Now

Data Analyst

Do you want to drive the conversation on how an organization should think about opportunities and then influence using data to make a difference?

Do you want to influence the success of your customers ?

Do you enjoy developing deep domain knowledge and being responsible to  across the organization?

We are looking for a customer obsessed, data driven detail oriented Business Analyst for its growing team. The team builds products to help customers quickly resolve their issues. The Business Analyst is responsible for driving deep insights about users behaviors and driving continuous improvement using the analysis. The person should have the ability to quickly get to the root cause of a particular business issue, and draft solutions to meet  or resolve the root problems. In this , you will succeed by bringing clarity out of ambiguity - getting the data and analyzing it so the organization can improve the user experience.

Key Result Areas Include:

    Analysis of Machine Learning predictions, impact, and identifying opportunities for improvement
    Understand trends and recommending stgies to stakeholders to help drive business growth.
    Design and analyze A/B tests.
    Inform the larger team of analysis results and provide recommendations for action.
    Own the design, development, and maintenance of ongoing metrics, reports, analyses, dashboards, etc. to drive key business decisions.
    Identify, develop, manage, and execute analyses to uncover areas of opportunity and present written business recommendations that will help shape the direction of the business.
    Respond with urgency to high priority requests from senior leaders.
    Ensure data accuracy by validating data for new and existing .
    Learn and understand a broad range of our customers data resources and know how, when, and which to use.

Basic :

    BA in related field
    4-6 years of analytics experience
    Experience utilizing SQL
    Experience in creating regression models or predictive models.
    Deadline driven, team player, with strong customer focus
    Outstanding analytical, problem solving, and organizational 
    Must be detail-oriented with a demonstd ability to self-motivate and follow through on issues
    Experience in a highly analytical, results-oriented environment with cross functional interactions. Strong analytical, mediation and problem resolution 
    Excellent written and oral communication 
    Attention to detail and capability to  on multiple  in parallel
    Proficiency in Excel, familiarity with relational databases

 :

    5+ years relevant  experience
    Experience with variety of industries (e.g. Banking & Finance, Legal , Retail, Professional Services, Public sector etc.) online advertising or e-commerce
    Experience in designing and implementing custom reporting using automation 
    Experience with statistical analysis, regression modeling and forecasting, time series analysis, data mining, financial analysis, demand modeling, and personalization.
    Knowledge of Python/R/or similar software packages used in ML and data analysis
    Experience in operations and financial analysis
    Experience building data and decision support systems

Ability to think adaptively and ope quickly in a dynamic financial and business environment

Contact
Muhammad Bajwa
careers@nexlogica.com


Last Upd: 10/07/19
30+ 
If you require alternative methods of application or screening, you must approach the employer directly to request this as Indeed is not responsible for the employer's application process.

	",Data Analyst,,indeed,Job post
"Sr. Business Analyst
UC Innovation - Irvine, CA 92602
 Now


     with client and project team to perform  gathering.
    Document current and future state business processes.
    Perform high level GAP analysis to map customer?s  and environment to packaged CRM solutions, like Salesforce and Oracle Siebel CRM.
    Build rapport and develop effective ing relationships with clients.
    Recommend alternative solution choices, determine trade-offs and perform impact analysis as .
    Provide guidance and best practices around Salesforce solutions.
    Provide quality deliverables and  products.
    Assist in presales activities as schedule permits.
    Obtain and maintain  technical certifications related to packaged applications used.
    Analyze  using documentation and  tracking  like RequisitePro or point and Visio.
    Design solutions using configuration and development  like Apex, Visualforce, Process Builder, SQL and SOQL, HTML.
    Perform cleansing and mass ups using the company?s proprietary  like Cloud Explorer and Rubrik.
     with Siebel, Salesforce, SQL and SOQL.



    Master?s degree in Computer Science, Engineering or a related field plus 2 years of experience. In lieu of the above, we will also accept a Bachelor?s degree plus 5 years of progressively responsible post-baccalaureate experience. Foreign degree equivalent is acceptable. We will also accept any suitable combination of education, training or experience. Experience to include ing with Siebel, Salesforce, SQL and SOQL.

Hours

    40 hours per week, M-F, 9:00 a.m. ? 6:00 p.m.

 

    230 Commerce, Suite 110, Irvine, CA 92602. This  requires 70% domestic travel to clients? s across the US. Travel reimbursement including mileage and/or airfare/hotel, etc.

Contact Person

    Email resume referencing  code #SBA to UC Innovation, Inc. at s@ucinnovation.com. 

Note: This  is eligible for the company?s referral incentive program. Please refer to the posted policy for details.
30+ 
If you require alternative methods of application or screening, you must approach the employer directly to request this as Indeed is not responsible for the employer's application process.

	",Data Analyst,,indeed,Job post
"Senior Data Analyst
First American1,850 re - Santa Ana, CA 92707


Join our team! As a global leader in providing  insurance, settlement services and risk solutions for real estate transactions, First American (NYSE: FAF) is an ideal place to build your career. We have been entrusted with helping our customers achieve and protect their dream of homeownership since 1889. We believe that our people are the key to the company?s continued success, and we invest in diverse talents and backgrounds and empower our teams to achieve more than they could anywhere else. First American has created an award-winning culture and has been named to the Fortune 100 Best Companies to  For? list for the fourth consecutive year and to more than 50 regional Best Places to  lists. For more information, please visit www.careers.firstam.com

 Summary

The Senior Data Analyst/Data Steward will focus on improving the availability and quality of data that is made available throughout the organization. In this , you will expand your knowledge of the data assets available within the organization and act as the subject matter expert and data steward. The primary responsibility will include making this knowledge available to others within First American. The individual will partner closely with data scientists, data owners, business stakeholders, and engineers to get a deeper understanding of the data and find ways to improve its consumption.

The ideal candi will have an analytical mindset with the ability to process qualitative data and draw insightful conclusions from data. The Senior Data Analyst will have excellent communication  and have a detail-oriented mindset.



    Acquire intimate knowledge of the data and information available in various systems within the organization
     with data scientists to identify, analyze and collect data from numerous internal and external sources
    Understand and document important contextual data about data assets stored and maintained within our analytical solutions (metadata, business context, quality)
    Assess business  and provide appropriate recommendations and justifications related to data and existing analytical solutions
    Support ad-hoc requests related to data assets, including developing ad-hoc reports or queries to answer a business need
    Help develop logical data models that enable others to employ and capitalize on the insights discovered through previous use cases
    Assure our data assets are secure and adhere to the policy
    Identify data quality challenges and make appropriate changes or recommendations
    Demonst and  insights to non-technical or senior executive individuals
    Understand algorithms of mode to high complexity
     to get a good understanding of advanced analytical models
    Ability to gather and analyze information to make decisions about our data assets and its quality, , restrictions or use



    Bachelor?s in Business, Statistics, Mathematics, Information System or related  experience
    8+ years of business intelligence or data analysis experience
    Extensive experience performing data quality re and analysis
    Expert knowledge in SQL with the ability to write complex queries
    Must be intellectually curious and motivated to learn continually
    Expertise in data modeling and data visualization 
    Knowledge and experience ing with statistical models a plus
    Experience ing with Python, R or Java code a plus
    Excellent verbal and written communication 

First American invests in its employees' development and well-being, empowers them to provide superior customer service and encourages them to serve the communities where they live and . First American is committed to diversity and inclusion. We are an . For more information about our Company and our dedication to putting People First, check out firstam.com/careers.
6 

	",Data Analyst,,indeed,Job post
"Business Analyst
Hikvision USA Inc.10 re - Industry, CA
 Now

 SUMMARY

The Business Analyst is  responsible for identifying data needs and metrics and ensuring the integrity of all data used to support departmental reporting needs.

ESSENTIAL  FUNCTIONS

    Effectively translate customer reporting  into design documents; conduct report consolidation and validation; use statistical methods to analyze data and gene a useful business report.
    Communicate effectively with internal teams and external customers to deliver functional .
    Act as a primary liaison between the external sales team and the internal team to ensure the solution aligns to project scope and that it will ultimately meet user needs.
     with departmental managers to outline the specific data needs for each business method analysis project.
    Responsible to verify data accuracy and maintain records or files in accordance with standard procedures and methods; may also assist the team in various  as needed.
    Develop new metrics to assess operational performance. Create, maintain, optimize and support new and existing reports.

ESSENTIAL CORE COMPETENCIES

    Strong Excel spreadsheet  with either professional or academic experience in order to sort, track, compile and report-on large amounts of data effectively
    2+ years Data mining experience
    4+ years in a data analyst 
    Ability to collabo effectively and  as part of a team
    Strong attention to detail
    Advanced decision making and problem-solving 
    Excellent verbal and written communication  . Preferably bilingual in Chinese

EDUCATION

Bachelor Degree (Business with accounting emphasis)
8 
If you require alternative methods of application or screening, you must approach the employer directly to request this as Indeed is not responsible for the employer's application process.

	",Data Analyst,,indeed,Job post
"Business Analyst
Americor27 re - Irvine, CA 92612
 Now

Americor is an innovative finance  company, offering a unique approach to debt resolution. Our high-growth company is  a highly motivated Business Analyst. The Business Analyst identifies key value drivers in service and support processes to improve customer experience, reduce service costs and increase operating profit.

 Include:

    Proactively monitoring, analyzing, and reporting solutions and service results daily for optimal business operational performance
    Status reporting and communicating on a regular basis with client and internal management teams, and mitigation of matters affecting delivery of service
    Demonstrating proven communication , as well as requiring the ability to leverage technical business  and financial  in both a face-to-face and virtual team environment
    Managing performance relative to Service Level Agreement (SLA), while delivering against monthly and quarterly s
    Managing contractual cost, schedule, and service or product deliverables as they relate to the delivery organization. Manage resources and coordinate client resources to deliver services and solutions to support the client organization
    Evaluating, refining and reporting on daily operational issues/successes, execute daily process/audit re and resolve outstanding issues in a timely manner
    Maintaining relationships with existing clients by re and understanding their business needs, responding to client concerns and problems and acting as the liaison between the client and internal departments
    Supporting data and reporting requests from internal and external auditors
    Providing business consultation to determine scope of  to support client needs
    ing cross-functionally with Client Services, Sales, Legal, Operations, Compliance, and  as needed to support business s

:

    Bachelor's degree in Business Administration, Finance, Economics or related field
    Proven successful project management 
    Basic experience with SQL, Alteryx and/or Tableau
    Knowledge of statistical, analytical and influential , attention to detail, and creativity for problem-solving, with the ability to balance competing priorities
    Consumer Loan/Financial Services experience a plus
    Demonsts proactive problem solving and attention to detail
    Effective writing and presentation 
    Excellent PC  in Word, Excel and PowerPoint

What we provide:

    A fun and energetic  environment
    Health and Dental Benefits
    Free gym access

xqRhQlcxUt
7 
If you require alternative methods of application or screening, you must approach the employer directly to request this as Indeed is not responsible for the employer's application process.

	",Data Analyst,,indeed,Job post
"Sr. Data Analyst
Acorns - Irvine, CA
 Now
Senior Data Analyst | Acorns

At Acorns, we're building a different kind of investing experience - one that looks after the financial best interests of the up-and-coming. The Acorns team is passionate about pursuing our mission and living our values. It all begins with Heart. Our Heart leads us in everything we do. It allows us to make bold decisions, build trust, grow and advance our society. Come find your place at Acorns and join us in the revolution.

The Analytics team at Acorns leads data-driven stgies and decision making across the company. We are  an Analyst with the ability and experience to help accele our continued growth. This  will  closely with stakeholders across the organization to define analytics  and create solutions that will accele the speed from questions to insights. You will use your analytical expertise to deliver and develop an end-to-end analytics process.

You will be ing on a variety of . You thrive on ing in a fast paced environment, delighting your customers, and winning. You're a team player, committed to learning, developing your team, and yourself. You believe in setting the highest standards and ing hard as a team to reach them.

Within 1 month, you will:

    Introduce yourself and become integd with the Acorns team.
    Familiarize yourself with the business, metrics definitions, and data structures.
    Complete training on analytics , including Tableau, Databricks, Redshift.

Within 6 months, you will:

     closely with stakeholders in Management, Product, Engineering and Operations to identify business needs and analytics opportunities.
    Analyze big data, distill insights, and make business recommendations to drive product and stgy decisions.
    Manage existing analytic solutions, including modernization/standardization, troubleshooting, and maintenance as needed.
    Develop and deploy new  for automated reporting and decision-making.
     statistical knowledge and data modeling to drive business s

Within 12 months, you will:

     independently or as a member of a small team to conduct rigorous, fact-based research to advise senior management on critical business decisions
    Scope and develop scalable analytical frames, roadmaps, and metrics for the team and the broader organization.
    Augment and develop data reporting  that automate and streamline business intelligence
    processes
    Stay on top of industry trends to continue to evolve the Analytics organization to ensure best-in-class  and techniques are used.

What you will bring to Acorns (minimum )

    Bachelor's Degree in Economics, Finance, Statistics, Mathematics, or another quantitative field
    5-7 years of relevant experience in business intelligence or analytics 
    Expert knowledge of complex analytical techniques including modeling, optimization techniques, etc. Preferably in churn reduction, customer lifetime value, targeting, and attribution
    Extensive knowledge of Tableau, SQL and Microsoft Excel
    Background in data mining, statistical analysis, and modeling, with experience in deploying models in a production environment
    Proficiency in Python, Scala, R, with related statistical and machine learning packages
    Superior communication , including the ability to take complex, ambiguous topics and create compelling narratives for different audiences
    Driven to learn fast, be creative, and win as a team
    Experience with A/B and multivariate testing

What we offer:

    Competitive salary and stock options
    A comprehensive benefits package to meet the needs of you and your family
    Flexible paid time off
    Corpo gym access
    Daily breakfast, weekly team lunches, and an endless supply of snacks
    Numerous career possibilities that allow you to grow with Acorns
    Talented and motivated team members who care deeply about one another, our mission and our customers
    The rare opportunity to create a new world. We inspire one another every day to do meaningful  that solves big societal challenges
    Thirst for delivering game-changing products
    Exceptional drive and precision in delivery
    A belief that your  is tied to your life's mission
    Optimistic about the potential of societal change

About Acorns:
Acorns is the leading micro-investing app in the U.S. It allows users to round up their daily purchases and automatically Invest the Change? into a low-cost, diversified portfolio of exchange-traded funds offered by some of the world's top asset managers (including Vanguard and BlackRock). Founded in Newport Beach, Calif., by father and son team Walter and Jeff Cruttenden, Acorns provides a simple entry-point using the Acorns app on iPhone or Android. Customers accumulate fractional s in one of five portfolios constructed by world-renowned Nobel Laureate economist Dr. Harry Markowitz. Acorns' smart portfolio algorithms automatically  in the background of life, helping users build wealth naturally, pennies at a time. From Acorns, mighty oaks do grow.

Mission:
With benevolence and courage, we look after the financial best interests of the up-and-coming; beginning with the empowering step of micro-investing.

Values:

    Lead with heart
    Make bold decisions
    Always build trust
    Never stop growing
    Find a way

30+ 
If you require alternative methods of application or screening, you must approach the employer directly to request this as Indeed is not responsible for the employer's application process.

	",Data Analyst,,indeed,Job post
"Senior Business Analyst
SC Fuels44 re - Orange, CA 92867
 Now

 SUMMARY:

We are   a Senior Business Analyst, which is a critical  within the SC Fuels  organization. This Senior Business Analyst will maintain productive relationships with business stakeholders, built on a detailed understanding of our business operations, processes, people, systems, and priorities. This individual will document our stakeholders?  for  changes to systems, data, processes, and flows. The Senior Business Analyst will also  with application developers and vendors to review , approaches, timelines, and costs. They will serve as a subject matter expert to the application developers and vendors who are building new capabilities, and  with  and/or business teams to test the developed functionality. This  s closely with business analysts, application developers, vendors and stakeholders through each stage of the software development lifecycle and demonsts existing and new functionality to our stakeholders. This  requires a driven and detail-oriented analyst who can partner effectively at all levels of the organization, lead , drive results, and proactively identify and resolve problems.

The Senior Business Analyst must possess excellent written and oral communication  and be able to effectively present information to all levels in the organization, including senior executives.

ESSENTIAL DUTIES & :

    Maintain productive relationships with business stakeholders and consistently provide ups on project execution, approaches, costs, timelines as well as problems encountered and plans to overcome them.
    Develop a detailed understanding of our  platforms, including their criticality, features, configurations, interfaces, data models, and how employees leverage these systems to perform their s.
    Effectively capture detailed business , use cases, process flows and  with application developers to review.
    Develop documentation necessary to ensure successful estimation, technical approach and delivery of technical . Artifacts and level of detail will vary based on size and complexity of stakeholder requests.
    Contribute to the library of artifacts and processes that SC Fuels uses to capture  and execute  .
     with key resources (internal and external) to develop high-level labor estimates, conduct gap analysis between application and stated customer  and deploy functionality.
     with appropriate IT staff to communicate, verify, and test the desired functionality changes
    Ensure all business needs are addressed by demonstrating in-system changes
    Assist business stakeholders with the development of training materials for newly deployed functionality.
    Manage multiple competing priorities through effective organization and communication
    Manage a project?s scope, acceptance, installation, and deployment from start to finish.
    Report on technical issues and questions, make recommendations to the technical team
    Constantly be on the lookout for ways to improve business practices and efficiencies. Proactively suggest improvements to our  platforms, vendors and processes.
    Practice knowledge transfer and foster change management as the organization changes and new  are . There should be a focus on team/individual development as well as a drive to be a contributor.

:

    Exceptional written and verbal communication 
    A solid understanding of business functional areas, business management issues and data analysis
    Leadership aptitude
    Strong analytical thinking and problem-solving 
    Business case development and cost/benefit analysis experience
    Ability to prioritize  and create prototypes and mockups
    Team-focused/collaborative  style
    Demonstd ability to  successfully on large 
    Problem solving  evidenced by demonstd results in past s
    Ability to  under tight deadlines and handle multiple/detail-oriented tasks
    Ability to communicate and influence at all levels of the organization
    Experience with waterfall, agile, and hybrid project management approaches .
    Project management credentials .
    Ability to travel (minimal)
    : 5 years -related experience as a business analyst in an information  organization. Experience managing senior stakeholder relationships.
    :  - specifically with ERP systems, ETL, and databases. Experience with DM2 and Trinium or any standard ERP system

 TO BE USED:

    Hardware ?PC/Laptop
    Operating Systems ?Windows 10
     ? MS Office Suite: Office 365, Project, Visio
    Collaboration: Slack, Skype, Zoom, Webex, GoToMeeting
    Databases: MS Access, SQL Server
    ERP Systems: Trinium, DM2

SC Fuels is an . EOE/AA/M/F/Vets/Disabled

 Type: Full-time

Experience:

    Business Analyst: 5 years ()
    DM2: 2 years ()
    ERP Systems: 3 years ()
    Project Management: 5 years ()
    Trinium: 2 years ()

30+ 
If you require alternative methods of application or screening, you must approach the employer directly to request this as Indeed is not responsible for the employer's application process.

	",Data Analyst,,indeed,Job post
"Manager, Data & Analytics Modeler, Machine Learning
KPMG6,154 re - Irvine, CA 92618


Innovate. Collabo. Shine. Digital Lighthouse houses KPMG's specialized capabilities across the digital landscape: applied data science, AI, data engineering and insights, software engineering, automation, and big data. Here, you'll  with a sophisticated team of professionals to explore solutions for clients in a multiplatform environment. This means your ability to find answers is limited only by your creativity in leveraging a vast array of techniques and . Be a part of a high-energy, unique, fast-paced, and innovative culture that delivers with the agility of a tech startup and the backing of a leading global consulting firm. In this particular , you'll  specifically in the AI Analytics & Engineering Community within the Digital Lighthouse, on a wide range of . From applied AI to optimization to big data platform engineering, your analytical and  will drive real impact in the business world. So, bring your ingenuity and pioneering spirit to KPMG Digital Lighthouse.

KPMG is   a Manager to join our KPMG Lighthouse - Center of Excellence for Advanced Analytics.

:

    Lead multi-disciplinary and cross-functional teams to identify business opportunities and define artificial intelligence solutions; Utilize processes and best practices to plan, lead, and execute delivery of artificial intelligence engagements, and  with clients to manage risks, set expectations and ensure successful delivery across different areas (, financial services, emerging tech, government agencies ? federal, state and local, and utilities)
    Deliver client project modeling  stream through direct ownership of data integration, validation, mining, and quantitative modeling deliverables; Lead project delivery by tracking and communicating project risks, budget, s, and launch/closeout activities, including the administration of  papers and collaboration sites
    Assess, capture and translate issues and  into structured analytics use case, including rapid learning of industry/domain/client dynamics and development of effective  stream plans
     with clients to discover data sources, and create data requests; Lead the ETL process to ingest and enrich structured and unstructured data; Leverage a variety of data sources such as social media, news, internal/external documents, images, video, voice, emails, financial data and operational data
    Plan engagement s and key deliverables; manage using analytics processes to mitigate risks in data, modeling, validation and delivery;  with team members to capture assumptions, and risks, and develop approaches to mitigate issues; Deliver on engagement milestones by following analytics processes to mitigate risks in data, modeling, validation and delivery; manage assumptions, risks, and  with others to clear issues
    Proactively broaden and deepen client relationships by ing with varying levels of client team members

:

    Minimum five years of experience involving modeling (regression, machine learning, feature selection, dimension, reduction, validation); Data (extracting, preparing, munging, validating); Building analytics pipelines, data science landscape and software development lifecycle; Two years of training specific to artificial intelligence, and five years of experience leading teams of five or more data scientists, engineers and other data & analytics professionals
    Bachelor's degree or Master's degree from an accredited college/university in a quantitative discipline, such as Data Science, Analytics, Computer Science, Engineering, or Mathematics
    Strong knowledge in delivering analytics  using leading processes including skilled knowledge of data discovery, cleaning, model selection, validation, and deployment; designing and building of machine learning pipelines (data extraction, feature engineering from structured and unstructured data)
    Ability to  artificial intelligence techniques to achieve concrete business goals; ability to  with the business to understand available resources and constraints around data (sources, integrity, and definitions), processing platforms, and security; Provide assistance and resolve problems, using solid problem-solving  with strong verbal/written communication 
    Proficiency with sophisticated analytics  and programming languages such as SAS, R, Python, Java, Spark, Hadoop, Alteryx and SQL; Data visualization  such as Tableau and QlikView
    Ability to travel up to eighty percent of the time; Applicants must be  authorized to  in the United States without the need for visa sponsorship now or in the future

KPMG LLP (the U.S. member firm of KPMG International) offers a comprehensive compensation and benefits package. KPMG is an affirmative action-. KPMG complies with all applicable federal, state and local laws regarding recruitment and . All qualified applicants are considered for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, protected veteran status, or any other category protected by applicable federal, state or local laws. The attached link contains further information regarding the firm's compliance with federal, state and local recruitment and  laws. No phone calls or agencies please.
29 

	",Data Analyst,,indeed,Job post
"Business Analyst
First American1,850 re - Irvine, CA 92612

Company Summary

Join our team! First American's Direct division provides comprehensive  insurance protection and professional settlement services for real estate purchases, refinances and equity loans. As a global leader in providing  insurance, settlement services and risk solutions for real estate transactions, First American (NYSE: FAF) is an ideal place to build your career. We have been entrusted with helping our customers achieve and protect their dream of homeownership since 1889. We believe that our people are the key to the company?s continued success, and we invest in diverse talents and backgrounds and empower our teams to achieve more than they could anywhere else. First American has created an award-winning culture and has been named to the Fortune 100 Best Companies to  For? list for the fourth consecutive year and to more than 50 regional Best Places to  lists. For more information, please visit www.careers.firstam.com

 Summary

The process analyst for our New Homes Division oversees one or more  on an ongoing and regular basis. Individually, or as part of a team, responsible for project planning and risk management, and issue resolution. Duties may include: assembling project team, assigning individual , identifying appropriate resources needed, developing schedules to ensure timely completion of project, communicating project status and issues to team and management, acting to reduce risk and resolve issues. Coordinates with other  and  groups.

Essential Functions
Oversee small  or phases of a medium project.
Coordinates activities of project team, identifying appropriate resources needed, and developing schedules to ensure timely completion of project.
Responsible for defining system scope and project s to support business needs, as well as define the  and function of each team member, in order to effectively coordinate the activities of the team.
Regularly facilitates status and milestone meetings with cross functional groups.

 Complexities
s on problems of mode scope where analysis of situations or data requires a review of a variety of factors.
Exercises judgment to determine appropriate action.
Detects situations that may cause risk to the project and its completion and escalates these issues to other team members.
Errors may cause minor project delays and/or missed project deadlines.

Supervision Received or Extended
s under general direction
Uses expertise of more senior level team members and leverages additional resources to confirm solution or explore customized applications as a solution.
Assign duties, , and scope of authority to project personnel.
Provides input to and achieves set s.
Negotiates, persuades, and gains consensus from implementation team and/or management on assigned .No responsibility for supervision of others

Knowledge and / Used
MS Software 
Analytical review 
Strong project planning 
Experience facilitating meetings with cross functional team members.
Presentation 

Typical Education
Generally requires BS Degree in a business field or equivalent  experience.

Typical Range of Experience
Typically have 2- 4 years of directly related experience.

First American invests in its employees' development and well-being, empowers them to provide superior customer service and encourages them to serve the communities where they live and . First American is committed to diversity and inclusion. We are an . For more information about our Company and our dedication to putting People First, check out firstam.com/careers.
Today

	",Data Analyst,,indeed,Job post
"Senior Data Analyst
Kelly Services14,506 re - Irvine, CA 92618


Kelly Outsourcing & Consulting Group (KellyOCG), a managed solution provider and business unit of Kelly Services, Inc., is   an Sr. Data Analyst for a long-term engagement at one of our Global clients in Irvine, CA.

This  is a full-time, fully benefitted .

As a KellyOCG employee you will be eligible for Medical, Dental, 401K and a variety of other benefits to choose from. You?ll also be eligible for paid time off, including holiday, vacation, and sick/personal time. All KellyOCG employees receive annual performance re.

 :

    With supervision,  is responsible for analyzing industry, customer, and competitive insights within the Global Business Insights team (GBII.
     is responsible for executing primary and secondary research, identifying key insights, and building executive level presentation slides to communicate insights.
     will maintain and refresh market and  reporting processes and will be responsible for quality control, data validation, data coding, IT relationship management, and enhancing process efficiency.
     will require broad cross-functional neting, proactive dashboard creation and development, and strong presentation .
    With supervision,  will  with the Business Unit teams to develop stgic market insights, including but not limited to opportunity identification, competitor pipeline, commercial stgy, tactical and stgic plans, and marketplace conditions and  scenarios.
    With supervision, design and execute primary research studies to deepen market understanding and drive business stgy.

 Duties and :

    With supervision, synthesize data from various sources to form recommendations and actionable insights
    Utilize Excel, Tableau to identify and analyze data to support key organizational stgies
    Integ and analyze large datasets and ensure high quality of data
    Develop and maintain MS Excel and Tableau dataset.
    Develop and maintain internal Point data libraries and communication materials
    With direction, demonst an ability to implement competitive insight and analysis  in support of assigned business unit / region.
    Develop and execute primary research studies

 Education and/or Experience:

    Bachelor?s degree and 2 years of experience in Analyst/Sr. Analyst or similar 
    Advanced Excel  are , and a competency test will be administered. Advanced knowledge of V-Lookup, Formulas and Formatting .
    Proficiency with database management  and presentation of data analysis and visualization tool ? Tableau software is 
    Strong quantitative and analytic abilities to analyze and vali data
    Excellent computer  (Word, PowerPoint, Point)
    Experience in the medical device, hospital specialty, or healthcare field is  but not 
    Ability to  in a matrix environment where a high degree of collaboration is needed
    Demonstd success in creative problem solving and team partnership is .
    Due to the nature of information reviewed in the , candi must demonst a high level of discretion and confidentiality
    Strong project management 
    Primary research experience  but not 
    Ability to organize and present information to internal business partner
    **Once candi has an established track record of performance, client is supportive of ing from home 1-2 days per week (or more) as needed

Important information: This  is recruited for by a remote Kelly office, not your local Kelly branch. Applicants must be legally permitted to  in the United States immediately and without employer sponsorship (F1 OPT/H1B cannot be considered for this ).



Why Kelly??

As a er today, it?s up to you to take charge of your career and look for opportunities to learn, grow, and achieve your potential. Helping you find what?s next is what we?re all about. We know what?s going on in the evolving world of ?just ask the nearly 500,000 people we employ each year. Connecting with us means getting the support, guidance, and opportunities needed to take your career where you may have never imagined.

About Kelly?

At Kelly, we?re always thinking about what?s next and advising  seekers on new ways of ing to reach their full potential. In fact, we?re a leading advocate for temporary/nontraditional styles, because we believe they allow flexibility and tremendous growth opportunities that enable a better way to  and live. Connecting great people with great companies is what we do best, and our employment opportunities span a wide variety of styles, skill levels, and industries around the world.

Kelly is an  committed to employing a diverse force, including, but not limited to, minorities, females, individuals with disabilities, protected veterans, sexual orientation, gender identity. Equal Employment Opportunity is The Law.
17 

	",Data Analyst,,indeed,Job post
"Data Scientist- Re to Saudi Arabia
Saudi Aramco1,005 re - United States


Saudi Aramco is  an Engineer for the development and implementation of data analytics solutions to join the Process & Control Systems Department/Advanced Process Solutions Division, based in Dhahran, Saudi Arabia.

The Advanced Process Solutions Division provides engineering expertise in Manufacturing Operations Management, Decisions Support Applications, Advanced Process Control, Process Modeling and Optimization, and Advanced Analytics .

The successful candi will have a strong background in data science, machine learning, statistical modeling as well as a basic knowledge of standard process historians. Experience with data science and machine learning is .

The candi should understand statistical and predictive modeling concepts, as well as supervised and unsupervised learning techniques.

Minimum :
BS in Chemical Engineering, Electrical Engineering or Computer Engineering or other related degree from an accredited university.

Advanced degree in data analytics, data science, applied statistics, operations research, process optimization or a related quantitative discipline is an asset.

Course  in applied statistics, data mining, machine learning, and optimization techniques is a must.

Understanding of statistical and predictive modeling concepts, machine-learning approaches, neural nets, clustering and classification techniques, and optimization algorithms is .

Knowledge of Python, R, Matlab & other machine learning frames is a must. General analytics (e.g. INFORMS CAP) and/or platform specific certification as an asset.

Experience with selecting appropriate machine learning algorithms, building predictive and forecasting models, and solution deployment and maintenance is .

Experience with deployment of advanced analytics solutions based on machine learning techniques in upstream and downstream processes will be an asset.

Knowledge of plant automation systems, standard real time interfaces such as OPC and process historians such as PI is .

Experience with identification of new advanced analytics use cases, business value quantification and ing with multi-disciplinary teams on delivering analytics solutions will be an asset.

Awareness of Digital Transformation area and experience with delivering Digitalization initiatives will be an asset.

Candis should have 5-10 years of experience in a technical specialty, have experience in oil and gas business and applications (refineries, gas plants, producing), and recognized as a potential technical leader in his/her field.

Duties & :
The candi will be responsible for the following:

Provides expert advice and assistance to Company organizations, outside vendors, and  firms on matters relating to his/her area of expertise.

Originates and carries out studies to identify potential advanced analytics use cases and applications to existing problems resolve complex technical problems, increase revenue, optimize processes, protect the environment, or improve safety using their advanced analytics/data science background.

Develops and maintains Best Practices, or other documentation to enhance company profitability, safety and productivity in the advanced analytics/data science area.

Examines the business problem and identifies the  learning task (i.e. classification, clustering, regression, forecasting ? etc.)

Selects appropriate Machine Learning algorithms suited for the selected learning task and condition/structure of the data. This includes but not limited to Decision Tree/Forest, Support Vector Machines, kNN, Neural Net and deep learning.

Builds and deploys predictive models using algorithms such as linear/logistic/dynamic regression to address the business needs.

Re and evaluates the capabilities of vendors and contractors to determine their acceptability to perform major services to Aramco.

Acts as Aramco?s representative in dealings with vendors and manufacturers pertaining to their field of experience.

s on or with Task Forces assigned to investigate incidents or solve specific problems using their data science/analytics background. Undertakes special assignments requiring extensive technical experience and discretion when dealing with outside organizations.

Conducts presentations in the resolution of technical problems with personnel in other organizations.

Writes papers for peer-reviewed journals on studies and  they has overseen.

Provides leadership to others assigned to assist them.

Acts as a professional mentor to less senior specialists and engineers.

Promotes a learning environment and implements self-development to maintain and improve expert status within his/her specialty.

About us:
The Engineering & Project Management (E&PM) business line studies, plans and oversees the construction of the Company?s new facilities, including some of the biggest and most complex  in the petum industry. Recently, Saudi Aramco completed the largest capital program in its history that included new or expanded oil, gas and petrochemical facilities, raising maximum sustainable crude oil production capacity to 12 million barrels per day and significantly increasing gas production and processing capacities. Among the recently completed  was the largest crude oil increment in the history of the industry: Khurais, with a production capacity of 1.2 million barrels per day. More challenges lie ahead, with a slate of new or expanded oil, gas, refining and petrochemical  in the s. E&PM also manages the Company?s Research & Development Center where scientists investigate topics such as the desulfurization of crude oil, advanced fuel formulations for next generation combustion engines, and reservoir nano-scale robots (Resbots?) for injection into reservoirs to record their properties.
30+ 

	",Data Scientist,,indeed,Job post
"Lead Data Scientist
CitiusTech92 re - Lake Forest, CA 92630
 Now

    10 years of experience in Data Analysis
    Postgraduate qualification in Statistics (OR Computer Science) from a premier institute
    Having hands-on experience on advanced analytics concepts and ML algorithms (e.g. Regression analysis, Logistic Regression, Decision Trees and random forest, Neural net and deep learning, Survival Analysis, Time series analysis etc.)
    Exposure to  ? R, Python, Weka,
    Excellent communication 
    Excellent critical thinking and problem solving 
    Developing advanced algorithms that solve problems of large dimensionality in a computationally efficient and statistically effective manner
    Implementing statistical and data mining techniques e.g. hypothesis testing, machine learning, and retrieval processes on a large amount of data to identify trends, patterns and other relevant information
    Collaborating with clients and other stakeholders to effectively integ and communicate analysis finding
    Providing guidance and project management support to the Associates on the team
    Evaluating emerging datasets and  that may contribute to our existing analytical platforms
    Establish the scope of the project, lead internal communication with stakeholders, and ensure delivery of the project per commitment.
    Contributing to the thought leadership of the company by helping in re the evolving topics and publishing them

oXSnuEZD3l
30+ 
If you require alternative methods of application or screening, you must approach the employer directly to request this as Indeed is not responsible for the employer's application process.

	",Data Scientist,,indeed,Job post
"Data Scientist
Gustaine - Orange, CA
 Now
 Summary / Purpose

Highly motivated self-driven Engineer in statistics / predictive modeling / data quality to lead and guide multi-disciplinary project teams addressing key challenges for different businesses. Creation of intellectual property will be a key expectation in this .

Essential 

As a senior data scientist in the Modeling and Optimization, you will create and guide programs to invent and deliver predictive modeling and decision  for diverse businesses such as Finance, Aviation, Transportation, Oil and Gas and Healthcare.

You will lead and drive programs in areas such as statistical algorithms for processing massive time series data, statistical risk modeling and development of novel algorithms for detecting and correcting anomalies in complex data. Typical applications include developing novel algorithms for early warning systems, building models for medical prognostics and systems for anomaly detection / correction in monitoring and diagnostics data. You will be ing with some of the sharpest business minds across the globe on some of the most challenging business problems

 / 

We are looking for accomplished Fresh Graduates / colleagues with 1-2 years of experience, track record of project management, and demonstd ability to invent new approaches and/or  recent methodological advances in data science to solve applied problems. Candis should hold an MTech or MS in Industrial Engineering, Mathematics, Computer Engineering, Computer Sciences, Statistics with an excellent academic record and practical experience with recent advances in mathematical & computational sciences and statistical modeling.

The candi should have a demonstd strong foundation in probability, statistical theory with a deep understanding of statistical estimation, hypothesis testing, linear and logistic regression. A solid grounding in applied statistics including expertise in at least one of the following is a must: Reliability models, Bayesian modeling, statistical classification, cluster analysis, time series analysis, forecasting and multivariate statistics. Possessing strong Implementation and Programming  in one or more of Java, Python, SAS, R, Python, Matlab is a must.

Prior experience ing with very large datasets using Big Data  and platforms (Hadoop, PIG / HIVE / Mahout) is highly . Proven ability to lead complex  with multi-disciplinary teams and ability to  with global businesses to create new programs is a must. Expertise in modelling the behavior of a complex real time dynamic system.

Desired Characteristics

    PhD in Industrial Engineering, computer science, Comp Engg, Statistics, Mathematics, Statistics or related field
    Strong interpersonal 
    Excellent written and verbal communication 
    Experience ing with Financial engineering data ? aviation, healthcare, transportation, energy, oil & gas, etc

Send in your resume: guptaneel@gustaine.com
10 
If you require alternative methods of application or screening, you must approach the employer directly to request this as Indeed is not responsible for the employer's application process.

	",Data Scientist,,indeed,Job post
"Senior Data Scientist
Happy Money - Irvine, CA
 Now
ABOUT HAPPY MONEY

More than a finance company and more than a paycheck. We're looking for driven, bright and kind team members who are ready to help us deliver on our mission to help borrowers become savers.

Our team at Happy Money is made up of financial services professionals, marketers, technologists, service experts, data scientists, and designers with the d vision to provide financial  and services for human happiness!

Meet Happy Money, we're not just a business, we're a movement, providing financial  and services for human happiness, ing outside of the sad money complex with an innovative business money that s for members.

The category that needs a mission-based company the most ? finance/banking ? has none. It's time to leave mindless capitalism behind and usher in a new world of more mindful capitalism.

Our mission is helping borrowers become savers. Our values of love, trust and hustle are our culture and they inspire us to deliver for our team, our members, our partners, our investors and society.

Join the Happy Money Movement and help us deliver the future of Happy Financial Services.

ABOUT THE 

We are looking for an innovative and driven Data Scientist to join our team at Happy Money. In this , you will have a unique opportunity to build data-driven solutions that  to all facets of the Happy Money ecosystem. This includes financial modeling using alternative data sources, building behavioral models to enhance user experience and drive engagement, and building  to provide guidance to members. These  are all in the service of reducing stress, increasing happiness, and empowering people to take control of their financial lives.

Now is an exciting time to join the Happy Money team, as the company is growing rapidly and scaling quickly. The research and development carried out by the Data Science team is core to the company's philosophy and success, making each team member a vital contributor to the company as a whole.

    Create visualizations and dashboards to express your findings to key business stakeholders
    Uncover psychological and behavioral markers of positive financial outcomes to help educate members and increase the utility of our products
     closely with our Risk & Analytics team to uncover new ways to identify creditworthiness to provide financial support to a wider audience
    Partner with Product and Engineering teams to bring your models to production
    Be a thought pioneer in new and ongoing directions of scientific research and implementation at the company level

ABOUT YOU

    Master's degree in math, statistics, psychology, neuroscience, computer science, physics, or another quantitative field
    3+ years of industry experience in science and analytics , along with equivalent academic background
    3 + years of experience with experimental design and A/B testing frames
    3+ years of experience with analytics packages in R, Python, or Spark
    Proficiency in SQL
    3+ years of experience building supervised and unsupervised models using statistical or machine learning approaches
    Experience deploying production models or model pipelines either via API (e.g. AWS Lambda) or containerized solution (Docker, Kubernetes,

etc.)

    Ability to creatively problem-solve, and to thrive in a highly collaborative, multidisciplinary environment
    Ability to clearly articulate research goals and results to scientific and business audiences

BONUS POINTS FOR

    Ph.D. in psychology, neuroscience, statistics, physics, math, or another quantitative field
    Prior professional experience in the financial industry
    Ability to initiate and drive  from ideation to implementation
    Prior professional experience ing with behavioral or psychometric data
    Prior professional experience building deep-learning models and familiarity with the popular frames (Tensorflow or PyTorch is a huge bonus)

WHY WE'RE AWESOME

    Rich employee medical benefits offering?most paid 100% by Happy Money!
    Generous vacation policy!
    Unlimited snacks, coffee, teas?whatever you're into, we've got it!
    Weekly Happy Money Hour?we love to mingle and enjoy great brews!
    Immigration sponsorship for qualified candis

30+ 
If you require alternative methods of application or screening, you must approach the employer directly to request this as Indeed is not responsible for the employer's application process.

	",Data Scientist,,indeed,Job post
"Sr Data Scientist
Cox Automotive520 re - Irvine, CA 92618


At Cox Automotive, our data scientists are responsible for leading the development of advanced analytics models to solve our customers? problems, improve our products and services, and inform internal business operations and stgy. They will  closely with business stakeholders as the subject matter expert on the application of statistics and modeling across multiple leading digital automotive brands.


Day to day, the Senior Data Scientist will:

    Anticipate future business needs and identify appropriate opportunities for modeling, simulation, or machine learning
    Gather and analyze data to solve and address highly complex business problems, make predictions on future outcomes, and provide prescriptive solutions that support decision making
    Be involved in all phases of analytics  including question formulation, research, development, implementation, testing, and maintenance
    Explore data and build advanced analytical models, then present and discuss the resulting models to any level of audience

:

:

    Bachelor's degree in math, statistics, operations research, computer science engineering, econometrics, quantitative social science, or other quantitative field OR an equivalent combination of education and  related experience
    5+ years of industry ing experience performing advanced quantitative analyses
    Professional experience ing with Python/R and SQL
    Professional experience with big data manipulation
    Proven ability to  advanced statistical methodologies such as multiple regression model, mixed models, time series models (Bayesian ), neural nets, cluster analysis, text mining, and have prior experience in optimization, simulation, marketing mix, multivariate testing, ensemble modeling, graph algorithms
    Ability to  advanced optimization methodologies such as linear and mixed integer optimization
    Ability to  advanced simulation modeling methodologies and techniques.
    Must have relational database experience

    A strong passion for empirical research and for answering hard questions with data
    Curiosity, humility, and empathy

:

    Masters? degree or Ph.D.
    Digital experience
    Professional experience with software development practices


#LI-286

About Cox Automotive

Cox Automotive Inc. makes buying, selling and owning cars easier for everyone, while also enabling mobility services. The global company?s 34,000-plus team members and family of brands, including Autotrader?, Clutch , Dealer.com?, Dealertrack?, Kelley Blue Book?, Manheim?, NextGear Capital?, VinSolutions?, vAuto? and Xtime?, are passionate about helping millions of car shoppers, tens of thousands of auto dealer clients across five continents and many others throughout the automotive industry thrive for generations to come. Cox Automotive is a subsidiary of Cox Enterprises Inc., a privately-owned, Atlanta-based company with revenues exceeding $20 billion. www.coxautoinc.com

Cox is an Equal Employment Opportunity employer - All qualified applicants/employees will receive consideration for employment without regard to that individual?s age, race, color, religion or creed, national origin or ancestry, sex (including pregnancy), sexual orientation, gender, gender identity, physical or mental disability, veteran status, genetic information, ethnicity, citizenship, or any other characteristic protected by law.

Statement to ALL Third-Party Agencies and Similar Organizations: Cox accepts resumes only from agencies with which we formally engage their services. Please do not forward resumes to our applicant tracking system, Cox employees, Cox  manager, or send to any Cox facility. Cox is not responsible for any fees or charges associated with unsolicited resumes.

30+ 

	",Data Scientist,,indeed,Job post
"Machine Learning Engineer
SimInsights - Lake Forest, CA 92630
Full-time, Part-time, Contract
 Now



     state of the art machine learning techniques (deep learning, RL and other techniques) to solve problems in multiple fields including computer vision, natural language processing, recommendation systems and others.
    Vision problems include object and action recognition in indoor settings.
    NLP problems include question answering.
    Use rigorous statistical methods to analyze data arising from IoT systems, games, simulations, websites and apps to derive actionable insights.
    Comfortable ing in multiple domains including manufacturing, healthcare and education. Rapidly learn new domains by ing with others in the team.
    Automate processes by developing software modules in collaboration with software engineering teams. Requires software engineering .
     with our partner companies which include leading software and hardware vendors.
    Understand  through regular interaction with concerned stakeholders including counterparts in the US and other offices.
    Ensure thorough quality testing and documentation of the software developed.
    Constantly learn and innovate to stay on the cutting edge of knowledge and .



    PhD or MS degree in computer science, statistics or other engineering disciplines.
    Strong course, project and/or  experience.
    Must be comfortable reading research papers and implement ideas or gene new ones.
    Must have good references.

 Types: Full-time, Part-time, Contract
30+ 
If you require alternative methods of application or screening, you must approach the employer directly to request this as Indeed is not responsible for the employer's application process.

	",Data Engineer,,indeed,Job post
"Data Scientist
Driveway11 re - Aliso Viejo, CA
 Now

We are looking for a Data Scientist who will support our product, sales, leadership and marketing teams with insights gained from analyzing company data. The ideal candi will be intricately involved in running analytical experiments in a methodical manner and will regularly evaluate alternate models via theoretical approaches. They must have a proven ability to drive business results with their data-based insights.

Primary :

    Understanding data flow in a large, distributed system.

    Designing, coding and executing data investigations.

    Performing robust statistical analyses.

    Producing internal presentations, customer reports and visualizations to summarize the conclusions.

    Handle a diverse set of problems ranging from probabilistic interference on phone trajectories, data fusion between noisy sensors, modeling and classification of driving behavior, and data visualization.

    All other duties as assigned.


:

    Masters or PhD in Computer Science, Mathematics, or related field.

    3+ years performing data analysis on real world figures.

    Strong programming  for constructing and understanding data pipelines, particularly in Python (scipy/numpy) and SQL.

    Experience with modern/emerging scalable computing platforms and languages (e.g.Spark).

    Strong computer  including MS Office Suite, Google docs and other needed software platforms.

    Excellent oral, written and listening .

    Entrepreneurial spirit, desire to be part of an early stage startup.

    Bootstrapping mindset but obsessive about quality.

    Positive attitude with superior interpersonal .

    Honest and dependable.

    Comfortable ing across every level of an organization including company executives.

    Team player with the ability to display sensitivity, tact and responsiveness with current and prospective employees.

    Self-starter who accomplishes major tasks and initiatives with little supervision.

    Business-to-business experience a plus.

    Organized proactive problem solver.

    Ability to handle confidential and sensitive information.

    If you want to fill this , tell us why at s@driveway.ai

30+ 
If you require alternative methods of application or screening, you must approach the employer directly to request this as Indeed is not responsible for the employer's application process.

	",Data Scientist,,indeed,Job post
"Conversational Artificial Intelligence Designer, Manager - Applied Artificial Intelligence
Deloitte8,909 re - Costa Mesa, CA 92626

 you?ll do
At Deloitte you will manage and deliver components of client engagements that identify, design, and implement  and creative business solutions for large companies. Key  will include:

    Architect, , Design, Develop and Deploy enterprise solutions which include components across the Artificial Intelligence spectrum such as Chatbots, Virtual Assistants, Machine Learning, and Cognitive Services
    Manage teams in the identification of business , functional design, process design (including scenario design, flow mapping), prototyping, testing, training, defining support procedures.
    Formulate planning, budgeting, forecasting and reporting stgies.
    Manage full life cycle implementations.
    Develop statements of  and/or client proposals.
    Identify business opportunities to increase usability and profitability of information architecture.
    Experience with program leadership, governance and change enablement.
    Develop and manage vendor relationships.
    Lead shops for client education.
    Manage resources and budget on client .
    Assist and drive the team by providing oversight.

The team
Analytics & Cognitive
In this age of disruption, organizations need to navigate the future with confidence, embracing decision making with clear, data-driven choices that deliver enterprise value in a dynamic business environment.

The Analytics & Cognitive team leverages the power of data, analytics, robotics, science and cognitive  to uncover hidden relationships from vast troves of data, gene insights, and inform decision-making. Together with the Stgy practice, our Stgy & Analytics portfolio helps clients transform their business by architecting enterprise programs and differentiated stgies to win in their chosen markets.

The Analytics & Cognitive team s with our clients to:

    Implement large-scale data ecosystems including data management, governance and the integration of structured and unstructured data to gene insights leveraging cloud-based as well as client-owned platforms
    Deploy automation and cognitive and science-based techniques to manage data, predict scenarios and prescribe actions
    Drive operational efficiency by maintaining their data ecosystems, sourcing analytics expertise and providing As-a-Service offerings for continuous insights and improvements



    6+ years of relevant  experience
    2+ years of consulting experience on AI  ()
    Experience collaborating with AI engineers and other relevant team members
    Group facilitation 
    Experience with  gathering and articulation
    Experience delivering at least one pilot
    3+ experiences developing and deploying voice and/or text conversational flows for virtual agents or virtual assistants involving IPSoft Amelia, Kore.ai, Nuance Nina, Amazon Lex & Alexa, Dialogflow,or IBM Watson

    Basic understanding of SDLC (software development lifecycle) and agile delivery methodologies
    Experience with prototyping  such as Sketch, Axure, InVision, etc.
    Expertise in linguistics

    Understands complexity of natural language and conversational experiences

    Bachelor's Degree preferably in linguistics, human-computer interaction, interaction design, communications, psychology, or closely related field
    Ability to travel 80-100% of the time (Monday ? Thursday/Friday)


:

    Expertise in at least one functional application  (e.g., Virtual Travel & Hospitality Concierge, Virtual Product Specialists, Virtual Personal Shopper, Virtual Bank Teller & Advisor, Virtual Account Representative, Virtual IT Helpdesk Support, Virtual HR Specialist, Virtual Onboarding Advisor, On-the- Task and Process Guidance, Corpo Policy Guidance, etc.)
    Expertise in 2+ domain  (e.g., Customer & Marketing, Finance, M&A, Operations, Pricing, Risk, Supply Chain, force, etc.)
    Certification or expertise on at least one of the following platforms: IPSoft Amelia, Kore.ai, Nuance Nina, Amazon Lex & Alexa, Dialogflow,or IBM Watson

    Experience in , Media, Telecommunications, Consumer (especially transportation) and Financial Services & Insurance a plus

    Ability to  independently, manage small engagements or parts of large engagements.

    Strong oral and written communication , including presentation  (MS Visio, MS PowerPoint).

    Strong problem solving and troubleshooting  with ability to exercise mature judgment.

    An advanced degree in the area of specialization is .


How you?ll grow
At Deloitte, our professional development plan focuses on helping people at every level of their career to identify and use their strengths to do their best  every day. From entry-level employees to senior leaders, we believe there?s always room to learn. We offer opportunities to help sharpen  in addition to hands-on experience in the global, fast-changing business world. From on-the- learning experiences to formal development programs at Deloitte University, our professionals have a variety of opportunities to continue to grow throughout their career. Explore Deloitte University, The Leadership Center.

Benefits
At Deloitte, we know that great people make a great organization. We value our people and offer employees a broad range of benefits. Learn more about what ing at Deloitte can mean for you.
Deloitte?s culture
Our positive and supportive culture encourages our people to do their best  every day. We celeb individuals by recognizing their uniqueness and offering them the flexibility to make daily choices that can help them to be healthy, centered, confident, and aware. We offer well-being programs and are continuously looking for new ways to maintain a culture where our people excel and lead healthy, happy lives. Learn more about Life at Deloitte.

Corpo citizenship
Deloitte is led by a purpose: to make an impact that matters. This purpose defines who we are and extends to relationships with our clients, our people and our communities. We believe that business has the power to inspire and transform. We focus on education, giving, skill-based volunteerism, and leadership to help drive positive social impact in our communities. Learn more about Deloitte?s impact on the world.

Recruiter tips
We want  seekers exploring opportunities at Deloitte to feel prepared and confident. To help you with your interview, we suggest that you do your research: know some background about the organization and the business area you?re ing to. Check out recruiting tips from Deloitte professionals.

#LI:PTY

#IND:PTY
30+ 

	",Data Engineer,,indeed,Job post
"Data Scientist (ML/NLP)
ot17 re - Irvine, CA 92618
$150,000 - $200,000 a year

This ot  is hosted by: Reggie Panis
Are you a fit? Easy  now by clicking the """" button and sending us your resume.
Salary: $150,000 - $200,000 per year

A bit about us:
Based in Irvine, CA
we are a fast-growing AI company that is revolutionizing how digital data is extracted, quantified and normalized. Our patented AI Factory produces unprecedented intelligence to businesses on a global scale.  we have an excellent opportunity for a seasoned Data Scientist with strong technical expertise in Natural Language Processing (NLP), Machine Learning and Artificial Intelligence (AI).

Are you a Data Scientist with NLP, AI, ML and text extraction expertise, then please
read on?.

What can we do for you?

Competitive Package!

Flexible Schedules!

 on Game changing problems/solutions
Is your background a
fit? We are looking for?

Previous Startup/nimble development experience a plus.

Strong technical
background (AI, ML and NLP expertise - Text extraction experience over Image extraction is a must)

Current publishing?s
(we want to see some things that you are  or have
recently published)

A stgic approach
to the business problems/solutions.
4 

	",Data Scientist,,indeed,Job post
"Analyst, Statistical Programming - Transcatheter Heart Valve
Edwards Lifesciences370 re - Irvine, CA 92606

This is a unique opportunity to join the Transcatheter Heart Valve (THV) group, focused on developing minimally-invasive solutions for patients suffering from structural heart disease.

The Analyst, Statistical Programming is a newly created opening for the development, testing, validation, documentation, maintenance, and execution of SAS programs. This individual will primarily be providing statistical programming support for THV Clinical Affairs department.

Essential  Functions/Key :

    Develop or modify analysis datasets for inclusion in clinical reports or presentations; Create programs for summary output (tables, listings, or graphs) that meet regulatory and company standards to permit efficient programming, reporting, and review, utilizing statistical programming languages (e.g SAS or R/RStudio)
    Review and vali statistical programs and ensure that all appropriate program validation documentation to meet regulatory and company standards are consistently structured to permit efficient programming, reporting, and review
    Perform analysis in response to data requests in collaboration with designated statistician
    Create statistical programming related documents
    Participate programming standardization infrastructure buildup

Basic  and Minimum :

    Bachelor?s degree in Biostatistics, Statistics, Mathematics, Computer Science, Bioinformatics, or Engineering, with 3 years of statistical programming experience, or
    Master?s degree in Biostatistics, Statistics, Mathematics, Computer Science, Bioinformatics, or Engineering
    Strong SAS programming  (SAS/BASE, SAS/STAT, SAS/GRAPH)
    Proficiency with R, RStudo or Python
    Good understanding of statistical techniques
    Proficiency in MS Office Suite

 :

    Experience in clinical trials within the medical device, pharmaceutical, or biotech industry
    Good communication 
    Eager to learn and be proactive in chasing solutions to problems or achieving results
    Attention to detail
    Ability to manage and prioritize multiple 
    Ability to  effectively as a team member

#LI-GT1 #AS

Edwards is an Equal Opportunity/Affirmative Action employer including protected Veterans and individuals with disabilities.
GlassDoor.com - 30+ 

	",Data Engineer,,indeed,Job post
"Senior Statistical Programmer
Parexel706 re - United States


As a member of Parexel?s Statistical Programming group, you will be heavily involved in leading trials to successful completion, ing in close collaboration with sponsor teams, and Parexel teams in differing s. Depending on your career goals, you will be provided the opportunity to be exposed to a variety of different sponsors, products and therapeutic areas, or choose to focus your career on ing with a specific sponsor and gaining in depth knowledge of a specific therapeutic area and compound.

You will become a part of a team with an inherent breadth of knowledge and expertise in small, single center Investigator studies to multi-site, multi-national trials in Phases I to IV of clinical development, as well as observational studies. From initial planning meetings at the study design stage through to implementation and execution, our team of skilled statistical programmers will employ industry-accepted statistical  and techniques, as well as innovative consulting, to provide the quality and efficiency of .

Parexel develops and delivers cost effective, high-quality training programs for those employees that are directly involved with sponsor functional partnerships to ensure that our statistical programmers can quickly step in to make significant contributions to the success of a study. In addition, you will receive the opportunity to shape your development through quarterly conversations with your manager.

    Full Time
    Level: Mid
    Travel: Minimal


Success 

Check out the top traits we?re looking for and see if you have the right mix.

    Team player
    Tech-Savvy
    Communicator
    Proactive
    Detail-oriented
    Analytical


Rewards

    Global Impact

    We are one truly global team ing together to propel each client?s journey ahead faster.
    Balance

    We value -life balance. We try and keep regular hours and an emphasis on life outside the office.
    Development

    Opportunity to learn and grow through a performance and development goal-setting program.
    Health and Wellness

    Health and wellness programs, and a wide range of therapeutic areas and medical devices.
    Variety

    Opportunity to  on a wide range of therapeutic areas and medical devices.


What will you do as a Senior Programmer at Parexel?

You will provide technical expertise for the conduct of clinical trials, might act as an internal subject matter expert in specific areas providing technical support and expert advice, and s independently to support various programming activities related to the analysis and reporting of clinical study data. In addition, liaise with sponsors, Data Operations Leads, and other functional areas as .

    Input into and negotiate statistical programming timelines. Ensure that timelines are adhered to.
    Lead a statistical programming team to successful completion of a study within given timelines and budget
    Monitor project resourcing, project budgets, and identify changes in scope
    Interact with Sponsors as the key contact with regard to statistical programming issues
    Provide technical support and advice to the internal team
    Check own  in an ongoing way to ensure first-time quality
    Ensure quality control (QC) on all process and technical activities related to derived dataset, table, listing, and figure programming in accordance with corpo quality standards, WSOPs/Guidelines, ICH-GCP and/or other international regulatory  are performed.



  :

    Educated to a degree level (biological science, pharmacy or other health related discipline ) or relevant clinical or business equivalent
    5 plus years proficiency in SAS programming
    2+ years of ADaM, CDISC or SDTM
    Lead experience in Pharma
    Attention to detail, ability to be self-driven and solid organization 
    Excellent communication  (verbal and written)

EEO Disclaimer
PAREXEL is an . Qualified applicants will receive consideration for employment without regard to legally protected status, which in the US includes race, color, religion, sex, sexual orientation, gender identity, national origin, disability or protected veteran status.
29 

	",Data Engineer,,indeed,Job post
"Manager, Data Scientist, Natural Language Processing (NLP)
KPMG6,154 re - Irvine, CA 92618


Innovate. Collabo. Shine. Digital Lighthouse houses KPMG's specialized capabilities across the digital landscape: applied data science, AI, data engineering and insights, software engineering, automation, and big data. Here, you'll  with a sophisticated team of professionals to explore solutions for clients in a multiplatform environment. This means your ability to find answers is limited only by your creativity in leveraging a vast array of techniques and . Be a part of a high-energy, unique, fast-paced, and innovative culture that delivers with the agility of a tech startup and the backing of a leading global consulting firm. In this particular , you'll  specifically in the AI Analytics & Engineering Community within the Digital Lighthouse, on a wide range of . From applied AI to optimization to big data platform engineering, your analytical and  will drive real impact in the business world. So, bring your ingenuity and pioneering spirit to KPMG Digital Lighthouse.

KPMG is   a Manager to join our KPMG Lighthouse - Center of Excellence for Advanced Analytics.

:

    Lead shops and innovation sessions with clients, multi-disciplinary, and cross-functional teams to identify business opportunities and artificial intelligence solutions; utilize processes and best practices to plan, lead and execute delivery of artificial intelligence engagements across different areas (risk management, financial services, mergers and acquisitions, and public policy)
    Lead in a fast-paced and dynamic environment utilizing virtual and face-to-face interactions; manage complex  streams, expectations, budgets, deliverables, risks, and multiple  using structured approaches for operational excellence; Communicate results to executive level audiences
     with clients to discover data sources, and create data requests; Lead the ETL process to ingest structured data and annotation processes to enrich unstructured data leveraging a variety of data sources (social media, news, internal/external documents, images, video, voice, emails, financial data, and operational data)
    Utilize a variety of  and approaches to solve complex business s, from Statistical Natural Language Processing, Information Retrieval/Extraction, Machine Learning/ Deep Learning, Image Processing, Rules Engines, Knowledge Graphs, and Semantic Search
    Plan and manage engagement s and key deliverables using analytics processes to mitigate risks in data, modeling, validation and delivery
    Refactor, deploy, and vali models;  with clients iteratively to vali performance metrics, and sample output to drive towards a business-first solution utilizing APIs, platforms, containers, multi-threading, and distributed processing to achieve throughput goals

:

    Minimum of five years of experience leading teams of at least five data scientists, engineers, and other data & analytics professionals, including business development,  gathering, people development, and quality management using analytics and software development processes for natural language processing, machine learning on unstructured data, and/or information retrieval; Multidisciplinary backgrounds
    Master?s degree from an accredited college/university in Computer Science, Engineering, or related fields; PhD from an accredited college/university is 
    Ability to  with the business to understand business goals and create an artificial intelligence solution and an accompanying business case that meets the business s and business constraints. Expertise in delivering analytics  using leading processes including expert knowledge of data discovery, cleaning, model selection, validation and deployment
    Understanding of data preparation, machine learning, deep learning, natural language processing; ability to discuss mathematical formulations, alternatives, and impact on modeling approach; Knowledge of development practices (testing, code design, complexity, and code optimization)
    Fluency in Python; Proficiency in AI related frames (NLTK, Spacy, Scikit-Learn, Tensorflow); and experience with platforms such as Google Cloud, Azure, and AWS; Ability to pick up new languages and  quickly; Ability to  efficiently under Unix/Linux environment with experience with source code management systems like GIT; Ability to  with a variety of databases (SQL, ElasticSearch, Solr, Neo4j)
    Ability to travel up to eighty percent of the time; Applicants must be  authorized to  in the United States without the need for visa sponsorship now or in the future

KPMG LLP (the U.S. member firm of KPMG International) offers a comprehensive compensation and benefits package. KPMG is an affirmative action-. KPMG complies with all applicable federal, state and local laws regarding recruitment and . All qualified applicants are considered for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, protected veteran status, or any other category protected by applicable federal, state or local laws. The attached link contains further information regarding the firm's compliance with federal, state and local recruitment and  laws. No phone calls or agencies please.
29 

	",Data Scientist,,indeed,Job post
"Sr. Data Scientist
Trace312 re - Irvine, CA
 Now
Who is Trace3?

Trace3 is a leading Transformative IT Authority, providing unique  solutions and consulting services to our clients. Equipped with elite engineering and dynamic innovation, we empower IT executives and their organizations to achieve competitive advantage through a process of Integ, Automate, Innovate.

Our culture at Trace3 embodies the spirit of a startup with the advantage of a scalable business. Employees can grow their career and have fun while doing it!

Trace3 is headquartered in Irvine, California. We employ more than 850 people all over the United States. Our major field office s include Atlanta, Denver, Detroit, Indianapolis, Grand Rapids, Lexington, Los Angeles, Louisville, San Diego, San Francisco, and Scottsdale.

Ready to discover the possibilities that live in ?

Come Join Us!

Street-Smart - Thriving in Dynamic Times

We are flexible and resilient in a fast-changing environment. We continuously innovate and drive constructive change while keeping a focus on the ""big picture."" We exercise sound business judgment in making high-quality decisions in a timely and cost-effective manner. We are highly creative and can dig deep within ourselves to find positive solutions to different problems.

Juice - The ""Stuff"" it takes to be a Needle Mover

We get things done and drive results. We lead without a , empowering others through a can-do attitude. We look forward to the goal, mentally mapping out every checkpoint on the pathway to success, and visualizing what the final destination looks and feels like.

Team - Humble, Hungry and Smart

We are humble individuals who understand how our  impacts the company's mission. We treat others with respect, admit mistakes, give credit where it's due and demonst transparency. We ""bring the weather"" by exhibiting positive leadership and solution-focused thinking. We hug people in their trials, struggles, and failures ? not just their success. We appreciate the individuality of the people around us.

About the : Do you enjoy solving computer vision problems such as optical character recognition, object detection and recognition and image classification? Text and document classification using natural-language processing , or predicitve analytics applied to time series? Do you love learning and trying out new state of the art machine learning and deep learning algorithms? Are you passionate in building computer AI solutions and applications?

At Trace3, you will  on problems for our enterprise clients. You will design and build AI products that will impact some of the largest companies in the country. You will be able to learn from fellow deep learning and machine learning engineers and data scientists, and follow and  cutting-edge research. You will research and POC some of the coolest  available and will have access to a pipeline of up-and-coming  made available through Trace3's VC relationships.

The Senior Data Scientist needs to be self-motivated, self-directing, adaptable and comfortable ing in less-structured environments. The ideal candi should be looking for an opportunity to help client transform their business using ML/DL/AI. Candis will need experience in ing on and leading large complex data science and business transformation . There is an opportunity for the candi to eventually move into a leadership and management  as the Data Science team grows. Approximately 30-50% travel is .

What You'll Do:

    Perform project tasks independently, and may direct the efforts of others
    Lead analytical effort for client engagements
    Develop analytical  and act an advisor for junior staff and external partners
    Provide quantitative and business analytic  to develop and implement solutions across several industry verticals, specifically financial, industrial and manufacturing, healthcare life sciences, consumer product & retail, telecom.
    Define the key business problems to be solved and formulate mathematical approaches to solve those given problems
    Form and maintain trusted relationships with clients and be the 'go-to' contact point for 
    Participate in and/or lead the development of deliverable content that meets the needs of the client and contract
    Anticipate client needs and formulate solutions to client issues
    Review deliverables for accuracy and quality
    Contribute to new business proposals and proposal development
    Manages own personal and professional development; seeks opportunities for professional growth and expansion of consulting  and experiences

 & Interests:

    8+ years relevant experience with a proven track record of leveraging analytics and large amounts of data to drive significant business impact.
    Understanding of state of the art deep learning techniques
    Solid understanding of CNN, RNN, supervised and unsupervised learning, optimization techniques
    Experience with one or more deep learning frames such as TensorFlow, Deeplearning4j, Keras, PyTorch, etc.
    Proficiency in at least one higher-level programming language such as Java, Scala, Python, C++, R, etc.
    A focus on computer vision and/or machine learning and with Natural Language Processing (NLP) would be a plus.
    PhD in Machine Learning, Statistics, Computer Science, Operations Research, Engineering, Mathematics or related field would be a plus
    Proficient at translating unstructured business problems into an abstract mathematical frame.
    Exceptional interpersonal and communication , including the ability to describe the logic and implications of a complex model to all types of business partners.
    Self-starter with a passion for your  and attention to detail.
    Excellent written and verbal communication 
    Humble with a team-first attitude.

The Perks:

    Comprehensive medical, dental and vision plans for you and your dependents
    401(k) Retirement Plan with Employer Match, 529 College Savings Plan, Health Savings Account, Life Insurance, and Long-Term Disability
    Competitive Compensation
    Training and development programs
    Stocked kitchen with snacks and beverages
    Collaborative and cool office culture
    -life balance and generous paid time off

***To all recruitment agencies: Trace3 does not accept unsolicited agency resumes/CVs. Please do not forward resumes/CVs to our careers email addresses, Trace3 employees or any other company . Trace3 is not responsible for any fees related to unsolicited resumes/CVs.
30+ 
If you require alternative methods of application or screening, you must approach the employer directly to request this as Indeed is not responsible for the employer's application process.

	",Data Scientist,,indeed,Job post
"Applied Computational Mathematician / Engineer
Gustaine - Orange, CA
 Now

Number of s: 100

We are  for an applied computational mathematician/ engineer/ scholar to leader next-generation, complex, stochastic predictive simulations in a business practical real world scenario.

The  demands utilizing expertise in 1 or many of the of the following i.e. Applied mathematics, Statistics and computational science with the  of enabling uncertainty quantification and measurement at an extreme scale in a real world scenario across the following industries

o Telecommunications
o Biopharmaceuticals
o Metals & Mining
o Public Sector
o Consumer Products
o Process Industries
o Infrastructure
o Financial Institutions
o Retail
o Hardware & Software
o Social Welfare
o Health Care
o Telecommunications
o Insurance
o Consumer Products
o Media and Nets
o Oil & Gas
o Retail
o Utilities & Alternative Energy
o Medical Devices & 
o Automotive
o Transportation
o Travel & Tourism
o Airlines & Transportation
o Financial Services
o Industrial Services

The individual would be  to demonst scenario planning and visualize possibilities in a practical scenario setting

He should be able to communicate complex terminologies in a practical setting to a business audience



Fundamental  included

a. Participating in the development of Uncertainty Quantification & Measurement methodologies necessary for informed fact based decision-making
b. Design and execution of scalable numerical methods for Uncertainty Quantification and Measurement
c. Collaboration with experts in the above industry in 1 or many of the following area Statistics, Operations Research, Computer Engineering, Computer Sciences, Economics, Bio-Statistics, Bio-Informatics in the above industries
d. Design of experiments and building models with an expertise in the design and implementation of numerical algorithms and methods in one or more high-level computing languages within a team environment

Specialization in 1 or many of the following:

1. Model reduction
2. High-performance computing
3. PDE-constrained optimization
4. Computational fluid dynamics
5. Computational structural mechanics
6. Markovian Decision Processes

The individual should have applied 1 or many of the following in a practical setting/ scenario. Whilst ing /interviewing for the following  the individual would be  to talk and demonst how he or she has applied or will  the following

    Stochastic col and Galerkin methods

? Multilevel and reduced order methods
? Linear and Eigenvalue solvers
? Hierarchical and low-discrepancy sampling
? High-dimensional interpolation and integration

 

o The minimum  education is a BS/MS/ Ph.D. in applied mathematics, computational science or related field.
o Strong background in stochastic differential equations, partial differential equations, numerical analysis, methods for uncertainty quantification and high-performance computing.
o Excellent soft 
o Tested written and oral communication 
o Extensive expertise in numerical analysis of stochastic PDEs
o Experience in the development of large-scale numerical algorithms and simulation codes

Additional Information: Applicants should have received their latest degree not more than 5 years before making the application. All degree  must be completed before the starting of appointment.

We are necessarily not interested in individuals having all of the traits outlined above and below in as much that he/she has a willingness to acquire capabilities and has expertise in some of the areas. We will  with the specific individual to help him/her succeed.

For obvious reasons preference would be given to individuals having demonstd knowledge and expertise in many of the following.

Send in your resume: guptaneel@gustaine.com

Major Duties/

The  requires collaboration within a multi-disciplinary research environment consisting of mathematicians, computational scientists, computer scientists, experimentalists, and engineers/physicists conducting basic and applied research in support of the Laboratorys missions. Specific  include participating in the development of novel UQ methodologies necessary to facilitate science-informed decision-making, design and implementation of scalable numerical methods for UQ, collaboration with experts from various scientific disciplines on UQ and applications, and following team planning, goals and quality processes.

 

Minimum : The minimum  education is a Ph.D. in applied mathematics, computational science or related field. Strong background in stochastic differential equations, partial differential equations, numerical analysis, methods for uncertainty quantification and high-performance computing. Expertise in more than one area of particular relevance to simulations of interest, such as:

    Stochastic col and Galerkin methods
    Multilevel and reduced order methods
    Linear and eigenvalue solvers
    Hierarchical and low-discrepancy sampling
    High-dimensional interpolation and integration
    Design of experiments Demonstd experience in the design and implementation of numerical algorithms in one or more high-level computing languages within a team environment
    Effective interpersonal .
    Demonstd written and oral communication .
    Candis must have a proven publication track record.

 : Experience ing in a multi-disciplinary research environment. Extensive expertise in numerical analysis of stochastic PDEs Experience in the development of large-scale numerical algorithms and simulation codes.Additional Information:Applicants cannot have received the most recent degree more than five years prior to the  of application and must complete all degree  before starting their appointment.
Send in your resume: guptaneel@gustaine.com
10 
If you require alternative methods of application or screening, you must approach the employer directly to request this as Indeed is not responsible for the employer's application process.

	",Data Engineer,,indeed,Job post
"Senior Data Scientist
Kareo Inc2 re - Irvine, CA 92612

What We Need

    A Senior Data Scientist with solid problem-solving  to partner with the development teams.

Your Area of Focus

    Join a growing team of tech heads who love building things with ones and zeros
     in a fast-paced environment that sometimes fails fast and early, but always learns and improves
    Be unshackled by conventional thinking and allowed to use cool tech to solve hard problems
    Perform component design for a complex system or service. Considers scaling, reusability, maintainability, and performance into system or service design.
    Strong written and verbal communication  and ability to train and mentor Junior engineers
    Make recommendations to product  and business solutions based on an understanding of how Kareo's products  under the hood
    Write unit tests and integration tests for most of one's own  and ensure a high quality of deliverables


Your 

    Computer Science Degree (or degrees) or enough experience to convince us you do not need one
    Extensive knowledge of Python or R
    Ability to understand and interpret data and programmatically make predictions
    Experience with statistical  and visualizations techniques to extract hidden insights from large datasets
    Ability to use historical data to build models that can predict future outcomes and drive business decisions
    Experience with Microservices and CI/CD (Kubernetes is a plus)
    Deep understanding of latency, contention, computation, mutation, consistency, CAP theorem and system design trade-offs
    Experience in an UNIX environment
    Experience with SoA architectures and in SaaS products
    Extensive knowledge of RESTful APIs
    Familiarity with relational databases such as MySQL, Oracle, and SQL Server
    Experience with data warehousing  and 
    Ability to design and implement scalable big data pipelines and frames that can integ external data sources and third-party APIs
    Solid experience with concurrency, multithreading, server architectures, and distributed systems
    Experience with distributed search engines like Elastic search
    Strong problem solving and critical thinking 
    Strong attention to detail
    Process oriented: you can teach us a thing-or-two about machine learning and real-time data analytics in high throughput environment
    Deep understanding of the inner ings of one or more programming languages and tech stack
    Ability to build REST APIs that can process and distribute data to 3rd party applications at scale
    Experience with building AI bots
    A good understanding of current industry trends and best practices


Your Personal Characteristics

    Be Passionately Driven: We take pride in our , inspire others to excel, and are always curious to learn more. We hold ourselves to the highest standards of quality and integrity. We  with urgency because we love what we do.
    Dedicated to Customer Success: Helping our customers succeed is our number one goal and inspires every action we take. We want our customers' practices, and their patients, to thrive. We are empathetic, solution-oriented, and aligned with their needs.
    Together We're Better: We are honest, approachable, and collaborative. We believe great teams with members that are willing to do what it takes to get the  done can accomplish more. We put the team first and win together.
    Constant Innovation: We reject the status quo. We take a unique approach and make every effort to bring clarity to a needlessly complex industry. We are creative problem solvers. And we apply the same innovative thinking to our business and healthcare as a whole. We believe in making things better.


Kareo is an . All applicants will be considered for employment without attention to race, color, religion, sex, sexual orientation, gender identity, national origin, veteran or disability status.

uzoX6bZmqq
30+ 
If you require alternative methods of application or screening, you must approach the employer directly to request this as Indeed is not responsible for the employer's application process.

	",Data Scientist,,indeed,Job post
"
Senior Associate, Data & Analytics Modeler, Natural Language Processing

Irvine, CA; Washington, DC; Atlanta, GA; Chicago, IL; New York, NY; Dallas, TX; McLean, VA; Seattle, WA

Innovate. Collaborate. Shine. Digital Lighthouse houses KPMG's specialized capabilities across the digital landscape: applied data science, AI, data engineering and insights, software engineering, automation, and big data. Here, you'll work with a sophisticated team of professionals to explore solutions for clients in a multiplatform environment. This means your ability to find answers is limited only by your creativity in leveraging a vast array of techniques and . Be a part of a high-energy, unique, fast-paced, and innovative culture that delivers with the agility of a tech startup and the backing of a leading global consulting firm. In this particular role, you'll work specifically in the AI Analytics & Engineering Community within the Digital Lighthouse, on a wide range of . From applied AI to optimization to big data platform engineering, your analytical and technical skills will drive real impact in the business world. So, bring your ingenuity and pioneering spirit to KPMG Digital Lighthouse.

KPMG is currently seeking a Sr. Associate to join our KPMG Lighthouse - Center of Excellence for Advanced Analytics.

    Work in multi-disciplinary and cross-functional teams to translate business s into artificial intelligence approaches and s; capture business domains into knowledge bases, taxonomies, and ontologies across different areas (risk management, financial services, mergers and acquisitions, and public policy)
    Work in a fast-paced and dynamic environment with both virtual and face-to-face interactions; utilize structured approaches to solving problems, managing risks, and documenting assumption while communicating results and educating others through insightful visualizations, reports, and presentations
    Build ingestion processes to prepare, extract, and annotate a rich data variety of unstructured data sources (social media, news, internal/external documents, images, video, voice, emails, financial data, and operational data)
    Leverage a variety of  and approaches to solve complex business s (Statistical Natural Language Processing, Information Retrieval/Extraction, Machine Learning/ Deep Learning, Image Processing, Rules Engines, Knowledge Graphs, and Semantic Search)
    Deliver on engagement milestones by following analytics processes to manage and mitigate risks in data, modeling, validation, and delivery; Work with others to clear issues
    Refactor deploy and validate models; work with clients iteratively to validate performance metrics, and sample output to drive towards a business-first solution; utilize APIs, platforms, containers, multi-threading, distributed processing to achieve throughput goals

    Minimum of three years of experience leading work streams with two plus data scientists, engineers, and other data & analytics professionals, including innovation, quality management, utilizing analytics, software development processes, and modeling processes (data discovery, cleaning, model selection, validation, and deployment); Multidisciplinary backgrounds is preferred
    Bachelor's, Master's, or PhD degree from an accredited college/university in a quantitative discipline, such as Computer Science, Engineering, or Mathematics, including one plus year of training specific to artificial intelligence
    Ability to apply artificial intelligence techniques to achieve concrete business goals; ability to work with the business to understand available resources and constraints around data (sources, integrity, and definitions), processing platforms, and security; provide assistance, and resolve problems, using solid problem-solving skills, verbal/written communication
    Understanding of data preparation, machine learning, deep learning, natural language processing; ability to discuss pros and cons of modeling approaches; Understanding of development practices such as testing, code design, complexity, and code optimization
    Fluency in Python; proficiency in AI related frameworks (NLTK, Spacy, Scikit-Learn, Tensorflow); and experience with platforms (Google Cloud, Azure, and AWS); Ability to work with a variety of databases (SQL, ElasticSearch, Solr, Neo4j)
    Ability to travel up to eighty percent of the time; Applicants must be currently authorized to work in the United States without the need for visa sponsorship now or in the future

",Data Scientist,,Company Site,Job post
"  with around 5 years of experience in all phases of diverse   specializing in Data Science, Big Data, Azure Machine Learning, Google Cloud and Tableau, using Cloud based infrastructure.  
 ed on analyzing large datasets on distributed databases and developing Machine Learning algorithms to gain operational insights and present them to the leadership.  
 Extensively ed on Data preparation, exploratory analysis, Feature engineering using supervised and unsupervised modeling.  
 Experienced the full software life cycle in SDLC, Agile and Scrum methodologies.  
 Expert in using of statistical  and programming languages (R, Python, C, C++, Java, SQL, UNIX)  
 Adapted statistical programming languages like R and Python  
 Well-versed with Linear/non-linear, regression and classification modeling predictive algorithms.  
 Actively involved in model selection, statistical analysis using SAS and Gretl statistical tool.  
 Created dashboards as part of Data Visualization using Tableau.  
 Proficiency in using Spark for Bigdata processing in the Hadoop/DataProc/ EMRE ecosystem.  
 Performed preliminary data analysis using descriptive statistics and handled anomalies such as removing duplicates and imputing missing values using Talend tool.  
 Performed Dimensionality reduction using principal component analysis, auto encoders, and t-SNE.  
 Validate the consolidated data and develop the model that best fits the data. Interpret data from multiple sources, consolidate it, and perform data cleansing using R/Python/Spark.  
 Performed multiple Data Mining techniques and derive new insights from the data.  
 Skilled in Machine Learning, Statistical Modeling, and Big Data.  
 Creative problem-solver with strong analytical, leadership, and communication   
 Proficient in Python, R, Scala, Java, SQL, and C  
 Experienced in Machine Learning, Data mining with large datasets of Structured and Unstructured Data, Data Acquisition, Data Validation, and Predictive Modeling  
 Data Science Specialties include: Machine Learning, Sequential Modeling, Natural Language Processing (NLP)  
 Use of Analytical : Bayesian Analysis, Inference, Time-Series Analysis, Regression Analysis, Linear models, Multivariate analysis, Sampling methods, Forecasting, Segmentation, Clustering, Sentiment Analysis, Part of Speech Tagging, and Predictive Analytics  
 Experienced in stochastic optimization and regression with machine learning algorithms  
 Experienced in formulating and solving discrete and continuous optimization problems  
 Able to research statistical machine learning, supervised learning, and classification methods  
 Strong mathematical and statistical modeling and computer programming  in an innovative manner  
 Use of Various Analytics : Classification and Regression Trees (CART), Support Vector Machine (SVM), Random Forest, Gradient Boosting Machine (GBM), Principal Component Analysis (PCA), Regression, Nave Bayes, Support Vector Machines  
 Experienced in AWS cloud computing, Spark, and capable of ing with large datasets  
 Deep Learning: Machine perception, Data Mining, Machine Learning algorithms, Neural Nets, TensorFlow, Keras  
 Delivered presentations and highly  reports; collaboration with stakeholders and cross-functional teams, advisement on how to leverage analytical insights  
 Development of clear analytical reports which directly address strategic goals  
 Identified and learn applicable new techniques independently as needed  
 Able to  comfortably and effectively within an interdisciplinary research environment  
 Experienced with validation of machine learning ensemble classifiers  
 Utilized the online datasets to implement machine learning models using Spark ML for building prototypes 
Willing to relocate: Anywhere 
Authorized to  in the US for any employer 
 Experience 
Data Science Engineer 
Jefferies LLC - Jersey City, NJ 
February 2019 to Present 
Responsibilities:  
 Coded in Python with selenium and automated website data scraping  
 Scripted using R for cleaning, merging and extraction of relevant data  
 Created interactive visualization using Tableau and performed data analysis to report findings and trends  
 Analyzed massive data models with tables having over 100s of millions of records to draw insights and useful information.  
 Architect complex database systems on Hadoop to scale out data development processes.  
 Explore  and gather insights from Amfam's operational data stores and data warehouses (stored in Oracle, Greenplum, HDFS and S3) by querying them and creating data visualizations (in Tableau).  
 Designed and developed scripts to test data and find data defects.  
 Determined the quality of data, verify accuracy of information and ensure that the data is fit for modeling purposes.  
 Transformed data elements and attributes into usable form based on business requirements.  
 Blend data sets at different granularity levels using analytical queries, window functions and SQL joins.  
 Identified data duplicates and develop means to remove them.  
 Analyzed tabular data to determine or alter their grain (drill down or roll up) using analytical queries, MapReduce or python.  
 Designed and develop data pipelines to preprocess modeling data such as handle null values and clean up defective data attributes.  
 Developed Spark code to parse out and transform semi structured data such as XMLs, JSONs and CSVs into hive tables or data frames.  
 Explored and determined ways to organize data in Hive tables for fast read and writes through hive table partitions and buckets for optimized performance.  
 Developed programs to store data in appropriate file formats and logical grouping in tables.  
 Optimized code and queries to run faster and efficiently. Optimize ETL processes for distributed data marts on Hadoop  
 Maintained development activities in version control and create updated documentation.  
  
Environment: HADOOP (HDFS) Horton s, AWS, SPARK, Python, Java, Hive, Beeline, Apache pig, Tableau, SAS, Oracle, DB2, MySQL 
Data Scientist 
Anthem - Atlanta, GA 
June 2018 to February 2019 
Responsibilities:  
 Translated business questions into research s, design and conduct analyses, develop findings and synthesize recommendations to deliver valuable, relevant, and actionable insights  
 Strong track record of contributing to successful end-to-end analytic solutions (clarifying business s and hypotheses, communicating project deliverables and timelines, and informing action based on findings)  
 Used Pandas, NumPy, Scikit-Learn in Python for performing exploratory analysis and developing various machine learning models such Random forest  
 The missing data in the dataset is handled using Imputer method in SkLearn library  
 Performed categorical variable analysis using python Label Encoder, fit transform, One Hot Encoder methods in sklearn library  
 Responsible for design and development of advanced R/Python programs to prepare to transform and harmonize data sets in preparation for modeling  
 Defined a generic classification function, which takes a model as input and determines the Accuracy and Cross-Validation scores  
 Advanced SQL ability to efficiently  with very large datasets. Ability to deal with non-standard machine learning datasets  
 Built forecasting models in Python using Gradient Boost Regression Trees. Forecasted the revenue for future  
 ed with applied statistics and applied mathematics  for performance optimization  
 ed with K-Means clustering and Hierarchical clustering algorithm to do segmentation of stores  
 Collected various store attributes and added them into our segmentation model in order to better classify different segments using clustering algorithms  
 Used cross-validation to test the models with different batches of data to optimize the models and prevent over fitting  
 Analyzed the SQL scripts and designed the solution to implement using PySpark and developed scripts as per the requirement  
 ed with Tableau in order to represent the data in visual format and better describe the problem with solutions.  
  
Environments: Python, PyCharm, Jupyter, Notebook, Spyder, R, Tableau, MySQL 
Data Scientist 
US Bank - Brookfield, WI 
September 2017 to June 2018 
Responsibilities:  
 Used SQL alongside a variety or reporting  - BusinessObjects, Power BI, Tableau - to develop operational and visual reports for KPI monitoring  
 Data analysis and visualization (Python, R)  
 Designed, implemented and automated modeling and analysis procedures on existing and experimentally created data  
 Increased pace & confidence of learning algorithm by combining state of the art  and statistical methods; provided expertise and assistance in integrating advanced analytics into ongoing business processes  
 Parsed data, producing concise conclusions from raw data in a clean, well-structured and easily maintainable format  
 Implemented Topic Modelling, linear classifier models  
 Collaborate with UI engineers, project managers, and designers to develop web portal that aggregates reports from various sources and   
 Member of Data Science team tasked with helping clients turn data into a strategic asset  
 Focused on front end features, browser manipulation, and cross-browser compatibility.  
 Utilize  expertise including HTML5, CSS3, JavaScript, JQUERY, HTML, Node.js, Angular.js, and DOM to develop reporting portal.  
 Used Agile Scrum for BI  across different clients, which allowed for production prototyping, rapid deployment and transparency.  
  
Environment: Tableau, SQL, Java, HTML, Oracle, Agile, Hadoop. 
Junior Data Scientist 
Ordnance Factory Board - IN 
January 2016 to July 2017 
Description: Built a new team and managed in designing cost effective A/B tests to determine high performance marketing campaigns and contributed to increase in sales by 20% and reduced the promotional cost by 35%.  
  
Responsibilities:  
 ed on various phases of data mining- data collection, data cleaning, developing models, validation, visualization.  
 Captured Modelling requirements from Senior Stakeholders to fetch functional requirements for SAS/R, Python.  
 Performed Data Manipulation and Aggregation from various sources including HDFS and created various Predictive and Descriptive analytics using R and Tableau.  
 Used various libraries and developed various matching learning algorithms using Pandas, NumPy, Seaborn, Scipy, matplotlib, Scikit-learn in python.  
 Designed Predictive analysis algorithms using Historical Data.  
 Utilized machine learning algorithms as well as implemented algorithms such as Decision Tree, linear regression, multivariate regression, Naive Bayes, Random Forests, K-means, & KNN.  
 ed on Map Reduce/Spark Python modules for machine learning and predictive analytics in Hadoop on AWS.  
 ed on Reporting tool (Tableau) Test, Validate Data Integrity of Reports.  
  
Environment: Python, I Python, Scikit-Learn, MySQL, SQL, NoSQL, Data Modelling, Data Warehouse, Hadoop (MapReduce, HBase, Hive), Gradient Boost, Random Forest, Neural Nets, Sklearn etc. 
Data Science Intern 
iPrism Technologies - Hyderabad, Telangana 
April 2015 to December 2015 
Description: The project was to build a classification model predicting the probability of a customer who will not subscribe to paid membership, to help the marketing team to focus on improving the subscription rate.  
Responsibilities:  
 Collected data from end client, performed ETL and defined the uniform standard format  
 Wrote queries to retrieve data from SQL Server database to get the sample dataset containing basic fields  
 Performed string formatting on the dataset converting hours from date format to a numerical integer  
 Used Python libraries like Matplotlib and Seaborn to visualize the numerical columns of the dataset such as day of week, age, hour and number of screens.  
 Developed and implemented predictive models like Logistic Regression, Decision Tree, Support Vector Machine (SVM) to predict the probability of enrollment  
 Used Ensemble learning methods like Random Forest, Bagging & Gradient Boosting& picked the final model based on confusion matrix, ROC & AUC & predicted the probability of customer enrollment  
 Tuned the hyper parameters of the above models using Grid Search to find the optimum models  
 Designed and implemented K-Fold Cross-validation to test and verify the model's significance  
 Developed a dashboard and story in Tableau showing the benchmarks and summary of model's measure.  
  
Environment: SQL Server 2012/2014, Python 3.x (Scikit-Learn, NumPy, Pandas, Matplotlib, Dateutil, Seaborn), Tableau, Hadoop 
Education 
Master's in Computer Engineering in Data Science and Analytics 
Arizona State University - Tempe, AZ 
 
Apache spark, Hadoop, Hive, Javascript, D3.js, Mapreduce, Natural, Pig, Python, Mapreduce, Data science, Hadoop, Machine learning, Natural language processing, Nosql, Ms sql server, Sql server, Mysql, Pl/sql, Sql 
Additional Information 
 :  
Programming languages Python, Java, R. C  
Data  and Frames s Hadoop (Apache Spark, Pig, Hive, MapReduce), D3.js, Tableau, MATLAB, R-Studio  
Data Science Machine Learning, Deep Learning, Natural Language Processing, Computer Vision, Neural Nets, Machine Perception  
Operating systems Windows, Linux.  
Databases MySQL, MS SQL Server, NoSQL  
Web and Cloud Technologies AWS, HTML5  
Languages Python 3.7, SQL, R 3.6, Java, JavaScript, PL/SQL",Data Scientist,emailed,indeed job post,resume
"
 
Data scientist with a strong math background and 2+ years of experience in big data, machine learning, and statistics. Passionate about explaining data science to non- business audiences. 
 
 
        Programming: Python, R, SQL, CSS, HTML, Shell 
        Libraries: scikit-learn, pandas, cartopy, xarray, scipy, nltk, xgboost, hyperopt, matplotlib, seaborn 
        Machine Learning: Decision Trees, SVM, Neural Nets, Regression, Clustering, Boosting, Ensemble 
        Statistical Techniques: Time Series, Probability, Hypothesis Testing, Factor Analysis, ANOVA, PCA         : Git, AWS, Spark, Hadoop, Excel, Tableau, MS Office, Command Line, Jupyter Notebook 
 
 EXPERIENCE 
Data Scientist                                                                                                                                                                  Oct 2017  Present 
University of California, Irvine, CA 
 ing as a lead developer on NASA sponsored project to analyse climate change in Greenland and Antarctica 
 Identified winter-time melt events with around 90% accuracy using Random Forest and GBM algorithms 
 Discovered patterns, trends and indicators for global temperature change using Time-Series analysis 
 Translated findings to both  and non- audiences using Tableau dashboards 
 Developed open-source Python package JAWS, which translates data from idiosyncratic ASCII formats into homogenized netCDF format 
 Designed unit-testing frame to evaluate accuracy, robustness and fault-tolerance at each stage 
 Developed internal infrastructure, data pipelines and queries in SQL using a variety of data files 
 
Data Science Associate Instructor                                                                                                                         Sep 2016  Aug 2017 
Indiana University, Bloomington, IN 
 Assisted students in Python and R with special focus on machine learning models (Ensembling, XGBoost, etc.) 
 Delivered a guest lecture on Time Series Analysis and its implementation in R. 
 Identified the needs of learners and adapted course content and delivery style to meet their needs 
 
 
 Telstra Net Challenge: Developed a multi-class classification model to predict the severity of service disruptions on Telstras net. Built the model using Random Forest as well as XGBoost and used the Hyperopt library for tuning the parameters. (Kaggle Rank 55/974) 
 Predicting Correct Orientation of a given Picture: Implemented a fully-connected feed-forward net to classify image orientation, with backpropagation algorithm to train the net using gradient descent. Implemented K-Nearest Neighbor. (Neural Net, kNN) 
 Predicting Business Success Attributes: Developed a model using Yelp Dataset that helps businesses to identify what factors should they focus to make more profit or help new businesses to identify which location would be ideal for their business to thrive. (Python, Tableau) 
 Exploration & Exploitation of Darwins Reading: Discovered correlation between Darwins reading patterns and biographically important events. (RStudio, LDA, PCA, k-means) 
 Twitter Dataset Modelling and Analysis: Created MongoDB schemas for representing the twitter data, classified users based on their location and used Google Geocoding API for visualization. (MongoDB) 
 
  
Indiana University, Bloomington, IN                                                                                                                                 2015  2017  
Master of Science in Data Science 
 
Kurukshetra University, Kurukshetra, INDIA                                                                                                                   2009  2013 
Bachelor of  in Computer Science ",Data Scientist,emailed,indeed job post,resume
"


* Around 4+  years of experience in Data Analysis, Data Conversion, Data Validation, Data Profiling, UAT Testing and Report Creation and ing experience in Tableau, Teradata, AWS Redshift, AWS S3, Python, Unix and Oracle. 
* Experience in Teradata, SQL and Utilities like Tpump, Multiload and Fast load. 
* Good experience in Developing Teradata SQL queries and using Utilities such As BTEQ 
* Strong experience in using Excel and MS Access to dump the data and analyze based on business needs. 
* Experience in Creating Teradata SQL scripts using OLAP functions like rank and rank () Over to improve the query performance while pulling the data from large tables. 
* Experience in data analysis using Python (Pandas, NumPy) 
* Experience in moving data to cloud platform like AWS (S3) and manipulate data using Redshift. 
* Strong  in statistical methodologies such as A/Btest, experimentdesign, hypothesistest, ANOVA.  
*  Extensively ed on R platform for the data analysis and predictive modeling.  
*  Experience in implementing data analysis with various analytic , such as Anaconda 4.0, R 3.0 (ggplot2, Caret, dplyr) and Excel.  
*  Solid ability to write and optimize diverse SQLqueries, ing knowledge of RDBMS like SQLServer2008  
*  Experience in BigData  like Spark1.6, Sparksql, pySpark, Hadoop2.X, HDFS, Hive1.X.  
*  Experience in visualization  likeTableau9.X, 10.X, DataBlendingfor creating dashboards.  
*  Proficient in PredictiveModeling, Datamining Methods, FactorAnalysis, ANOVA, Hypotheticaltesting, normal distribution and other advanced statistical and econometric techniques.  
*  Developed predictive models using DecisionTree, RandomForest, NaiveBayes, LogisticRegression, ClusterAnalysis, and NeuralNets.  
*  Excellent knowledge of Machine Learning, Mathematical Modeling and Operations Research. Comfortable with R, Python, SAS , MATLAB, Relationaldatabases. Deep understanding & exposure of BigDataEco-system.  
*  Expert in creating PL/SQLSchemaobjects like Packages, Procedures, Functions, Subprograms, Triggers, Views, MaterializedViews, Indexes, Constraints, Sequences, ExceptionHandling, DynamicSQL/Cursors, NativeCompilation, CollectionTypes, RecordType, ObjectType using SQLDeveloper.  
*  Hands on Experience in implementing ModelViewControl (MVC) architecture using Spring, JDK, CoreJava (Collections, OOPSConcepts), JSP, Servlets, Struts, springs, Hibernate, JDBCand provided ServerAdministrator duties LogicalPosition.  
*  ed with complex applications such as R, SAS, Matlab,andSPSS to developeda neuralnet, cluster analysis.  Delivered in software development life cycle for various ETL and BI . Provided end-to-end solutions for requirement analysis, design, development, testing, and application maintenance.
* Identified organization process inefficiencies and gaps, implemented process improvement methodologies to streamline the process and to increase efficiencies.
* Maintained the application code quality at the organization level ensuring robust, stable applications
* Automated the processes requiring manual interventions using SSIS, SQL, SSRS. Power BI, and Tableau+ to reduce number of incidents, to maximize customer satisfaction, and to increase application usability.
* Tuned SQL queries to reduce cycle time, improving process execution time and ultimately stabilizing applications.
* Demonstrated attention to details, decision-making, problem solving, and critical thinking in the service delivery.
* Communicated findings using the most effective form of written and verbal communication to non- users.
* Experience in ing under Agile (Scrum) and waterfall models of software development life cycle.
* Ability to meet deadlines and handle multiple tasks, team player, motivated, able to grasp things quickly with analytics and problem-solving .


 :

Operating Systems    
Windows XP, Windows 7/10
Specialties                                                                     
   Data Visualization, Business Intelligence (BI),     Software Management, Data Collection.
Languages
 SQL, PL/SQL, XML, HTML, Java, C,Python, R language, Machine Learning 
BI 
Tableau Desktop/Public/Server, QlikView
Data Bases                   
Oracle 11g/10g, MS SQL Server 2008, Teradata, Snowflake, Amazon Web Serices (AWS)
Document Management
SharePoint
Modeling                    
Dimensional Data Modeling, Multi-dimensional modeling, Snowflake/Star Schema
Others                                    
MS Word, Excel, Visual Studio, Astah, Rational rose, Notepad++, JIRA                                                               Experience 
Data Scientist 
Dollar shave Club  Marina Del Rey, CA
November 2017 to Present
Responsibilities:

 Built data pipelines for reporting, alerting, and data mining. Experienced with table design and data management using HDFS, Hive, Impala, Sqoop, MySQL, Mem SQL, Grafana/Influx DB, and Kafka.  
 ed with statistical models for data analysis, predictive modeling, machine learning approaches and recommendation and optimization algorithms.  
 ing in Business and Data Analysis, Data Profiling, Data Migration, Data Integration and Metadata Management Services.  
 ed extensively on Databases preferably Oracle 11g/12c and writing PL/SQLscripts for multiple purposes.  
 Built models using Statistical techniques like Bayesian HMM and Machine Learning classification models like XG Boost, SVM, and Random Forest using R and Python packages.  
 ed with data compliance teams, data governance team to maintain data models, Metadata, data Dictionaries, define source fields and its definitions.  
 ed with Big Data  such Hadoop, Hive, Map Reduce  
 Developed MapReduce/Spark Python modules for machine learning & predictive analytics in Hadoop on AWS. Implemented a Python-based distributed random forest via Python streaming.  
 Setup storage and data analysis  in Amazon Web Services cloud computing infrastructure.  
 A highly immersive Data Science program involving Data Manipulation & Visualization, Web Scraping, Machine Learning, Python programming, SQL, GIT, Unix Commands, NoSQL, MongoDB, Hadoop.  
 Performed scoring and financial forecasting for collection priorities using Python, R and SASmachinelearning algorithms.  
 Handled importing data from various data sources, performed transformations using Hive, MapReduce, and loaded data into HDFS  
 Managed existing team members lead the recruiting and on boarding of a larger DataScience team that addresses analytical knowledge requirements.  
 ed directly with upper executives to define requirements of scoring models.  
 Developed a model for predicting repayment of debt owed to small and medium enterprise (SME) businesses.  
 Developed a generic model for predicting repayment of debt owed in the healthcare, large commercial, and government sectors.  
 Created SQL scripts and analyzed the data in MS Access/Excel and ed on SQL and SASscript mapping.  
 Developed a legal model for predicting which debtors respond to litigation only.  
 Created multiple dynamic scoring strategies for adjusting the score upon consumer behavior such as payment or right-party phone call.  
 Rapid model creation in Python using pandas, numpy, sklearn, and plot.ly for data visualization. These models are then implemented in SAS where they are interfaced with MSSQL databases and scheduled to update on a timely basis.  
 Data analysis using regressions, data cleaning, excel v-look up, histograms and TOAD client and data representation of the analysis and suggested solutions for investors  
 Attained good knowledge in Hadoop Data Lake Implementation and HADOOP Architecture for client business data management.  
 Identifying relevant key performing factors; testing their statistical significance  
 Above scoring models resulted in millions of dollars of added revenue to the company and a change in priorities of the entire company.  
  
Environment: R, SQL, Python 2.7.x, SQL Server 2014, regression, logistic regression, random forest, neural nets, Topic Modeling, NLTK, SVM (Support Vector Machine), JSON, XML, HIVE, HADOOP, PIG, Sklearn, SciPy, Graph Lab, No SQL, SAS, SPSS, Spark, Hadoop, Kafka, HBase, MLib


Data Scientist
Citi-Bank  Irving, TX
January 2017 to September 2017

 Performed Data Profiling to learn about behavior with various features such as caller ID, traffic pattern, location, number validity.  
 Application of various machine learning algorithms like decision trees, regression models, neural nets, SVM, clustering to identify fraudulent profiles using scikit-learn package in python.  
 Used clustering technique K-Means to identify outliers and to classify unlabeled data.  
 Evaluated models using Cross Validation, Log loss function, ROC curves and used AUC for feature selection.  
 Analyze traffic patterns by calculating autocorrelation with different time lags.  
 Ensured that the model has low False Positive Rate.  
 Used Principal Component Analysis in feature engineering to analyze high dimensional data.  
 Created and designed reports that will use gathered metrics to infer and draw logical conclusions of past and future behavior.  
 Performed Multinomial Logistic Regression, Random forest, Decision Tree, SVM to classify Scammer, Telemarketer.  
 Performed data analysis by using Hive to retrieve the data from Hadoop cluster, SQL to retrieve data from Oracle database.  
 Implemented rule-based expertise system from the results of exploratory analysis and information gathered from the people from different departments.  
 Performed Data Cleaning, features scaling, features engineering using pandas and NumPy packages in Python.  
 Developed MapReduce pipeline for feature extraction using Hive.  
 Created DataQualityScripts using SQL and Hive to validate successful data load and quality of the data. Created several types of data visualizations using Python and Tableau.  
 Communicated the results with operations team for taking best decisions.  
 Collected data needs and requirements by Interacting with the other departments within CitiBank.  
  
Environment:Linux, Hadoop, Python, Machine learning, MapReduce, HDFS, Python Libraries (NumPy/Pandas), Tableau, GIT, NLP, Neural Net, Hive, SQL


Data Analyst
Radiare software solutions-Ganga Nagar, Bangalore, India
October 2014 to November 2015

 Collaborated with data engineers to implement the ETL process, wrote and optimized SQL queries to perform data extraction and merging from Oracle.  
 Involved in managing backup and restoring data in the live Cassandra Cluster.  
 Used R, Python, and Spark to develop a variety of models and algorithms for analytic purposes.  
 Performed data integrity checks, data cleaning, exploratory analysis and feature engineer using R and Python.  
 Developed personalized product recommendation with Machine learning algorithms, including Gradient Boosting Tree and Collaborative filtering to better meet the needs of existing customers and acquire new customers.  
 Used Python and Spark to implement different machine learning algorithms, including Generalized Linear Model, RandomForest, SVM, Boosting and Neural Net.  
 Evaluated parameters with K-Fold Cross Validation and optimized performance of models.  
 ed on benchmarking Cassandra Cluster using the Cassandra stress tool.  
 A highly immersive Data Science program involving Data Manipulation and Visualization, Web Scraping, Machine Learning, GIT, SQL, UNIXCommands, Python programming, No SQL.  
 ed on data cleaning, data preparation, and feature engineering with Python, including Numpy, Scipy, Matplotlib, Seaborn, Pandas, and Scikit-learn.  
 Identified risk level and eligibility of new insurance applicants with MachineLearning algorithms.  
 Determined customer satisfaction and helped enhance customer using NLP.  
 Utilized SQL and HiveQL to query, manipulate data from variety data sources including Oracle and HDFS, while maintaining data integrity.  
 Performed data visualization and Designed dashboards with Tableau and D3.js and provided complex reports, including charts, summaries, and graphs to interpret the findings to the team and stakeholder.  
 
Environment: R, MATLAB, MongoDB, exploratory analysis, feature engineering, K-Means Clustering, Hierarchical Clustering, Machine Learning), Python, Spark (MLlib, PY Spark), Tableau, Micro Strategy, SAS, Tensor Flow, regression, logistic regression, Hadoop 2.7, OLTP, random forest, OLAP, HDFS, ODS, NLTK, SVM, JSON, XML and MapReduce.
SQL Developer
Radiare software solutions-Ganga Nagar, Bangalore, India
April 2014 to September 2014

 Gather data from different data sources like MySQL, Oracle and SQL Server
 Develop reports, dashboards using Tableau 9.3 for quick reviews to be presented to Business and IT users.
 Developed POCs by building reports and dashboards using Tableau to perform
 Statistical analysis of large data
 Developed Ad-hoc reports using Tableau Desktop, Excel
 Prototyped data visualizations using Charts, drill-down, parameterized controls using Tableau to highlight the value of analytics in Executive decision support control.
 Developed visualizations using sets, Parameters, Calculated Fields, Dynamic sorting, Filtering, Parameter driven analysis.
 Used SQL Server Reporting Services (SSRS) and SQL Server Management Studio 2008 -SSIS, SSAS, and SQL Profiler, including Crystal Reports to run scripts and maintain stored procedure and code functions across company's Databases
 Performed schema changes in the database per request and met with clientele and with various subgroups for requirement gathering in order to allow self-extracted reports and created critical field reporting  for Enterprise Data Management customer segments
 Validated data to correctly reflect reservations for passengers and carrying capacity of cargo.
 Created SQL stored procedures using SQL Query Analyzer and Transact-SQL as data sources for
 reports.
 Trained, mentored, supervised, and advised over a dozen analysts in the development of programs.
 Acted as a point of contact for answers to questions over two groups.
 Trained analysts in development standards; according to corporate models and paradigms.
 Developed databases to interface with SQL Server, Oracle, and MS Access databases.
 Developed and supported applications for the Revenue Management and Revenue Management

Environment: Tableau, SQL, UNIX, JSON, Jira, HADOOP (HDFS), Hive, Sqoop, Spark, JAVA, HBase, AWS, SSIS, Teradata, Oracle 10g.

Education:

* Bachelor of Technology, Electronic 	                                                      2010-2014
  Auroras Technological and Research Institute, Hyderabad, India.

* Masters in Information System                                                                        2016                             The University of Mary-Hardin Baylor, Belton, TX


",Data Scientist,emailed,indeed job post,resume
"

Expert in logical and problem-solving  with strong knowledge in math and statistical concepts. Ability to take strong business judgement for ambiguous problems and solve them in a structured, hypothesis-driven and data-supported way. Data geek with strong programming background, experience in building and deploying machine learning and predictive models.

 :
> 4+ years of  experience as a Data Scientist/ Data Analyst, including deep expertise and experience with Statistical Analysis, Data Mining and Machine Learning  using R, Python and SQL.
> Extensive programming  in analytical and statistical programming languages such as python, R, SAS, and SQL.
> Data Driven and highly analytical with ing knowledge and statistical model approaches and methodologies like Clustering, Regression analysis, Hypothesis testing, Decision trees, Machine learning rules and ever evolving regulatory environment.
>  ing experience in Machine Learning algorithms such as Linear Regression, Logistic Regression, Random Forests, Decision Trees, K-Means Clustering and Association Rules.
> Involved in the entire data science project life cycle and actively involved in all the phases including data extraction, data cleaning, statistical modeling and data visualization with large data sets of structured and unstructured data.
> Extensive ing experience in writing SQL queries.
> Knowledge and experience in various sectors like Banking services, Healthcare and Industries.
> Involved in entire lifecycle of software development (SDLC) and Data Science Project lifecycle.
> Knowledge about Auditing the data to assess its quality or profitable for a specific purpose
> Proficient in statistical programming language for analyzing data using R and SAS
> Experience ing on Python libraries such as Numpy, Pandas, scikit learn, NLTK, Keras and Tensor flow.
> Sound knowledge on interpreting data analysis using multivariate data  longitudinal and mixed models.
> Knowledge on supervised/unsupervised learning, models classification, parametric/non- parametric and Neural Net algorithms.
> Knowledge about Database management system i.e. systematic way to create, retrieve, update and manage data.
> Experience in Machine Learning, Deep Learning, and Data mining with large datasets of Structured/Unstructured Data, Data Acquisition, Data Validation, Predictive Modeling and Data Visualization.
> Proficient in Tableau, Qlik and R-Shiny data visualization  to analyze and obtain insights into large datasets.
> Effective communication , experienced in progressing problem statement to documentation, have knowledge of deploying code on GitHub, perceptive to details and has great commitment to deadlines.

 :

Languages		: 	Python, R, SAS, Java, ASP.NET
Operating Systems	: 	Windows, Linux, Unix, and Mac OS
Database		:	Oracle, MySQL, MS SQL, Sqlite
Big Data Eco Systems	: 	HDFS, Spark, Flume, Pig, Hive, HBase
Regression Techniques: 	Linear regression, Logistic Regression, Gradient Descent
Data 		: 	NumPy, Pandas, Scikit-learn, Tensorflow, NLTK, ggplot2, 
				Advanced Excel
Reporting 	: 	Tableau, Power BI
Statistical Methods	: 	Time series, Regression models, hypothesis testing,
				Supervised/unsupervised Algorithms, Bayesian statistics, SVM 
				And deep learning.
: 

Infuzion Solutions, Delaware						June 2018 -Present
Data Scientist 

Responsibilities:
> Using SQL to analyze the companys Dataset and create the reports using SSRS.
> Developed the SAS models to import the Excel dataset into SQL.
> Using performance point dashboard to deploy the report to SharePoint.
> Performed statistical analysis to determine key factors for planning and conducting experiments to prove Total fraud loss using prescriptive and predictive analytics by application of appropriate machine learning algorithms.
> Designed and   built real-time   contextual behavior personalization system using econometric and machine learning to predict customer's behavior and help them to navigate through products of their choice. 
> ed in low latency models to learn and predict online, enabling it to respond constantly to changes in card member's payment behavior at various stages, with accuracy over 90%.
> ed  with  credit card  Profit  and  risk  analysis  team  to  analyze  the  current  customer  base  to  model  better offers and schemes to maximize the profits.
> Developed statistical models and applied them to assign a risk score to credit applications and existing credit accounts.
> Hands on experience with pivot tables and Lookups.
> Improved the accuracy of the developed statistical models and monitored the effect that score-based decisions have on key business performance indicators.
Norfolk Southern, Atlanta GA        					         Nov 2017  May 2018
Data Analyst

Responsibilities:
> Analyzed and processed complex data sets using advanced querying, visualization and analytics .
> Create complex SQL stored procedures, Triggers, Functions, Views, Indexes in Microsoft SQL Server
> Scheduled jobs to automate different database related activities such as backups, maintaining index, and monitoring disk space and backup verification.
> Developed subject segmentation algorithm using R.
> Involved in the process of load, transform, and analyze data from various sources into HDFS (Hadoop Distributed File System) using Hive, Pig and Sqoop.
> Used Python 3.0 (numpy, scipy, pandas, scikit-learn, seaborn, NLTK) and Spark 1.6 / 2.0 (PySpark, ML) to develop variety of models and algorithms for analytic purposes.
> Developed an algorithm that can identify bad assessments that are expected to Fail under central review.
> Used statistical methods to analyze the performance of each clinical site across 27 countries on 30 studies, predicting number of days to reach the target number of sites for a clinical study.
> Processed huge datasets (over billion data points, over 1 TB of datasets) for data association pairing and provided insights into meaningful data association and trends.
> Developed pipelines for test data.
> Enhanced statistical models (linear mixed models) for predicting the best products for commercialization using Machine Learning Linear regression models, KNN and K-means clustering algorithms.
> Builds machine learning models on independent AWS EC2 server to enhance data quality.
> Handle Unstructured Data to derive some information from which helps in development of the company.
> Finding the sentiment about the organization using Text Mining and NLP techniques.

Deutsche Bank, NYC 							June 2015  Oct 2017
Jr Analysts 

Responsibilities:
> Predicted sales of a store given the store attributes to plan budgeting, staffing, and marketing.
> Built an ensemble of machine learning models to forecast sales and perform model comparison using MAPE and segmented the customers based on demographics using K-means Clustering.
> Generated insights from the clusters to perform target marketing based on the customer  such as age, , income levels and median rooms.
> Developed data pre-processing modules and rule extraction engines in Python using Random Forest and Decision Trees for an analytics product and reduced the execution time by 30%.
> Performed data discovery by generating compliance trend reports to show the calculated value for certain compliance rules over time for significant KPI's.
> Developed adhoc reports on top performing products on a weekly basis using tableau.
> Develop campaign insights from a combination of proprietary client data, syndicated industry information, with geographically relevant consumer and vendor demographics.
> Audited and validated the customer selection results, segmented based on behavior, predictive models, and other criteria.
> Supported new products, communication methods, marketing campaigns, and brands.
> Created visualizations, reports and dashboards to present clients and stakeholders.

Academic  and Related Course:
* Predicting house prices with relevant features after feature scaling.
* Email Spam classifier using logistic regression and gradient descent
* Photo OCR technique
* PageRank algorithm visualization
* Web scraped using JSON
* Generated word cloud from webpages

Online Courses
Machine Learning-Stanford University
https://www.coursera.org/account/accomplishments/verify/759VDG99SK3H
Python Specialization-University of Michigan 
https://www.coursera.org/account/accomplishments/specialization/MNVVDDY5D86T


Bachelors Degree in Computer Science and Engineering
Hindustan College of Engineering
Chennai, India
",Data Scientist,emailed,indeed job post,resume
"Experienced  with 2+ years of hands-on experience in statistical analysis using Python/R/SQL/SAS/Tableau/Excel interested in leveraging analytics to develop strategies. 
 
 
University of Connecticut School of Business  	 	 	 	 	 	                                                                                            Hartford, CT 
Master of Science in Business Analytics and Project Management, GPA: 3.74/4.00                                                                                                                                          Dec 2019 (Expected)  
Relevant Course: Statistics with R, Predictive Modeling, Data Mining and Business Intelligence, Python, Survival Analysis with SAS, Big Data/Hadoop 
 
Guru Gobind Singh Indraprastha University 	 	 	 	 	                                                                                                                             New Delhi, India 
Bachelor of Engineering in Electronics and Communication                                	                                                                                                                                                            May 2017 
 
  
 Statistical Analysis: Linear Regression, Logistic Regression, Dimension Reduction, Decision Trees, Random Forests, Boosted trees, Feature Selection, Bagging, Neural Nets, Clustering, Machine Learning, Time Series, Recommender Systems, Forecasting, Sentiment Analysis, Data Mining, Statistical Process Control, matplotlib, scikit-learn, scipy, pandas, numpy, ggplot2, dplyr, gvlma, statsr, caret,  
 : Python, R, Hadoop (HDFS, Sqoop, Pig, Hive, Spark), MapReduce, Scala, SQL (Oracle/Postgre), SAS, AWS (S3, Glue, IAM), JMP, Linux, Tableau, Visio, Advanced 
Excel, JAVA 
 
Experience 
Keysight , Santa Rosa, CA 
Data Analyst Intern (Python, R, JMP, SQL)                                                                                                                                                                                                                  May 2019- present 
 Devised a mathematical model to reduce the measurement locations on a production wafer using Neural Nets, Feature Selection, Clustering and Combinations by text parsing more than 11000 files (.txt) on Python (sklearn, scipy, matplotlib, iter, pandas, numpy). 
 Automated Gage R&R study between new and old profilometers to measure performance and variance using Hypothesis Testing (One-Sample, Two Sample t-tests) resulting in a 14% increase in manufacturing technician productivity on Python, JMP and R. 
 Developed a GUI platform to predict the IC Test Wafer time and email the operator (on real time basis) with a comprehensive list of all the wafers with their test times and estimated completion dates using Time Series and Advanced SPC on Python (pandas, tkinter). 
 
Deep Play for Kids, LLC (UConn Consulting Group), Hartford, CT 
Analytics Consultant (Python, R, SQL)                                                                                                                                                                                                                      Oct 2018  Apr 2019 
 Recommended solutions to increase revenue and maximize operational efficiency by 35%. 
 Performed A/B Testing and ETL on websites to identify ways to improve content which led to higher traffic using Python, R, and SQL. 
 Created predictive statistical models to identify target customers using data from engagement surveys, and behavioral measures. 
 
Accenture, Gurugram, India 
Data Analytics Associate (SAS, SQL, Excel)                                                                                                                                                                                                               Sep 2017  Jul 2018 
 Reduced organizational problems by 18% for the BMT (Business Manufacture Transform) owners through regression analysis, hypothesis testing and k-means clustering in order to determine new possibilities for owners using SAS. 
 Implemented data migration, cleansing and profiling by analyzing large volumes of banking data to identify useful trends and patterns for Budgeting/Forecasting using SAS, SQL and Advanced Excel. 
 Performed ETL on banking data by executing complex SQL queries to generate analysis on datasets with over millions of records to derive customer insights. 
 
Quality Council of India, New Delhi, India 
Data Analyst (R, Python, Tableau, Excel)                                                            	 	 	 	                                                                             Apr 2017  Aug 2017 
 Conducted regression analysis (decision trees and random forests) and hypothesis testing with 76% accuracy using R (ggplot2, gvlma, caret) and Python, to bridge the gaps and suggest possible policy solutions to NABET, India. 
 Created visually impactful dashboards in Tableau and explored missing values, outliers, incorrect data using R (dplyr, tidyr, ggplot2), Python (matplotlib/numpy) and Advanced Excel (VLOOKUP, pivot analysis) for 107 NGOs pan India. 
 Successfully developed strategies for curtailing the rake turnaround time by building regression models (linear/logistic) to reduce the platform congestion at New Delhi Railway Station, piloted in June 2017. 
 
Analytics  
Text mining and Sentiment Analysis -Amazon Reviews Dataset (Python/R/Tableau) White Paper                                                                                                                                 April 2019 
 Created word clouds, text clusters followed by sentiment analysis of Amazon unlocked phones reviews using bag of words model, tfidf and ngram with an accuracy of 
97.8% using Python (matplotlib, sklearn, countvectorizer, tfidf, ngram), SAS EM, R (ggplot2) and Tableau. 
 
Black Friday Sales Prediction (R/Python/SAS JMP/Excel) White Paper                                                                                                                                                                                   Oct 2018 
 Estimated the amount of purchase with 83% accuracy, by performing regression analysis (decision trees, neural nets and LASSO) on Black Friday sales data, using 
R (ggplot2, caret), Python (matplotlib, sklearn) and SAS JMP 
 
Modeling and Prediction of Movies (R/Excel) Certificate                                                                                                                                                                                                        Sep  2016 
 Predicted what attributes make a movie popular by completing stepwise regression analysis with 72% accuracy on data from Rotten Tomatoes and IMDB for a random sample of movies, using R (dplyr, statsr, ggplot2) and MS Excel. 
 
Leadership/Certifications 
 Data Scientist at LIMRA, Health Insurance, building predictive models to review customer s that are likely to lapse - Aug 2019-present 
 Advanced Statistical Process Control- Keysight-Mic Quality - Jul 2019 Certificate 
 Python for Data Science-IBM-Jan 2019 Certificate 
 Programming in R for Data Science-Microsoft (edX)- Nov 2016 Certificate ",Data Scientist,emailed,indeed job post,resume
"
 
Self-directed  with over ten years of analytics experience.   functions have included project coordination, consulting, training, programming, data mining, reporting, training, and teaching.  Seeking role with an organization that would allow me to showcase my mastery of data exploration/visualization/modeling, statistics, and machine learning.

 AND EXPERIENCE Analytics
 Devised an XGBoost regression model in Python to predict revenue as a function of marketing spend for a dental support organization. Utilized the predictive model to optimize marketing spend resulting in an average cost reduction of 70%.
 Modeled dental patient no-shows with a logistic regression in R to determine key driving factors.
 Designed a direct-mail marketing campaign and advised client on necessary statistical testing.
 Developed a CART model in SAS Enterprise Miner to find primary drivers of life annuity surrenders.  Performed exploratory data analysis, feature engineering, and feature selection utilizing R.  Presented the teams results to a panel of insurance experts.
 Performed market segmentation through latent-class analysis in R to discover relationships between spending habits and hobbies/interests among people aged 15 to 30.
 Analyzed $50B of leads data for a wealth-management firm through SQL, Tableau and R to improve sales activity.  Created a linearregression model to predict net new assets by branch and a decision-tree model to understand sales representatives behavior.
 Created an XGBoost classifier in Python to estimate insurance policy renewal probabilities.  Optimized incentives to insurance agents to maximize net revenue.  (McKinsey Analytics Online Hackathon, July 2018, ranked in the top 1% of 5,052 participants.)
 Forecasted the disappearance of Arctic sea ice through an ARIMA model in Base SAS.  Predictions for key dates in sea ice measurement compared favorably with those by meteorological and oceanographic authorities.
 Performed survival analysis (parametric and Cox regressions) to identify factors driving employee attrition at a pharmaceutical company.
 Applied net and integer optimization techniques for social net analysis using Excel, VBA, and Risk Solver Platform.
 Crafted a Tableau storyboard to assess gender inequality globally.  Visualizations covered the areas of employment, health, and .
 Directed the Fraud, Waste, and Abuse team's transformation to a data-driven operation through the introduction of Access-Excel integration for financial and operational reporting. This led to the creation and automation of 20 recurring reports (daily metrics, production metrics, measuring, managing, accelerating  in progress) for investigators, managers, and commissioners of insurance.
Databases and Programming
 Created a prototype Oracle database to model Evercare CTs skilled-nursing-services process and address appeals brought forth against them.
 Automated the collection and cleansing of real-estate data from the Internet using VBA (90% time saving over manual process).
 Developed a document-sharing website prototype utilizing HTML, CSS, PHP, JavaScript, and MySQL.
 Redesigned the Transfer Alternative Protocol process (a hospitalization tracking system used by health services and medical directors) through VBA by devising a user-friendly, centralized storage and reporting Access database.
Presentation, Teaching, and Training
 Instructed and provided support to 40 graduate students in the courses Analytics in the Organizational Context and Storytelling with Data.
 Taught a class of 45 graduate students to use SAS software (Base, Enterprise Guide, and Enterprise Miner) for time series and text mining.
 Presented to a group of 70 students the merits of R for visualizations.  Packages shown included ggplot2, Plotly, Shiny, and googleVis.
 Provided training and  support for the Excel/Access/VBA reporting and automation  that I developed (audiences of up to 100 trainees ranging from analysts to C-level employees).
 HISTORY
Adjunct Faculty Associate, Columbia University School of  Studies, New York, NY
2018  Present
Principal, Data Science, Xaxu Consulting, Bristol, CT
2014  Present
Consultant, Evangelista Consulting, Bristol, CT
2014  Present
Data Mining Teaching Assistant, University of Connecticut School of Business, Hartford, CT
2017
Senior Reporting Analyst, United Payment Integrity  Fraud, Waste, and Abuse, UnitedHealth Group, Rocky Hill, CT	2011  2013
Senior Business Process Analyst, Evercare Corporate Finance, UnitedHealth Group, Rocky Hill, CT	2009  2011
Operations Analyst, Evercare Connecticut, UnitedHealth Group, Hartford, CT	2005  2009
 AND  DEVELOPMENT
MS, Business Analytics and Project Management, University of Connecticut School of Business, Hartford, CT
Relevant course:  Predictive Modeling, Business Decision Modeling, Visual Analytics, Data Analytics Using R BS, Physics, University of Connecticut, Storrs, CT
Honors:  Graduated Summa Cum Laude.  Member of Sigma Pi Sigma Society
Certificates, Coursera:  Neural Nets and Deep Learning, Structuring Machine Learning 
Open Badges, Cognitive Class (IBM):  Hadoop Foundations, Hadoop Programming, Scala Programming for Data Science",Data Scientist,emailed,indeed job post,resume
"Senior Data Scientist 
Unisys  San Diego, CA 	   	Apr. 2017 to Present 
 Time series and anomaly detection for forecasting demand and identifying events (e.g. ARIMA, LSTM). Automated pipeline to notify events to the team. 
 Created custom semi-supervised text-mining algorithms to extract info and categorize documents using topic modeling, sentiment analysis, NER, word embedding, text summarization using SpaCy, GENSIM, Stanford CoreNLP, NLTK etc.  
 End to end ML models for consumer segmentation, lift analysis and lifetime value estimation to optimize marketing and revenue prediction (TensorFlow, Keras, Scikit Learn etc.)  
 ed closely with product managers and engineers to design and evaluate solutions in cloud environment (AWS, 
AZURE). Lead migration of data and machine learning pipelines to the cloud environment (Spark, Spark ML, Scala) 
 Applied statistical analysis, dimensionality reduction and visualization techniques (e.g. PCA, T-SNE) to discover and communicate insights from data.  
 Conducted research, developed data driven prototype for business use cases and presented to stakeholders and clients. 
 Extraction, cleaning and manipulating of data in Linux environment (Python, Scala, Perl, SQL, Hive, Pandas) Data Scientist II 
Speedwell Holdings - Las Vegas, NV 	 	Dec. 2015 to Mar. 2017 
 Created data pipelines and ML models for content marketing, ping tree and consumer segmentation, fraud detection and evaluating credit worthiness of loan applications. 
 Designed and evaluated experiments for hypothesis testing (AB testing, cohort analysis, Bayesian Bandits). 
 Automated summarization reports and visualization  to discover underlying trends and time series forecasting.  
 Production using machine learning APIs, JavaScript, PHP, Python, SAS, PostgreSQL and MySQL. 
Data Science Research Fellow 
Galvanize  San Francisco, CA 	 	Jun. 2015 to Nov. 2015 
 Geospatial image processing and classification using Convolutional Neural Nets (CNN).  
Post-Doctoral Researcher  
University of California - Irvine, CA   	 	Oct. 2014 to Mar. 2015 
 Monte Carlo simulations on stochastic fracture nets, revealing effects of micro-scale net properties on largescale transport. 
 	 
 	   
 
Research Assistant 
University of California - Irvine, CA   	 	Jan. 2010 to Sep. 2014 
 Studied stochastic transport in fracture nets. Proposed and verified mathematical schema for large-scale anomalous transport. Developed distributed computing models using MPI, Fortran, Matlab, R, Python and C++ to simulate flow and transport in fractured rocks and porous media. 
 Simulated streamline-scale flow in nets of fractures using Finite Element Analysis, and reported algorithms with enhanced computational efficiency as alternative to direct solution of convective-diffusive PDEs (NavierStokes Eq.) 
 
Ph.D., University of California Irvine, Irvine, CA 	 	2014 
Computational Geosciences 
 Thesis: High-Resolution Analyses of Anomalous Transport in Large-Scale Variable-Aperture Discrete Fracture Nets 
M.Sc., Temple University, Philadelphia, PA 	 	2009 
Civil Engineering   	 	 
B.Sc., Sharif University of , Tehran, IR 	 	2007 
Structural Engineering 
 
Data Analysis, Statistics, Machine learning, Time series, NLP, Transfer Learning, Entity recognition, Text 
Summarization, Python, Scala, R, SAS, Matlab, Pandas, Scikit-Learn, TensorFlow, Spark, Web Services, Neural 
Nets and Deep Learning, Linear regression, Decision Trees, Boosting, SVM, PostgreSQL, MySQL, ArcGIS, 
Microsoft Office, AWS, Business Intelligence, Hydrogeology, Numerical Analysis 
 
 ",Data Scientist,emailed,indeed job post,resume
" 
Data Scientist with 4+ years experience executing data-driven solutions to increase effectively, accuracy, and utility of data processing. Experienced at creating data regression and classification models, using data mining and machine learning models to deliver insights and implement action-oriented solutions to complex research and business problems. Hunger for impact, be creative, and win as a team.  
Computer Software:  R (neuralnet; randomForest; rpart, gam)  SAS  JMP  SPSS  Python (netx; numpy; pandas; seaborn; math; sklearn)  HTML  ArcMap GIS  AutoCAD  Microsoft Office  Excel Visual Basic  Microsoft Project  Microsoft SQL Server  MySQL 
Machine Learning (ML) & Artificial Intelligence (AI):  Multiple Linear Regression (MLR)  Generalized Additive Models (GAM)  Logistic Regression  Neural Nets  Decision Tree Learning  Random Forests  Support Vector Machines (SVM)  Cross-Validation  Bootstrap  Data Cleaning  Data Imputation  Model Selection  Data Visualization 
Others:  Leadership  Collaboration  Problem Solving  Detail Oriented 
Experience 
Postdoctoral Researcher; Louisiana State University 	 	 	 	 	2019?Present 
 Develop data scientist programs in python to analyze research data related to natural hazard mitigation 
 Supervise graduate PhD students 
 Develop and conduct an online research survey 
 Write research papers 
Research Assistant; Louisiana State University  	 	 	 	 	2014?2019 
 Develop an advanced statistical model to estimate flood mitigation project cost in R program by examining several prediction models such as multiple regression, random forest, and neural nets 
 Impute missing data for a project with missing data on 60% of properties by developing a method with using generalized additive models in R program 
 Develop a methodology for probabilistic flood loss modeling by implementing Monte Carlo simulation in Excel 
 Leading a collaborative  with two other graduate students for developing a program in Python for hurricane wind loss modeling with generalizing the loss functions in HAZUS-MH Hurricane Model repository with 103 MB data 
Teaching Assistant; Louisiana State University 	 	 	 	 	 	Spring 2014 
 Prepare course materials and help students in three online courses 
al Background 
Ph.D. in Engineering Science; Louisiana State University  	 	 	 	Jan. 2019 
Title of dissertation: Costs and Benefits of Flood Mitigation in Louisiana 
Ph.D. Minor in Applied Statistics 
Related course: Statistical techniques I & II (SAS programming)  Regression analysis (SAS programming)  Statistical methods for reliability and survival data (JMP)  Principles and theory of statistics  Statistical data mining (R programming) 
 
M.Sc.  in Engineering Science; Louisiana State University 	 	 	 	Dec. 2017 
Title of project: Prediction of Flood Mitigation Elevation Project Cost in Louisiana by Statistical Modeling of Current Mitigation Practices 
 
M.Sc. in Construction Management; University  Malaysia 	 	 	Mar. 2013 
 
 	1 
 
Arash Taghinezhad 
Bachelor of Science in Civil Engineering; University of Mashhad, Iran 	 	 
Training Certificates 
May 2008 
SQL Essential Training; Lynda.com 	 	 	 	 	 	 
July 2019 
Python for Data Science Essential Training; Lynda.com  	 	 	 
Fun Data Scientist  
May 2019 
 Analyze Lending Club dataset to make a prediction model to see if the loan applicants able to pay off their loans based on their credit history 
 Develop a machine learning model to predict concrete strength 
 Presentations 
Presenter in Construction Management GSA research poster competition (3rd place) 	 	2019 Performing an R-Programming shop for LSU Construction Management graduate students 2018  
Lecture to Episcopal High School students about engineering careers 	 	 	 	2018 
Presenter in three-minute thesis (3MT) competition at LSU 	 	 	 
Sociaety Memberships 
 	2017 
The Data Science Society @ LSU 	 	 	 	 	 	 
2019-present 
Construction Student Association (CSA)  	 	 	 	 	 
2018-2019 
LSU Construction Management Graduate Student Association 	 	 	 
2018-2019 
LSU International Ambassadors  	 	 	 	 	 	 
2017-2018 
American Society of Civil Engineers (ASCE) 	 	 	 	 	 
2016-present 
Association of State Floodplain Managers (ASFPM) 	 	 	 	 
2016-2017 
Iranian Student Association at LSU (ISA)  	 	 	 	 	 
 Funded Grant Research Experience 
2013-present
Multi-Scale Spatiotemporal Evaluation of Mitigation Effectiveness in Reducing Natural Hazard Damage 
and Loss 	 	 	 	 	 	 	 	 	 	 
FEMA (sub-grantee through Louisiana GOHSEP), collaborative project with Arizona State University and East Tennessee State University, March 2015  February 2018, Budget: $475,673 + $104,974 internal/external budgeted match = $580,647 
Role: Lead graduate research assistant for building-level mitigation effectiveness 
Volunteer s 
 LSU representative at American Institute of Architects and the initials (AIA) exhibition (2016)  Student helper for ABC's National Construction Management Competition (2017)  Presentor at STEM Exhibition 
Day at St. Jude the Apostle School (2017)  Poster competition judge at LSU Discover Day (2019)  Publications 
Taghinezhad, A., Friedland, C. J., Robert, R. V., and Marx, B. D. (2020). ""Data imputation for avoided loss analysis in natural disaster mitigation ."" Under Consideration by Journal of Performance of Constructed Facilities. 
Taghinezhad, A., Friedland, C. J., Rohli, R. V., Marx, B. D., and Giering, J. (2020). ""Predictive statistical cost estimation model for existing single-family home elevation ."" Under Consideration by Journal of Flood Risk Management. 
Taghinezhad, A., Jafari, A., and Kermanshachi, S. (2020). ""Best practices for state DOTs to determine project delivery time, project management, and ratio of consultant to in-house design."" TBD. 
Taghinezhad, A., Jafari, A., and Kermanshachi, S. (2020). ""Exploring project management best practices for project success in U.S.transportation agencies "" Under Consideration by International Journal of Project Management. 
 
 
 	2 ",Data Scientist,emailed,indeed job post,resume
" Over 5+ years of  as a Quantitative Analyst /Data Scientist ing across
Finance, Telecom and Retail industries with a master's degree in Science and Specialization in
Mathematics and Quantitative Finance. 
 Proven ability to translate high-level  into practical analysis and deliver actionablerecommendations. Record of managing complex  and creating solutions that . Selfdirected innovator searching for challenges. 
 Proficient in Machine Learning Techniques, R, Python, SAS, Tableau, SQL & Advanced Excel.  Developed predictive models to identify the most significant behavioral patterns that lead to a member conversion which eventually increased the ROI of 0.96 million$ per quarter 
 Re-built the existing model and increased its accuracy from 68% to 89% using Advanced StatisticalAlgorithms 
 Proficient in managing entire phases of CRISP-DM project life cycle including data acquisition, datacleaning, data engineering, features scaling, features engineering, statistical modeling (Decision trees, regression models, neural nets, SVM, KMeans Clustering), dimensionality reduction using Principal Component Analysis and Factor Analysis, testing and validation using ROC plot, K- fold cross validation and data visualization. 
 Good practical knowledge in performing Data Analysis process using Python like Importing datasets,
Data wrangling, Exploratory Data Analysis, Model development and Model Evaluation. 
 Hands on Expertise on Classification, Regression, Time Series Data, Churn Prediction, Home Pricevaluation, Exit Strategies using various packages in R and Python 
 Adept and deep understanding of Statistical modeling, Multivariate Analysis, model testing, problemanalysis, model comparison and validation. 
 Skilled in performing data parsing, data manipulation and data preparation with methods includingdescribe data contents, compute descriptive statistics of data, regex, split and combine, Remap, merge, subset, re index, melt and reshape. 
 Experience in using various packages in R and libraries in Python. 
 ing knowledge in Hadoop, Hive and NOSQL databases like Cassandra and HBase. 
 Ability to handle multiple tasks simultaneously. 
 Proven leader with outstanding relationship building  and strong communication abilities 
 Highly motivated team player with ability to  cross-organizationally and manage strict deadline.
 
 Extensive ing experience in developing mappings in Informatica, tuning them to achieve optimalperformance and migrate objects in all environments including DEV, QA testing and PROD.
 Experience

Quantitative Analyst/Data Scientist
Tourmalet, CT
June 2017 to Present
Project: Conversion Efficacy - Review and assess the Exit strategy and performance of the targeting system for converting NPL to payoff 
 Created a dataset with data in 7-8 different repositories using SQL Teradata involving complexquerying. 
 Analyzed dataset of about 1.34M records and identified trends and effective factors for datamodelling. 
 Preprocessed the data and developed various visualizations using packages ggplot/choroplethr/caretin R. 
 Rebuilt the propensity model with increased accuracy from 68%-89% using Advanced ML Algorithms
 
 The model estimates to increase the ROI and least estimated standard error 2.05% 
 Developed dashboards using Tableau and automated the process using SSIS for daily update  Documented the phase2 Analysis for the project and also designed roadmap and conducted KT sessions 
 Built multiple models using various Machine Learning Algorithms like Multinomial Regression,
Decision Tree, SVM, Random Forest, Neural Nets etc., and finalized with the better model using ROC Curve 
 Used various Parameter Tuning Techniques to get better results from the model. 
 Used different methods like Univariate approach, Boxplots, Cook's distance to find the outliers  Developed time series forecasting model (ARIMA) & provided strategic analysis for the business demands and supply, in which the model predicted actual demand with 92% accuracy. 
 Created Exploratory Data Analysis to identify trend, seasonality and outliers etc. 
 Managed team processes and deliverables for Ramp-up and Ramp down demand forecasts 
 Responsible for providing reports, analysis and insightful recommendations to business leaders onkey performance metrics pertaining to employee performance 
 Built predictive models to identify the most significant behavioral patterns that lead to employeechurn 
 Created Propensity model to identify the most influential attributes contributing to Indent/Demandcancellation
DATA SCIENTIST
Idea Cellular Ltd
December 2013 to August 2016
Project: Customer Churn Model 
 Understanding business context and strategic plans and develop a data-driven business plan tosupport the attainment of business goals. 
 Created a dataset using SQL Teradata complex queries along with identification to factors tocustomers calling to customer care from customer care text repository. 
 Data manipulation/treatment based on nature of data (for example missing value imputation,Information Value (IV), Weight of Evidence (WOE), Data profiling, correlation matrix, relative importance between predictors, variable clustering, univariate and bivariate plots, etc.) 
 Building predictive models from start-to-finish following CRISP-DM life cycle (i.e. extract data,manipulate data, Data Profiling, build and validate model) and then deployed model using flask.  Preparation of final project presentation documents for overall significance of the project in a welldefined manner 
 Identify the most significant behavioral patterns that lead to customer churn and build an attritionmodel using Random Forest, SVM with Precision-Recall curve as evaluation metric for the model.  Data exploration to make the data useable for building data insights and initial hypothesis. 
 Used the analysis of behavior and characteristics of terminated employees to drew the importantfactor for attrition. 
 Build a model to check the factors affecting the termination of employees and their reasons fortermination using various dimension reduction techniques. 
 Predict the employees who are high risk to maintain the talent pool and to effectively retain talent atevery level (High Potential and High performer) 
 Responsible for ing with stakeholders to troubleshoot issues, communicate to team members,leadership and stakeholders on findings to ensure models are well understood and optimized.
DATA ANALYST
Axis Bank - IN
August 2011 to December 2013
 Creating output to explain data analysis, data visualization, and statistical modeling results tomanagers. 
 Modeling survey data responses with ordinal logistic regression. 
 Experience with ing on clickstream activities, Customer Journey activities, Fraud Detection,
Sales and managing Store items. 
 Analyzing and visualizing user behavior migration. 
 Created mappings to load data from source and target to staging, staging to reporting tables byapplying business requirements using Informatica Power Center. 
 Applying machine learning concepts to capture insights. 
 Handled importing data from various data sources, performed transformations using Hive,
MapReduce, and loaded data into HDFS. 
 Providing timely, relevant, accurate reports and analysis of the organization's performance tofacilitate decision-making towards achievement of the budget and strategic plan. 
 Documented all phases of project implementation for future reference and conducting KT sessions
Emerging Market Analyst
IndusInd Bank - IN
May 2010 to August 2011
 Gathered requirements from onsite coordinators, performed Requirement Gap analysis and finalizeddesign documents using Erwin Data Modeler and Microsoft Visio. 
 Designed complex SQL queries to input at the beginning of mappings to filter the data as perrequirements. 
 Created reporting tables for comparing source and target data and report data discrepancies
(mismatch, missing scenarios) found in the data. 
 Performed validations not received in the requirement document from the customer end and learntthe SQL queries which helped to attend defect triage calls. 
 Results obtained from report mappings were displayed using MicroStrategy which is a better UserInterface tool. 
 Extensive hands on experience of HP Quality Center tool used for performing production supportactivities. 
 Implemented Microsoft Visio and Rational Rose for designing the Use Case Diagrams, Class model,
Sequence diagrams, and Activity diagrams for SDLC process of the application. 
 Performed debugging of the code as per inputs given by IST (Integrated System Testing) team anddeployed code into PROD environment after receiving approval from IST team. 
 Performed performance improvement of the existing Data warehouse applications to increaseefficiency of the existing system. 
 Finalized the factors required for predictive model development by gathering inputs from MCC(Customer service) team, performed feature engineering to identify factors best suited for model development and gathered data from various repositories as per required factors using SQL complex querying n Teradata.
FSM/Data Analyst
ICICI Bank - IN
May 2006 to April 2009
 Gathered requirements from onsite coordinators, performed Requirement Gap analysis and finalizeddesign documents using Erwin Data Modeler and Microsoft Visio. 
 Understanding the requirements and develop various packages in SSIS. 
 Gathered requirements from JAD/JAR sections with developers and business clients. 
 Designed the business requirement collection approach based on the project scope and SDLCmethodology. 
 Designs and develops the logical and physical data models to support the Data Marts and the DataWarehouse 
 Create SQL queries for product components to update FACETS backend tables and create productprefixes. 
 Involved in formatting data stores and generate UML diagrams of logical and physical data. 
 Developed project plans and managed project scope. 
 Performed user acceptance and parallel testing for coding, pricing, and benefit builds in Facets. 
 Prepared a handbook of standards and Documented standards for Informatica code development.
Education

Master of Science in Mathematics and Quantitative Finance
Sacred Heart University December 2017
MBA
Institute for Technology and Management May 2006
B.com
Lucknow University - Lucknow, Uttar Pradesh May 2003


Sql server, Sql server 2005, Mysql, Oracle, Sql, Cassandra, Clustering, Hadoop, Informatica, Machine learning, Nosql, Teradata, Microstrategy, Sas, Tableau, Decision trees, Deep learning, Logistic regression, Random forest, Support vector machines
Additional Information

TECHNICAL 
 
Expertise 
Caret, Tidyverse, MASS, GGPLOT2, Scikit-Learn, NumPy, SciPy, Deep learning, RNN, CNN, Tensor flow, Keras, matplotlib, Microsoft Visual Studio, Microsoft Office 
 
Machine Learning Algorithms: 
Multinomial Regression, Logistic Regression, Decision Trees, Random Forest, K Means Clustering, Support Vector Machines, Gradient Boost Machines & XGBoost, 
 
RDBMS SQL Server 2005/2008/2012, MySQL, Teradata 
NoSQL DB Cassandra 
Frames Hadoop Ecosystem, Apache Spark 
Programming Languages R, Python 
Tools/Platforms 
RStudio, Tableau, Informatica, MicroStrategy, Toad, SAS, Eclipse, Windows, SQL developer, Toad for
Oracle, Microsoft SQL, Teradata, Hadoop",Data Scientist,emailed,indeed job post,resume
"A Computer Science graduate from Purdue University with programming experience in scraping, combining and managing data from diverse sources and a data scientist with expertise in deriving insights from the raw & messy data, with a strong interest in Machine Learning, data mining, and information visualization.  
I am passionate about Data Science and its applications in real-world and would love to apply it to challenging business problems to bring value to the company. 
 
My career is a one-part strategy and two-parts  expertise. 
 
 Areas of Expertise:  
 Data Science: Machine Learning, Data Mining, Predictive Analytics  
 Data Engineering: C/Java, SQL, Python (NumPy, Pandas, SciPy, Scikit-learn, Seaborn, Matplotlib)  
 Hadoop: MapReduce, Hive, Pig, HDFS 
 Open Source Relational Databases: MySQL, PostgreSQL, Microsoft SQL Server 
 Big Data Analytics and Data Warehousing: Amazon Web Services (S3, EC2, EMR, Machine Learning,Redshift) 
 Data Visualization: Tableau, D3.js 
 Machine Learning: Supervised (Classification: KNN, Neural Net, SVM, GBM, XGBoost),
Unsupervised(Clustering: K-means, Graph, Dimensionality Reduction: PCA, SVD, Recommender
Systems- Frequent Itemset Mining: Apriori, FP Growth) 
 Deep Learning: TensorFlow, Keras 
 Statistical Modeling: R, MATLAB, SAS, SPSS 
 Business Intelligence: Qlikview, Cloud Analytics 
 Database Administration: Microsoft SQL Server 
 Database Development 
 Data Architecture, Data Modeling 
Most part of my  has involved in understanding the Linear Algebra, Probability, and Statistics involved in Machine Learning and implementing those Machine Learning algorithms from scratch. 
 
I like designing, developing, deploying, and managing Machine Learning models in the AWS environment. 
 
I would love to  as a Data Scientist, Machine Learning Engineer, Data Analyst roles. 
 

Willing to relocate: Anywhere
Authorized to  in the US for any employer
 Experience

Database Developer and Data Analyst
IU SCHOOL OF MEDICINE - Indianapolis, IN December 2017 to December 2018
Database Developer & Administrator: phpMyAdmin, MySQL 
 Built a web application for IU School of Nursing to provide services for nursing centers and patients across Indiana State. 
 Designed, Developed, Administered, and Maintained a database for storing, retrieving, updating the data at the back-end. (Prototype won grant money of $30,000).
Business Data Analyst Intern
ProGen Business Solutions
April 2016 to October 2016
Business Intelligence Developer: Tableau, R, Python, SQL 
 Assisted in getting the insights from sales data and recognized the patterns associated with the customer transactions. 
 Prepared Financial and Business Sales Reports 
 Created sales data visualizations using Tableau Dashboards. 
 Improved product sales from 70% to 85%.
Data Analyst Intern
TakenMind
May 2019
 Performed EDA, Data Cleaning, and Statistical Analysis in Python. 
 Delivered a prediction analysis using supervised machine learning algorithms (Decision Tree, Random Forest, SVM). 


Master of Science in Computer & Information Science (Data Science Track)
Purdue University - Indianapolis, IN January 2017 to December 2018
Bachelor's in Computer Science and Engineering
Jawaharlal Nehru Technological University
June 2011 to June 2015


AWS (3 years), Data Analytics (3 years), Detail Oriented, SQL, Excel, Business Intelligence, Machine
Learning (2 years), Python (3 years), R (3 years), MySQL (4 years), Database Management (4 years),
Data Mining (2 years), Data Modeling (4 years), Linear Algebra (5 years), Statistical Analysis (3 years),
Tableau (3 years), Microsoft SQL Server (3 years), Object Oriented Programming (3 years), Data
Science (3 years)
Links

Certifications/Licenses

Artificial Intelligence Foundations: Machine Learning
January 2019 to Present
Learning Cloud Data Storage
January 2019 to Present
SQL for Data Science
August 2017 to Present
Database Modeling and Design
August 2017 to Present
Kaggle Python Tutorial on Machine Learning
February 2019 to Present
Groups

Data Science Indy
January 2017 to Present
Student Group Leader for Data Science Indy Meetup.
Additional Information

Available Immediately. Open for Relocation. Authorized to  for any employer in the US.",Data Scientist,emailed,indeed job post,resume
"
Experienced data analyst and machine learning engineer in manufacturing and customer services. Looking for Intern/Full-time Data Scientist/NLP specialist positions. Proficient in Machine learning, NLP, Python, R, SQL, TensorFlow and Tableau. 
 
 

Programming: Python (Pandas, Numpy), R (dplyr, ggplot2), SQL (MySQL, PostgreSQL), MongoDB, C++, Java 
Frame: pySpark, Tensorflow, Keras, Sk-learn, Tableau, PowerPoint, Excel, Kafka, Ariflow 
Modeling: Regression Analysis, Hypothesis Test, Clustering Analysis, Predictive Modeling, Time Serie Analysis Platform: AWS (EC2, S3, Redshift, EMR), Linux, Git, Hadoop, Spark (SparkSQL, MLlib, Hive), Docker 
 
 

Georgetown University                                                                                                                        Washington, D.C.    M.S. in Mathematics and Statistics                                                                                                   Aug. 2017 - May 2019 
Shanghai Jiaotong University                                                                                                                  Shanghai, China       
M.S. in Material Science and Engineering                                                                                       Sep. 2013 - Mar. 2016 
Southeast University                                                                                                                                   Nanjing, China        
B.S. in Material Science and Engineering                                                                                        Aug. 2009 - Jun. 2013 
Relevant Courses: Reinforcement Learning, C++, Advanced Statistical Computing, Numeric Optimization, Time Series Analysis, Matrix Theory, Social Net Analysis, Probability Theory 
 
 EXPERIENCE 

Georgetown University, McDonough School of Business                                                                  Washington, D.C.   
Business Research Assistant                                                                                                           Nov. 2018  May 2019             
 Led a team of 5 constructing business risk database providing augmented analysis for thousands of companies 
 Implemented automated ETL pipelines with Python and MongoDB, saving 65% time spent on data processing 
 Formulated test-driven international business risk management using text mining and machine learning toolkits  
 
Pingan                                                                                                                                   Shenzhen, China 
Machine Learning Intern-NLP                                                                                                       May 2018 - Sep. 2018         
 Structured business use cases and redesigned databases that increased system recall rate by 36% 
 Designed feedback analysis system using unsupervised learning, leading to significant reduction in  capital 
 Reduced over 30% operating cost by designing actionable metrics and personalized solutions 
 Enhanced customer satisfaction rate by solving 92% of requests applying deep learning conversation system  
 
Ford Motor Co., Ltd                                                                                                                                   Nanjing, China             
Marketing Data Analyst Intern                                                                                                      May 2015 - Aug. 2015             
 Managed research on 4 Ford C-type cars, discovering 2 leading factors that contributed to 34% negative reviews 
 Constructed dashboards using Excel and Tableau to provide insights for 12 production problems 
 Presented reports on sales performance, leading improvement in 3 key components 
 
Shanghai Jiaotong University, State Key Laboratory                                                                           Shanghai, China 
Material Science Research Associate                                                                                              Jun. 2014  Sep. 2016 
 Designed computation driven experiments that motivated 2 manufacturing innovations 
 Overcame 3 key techniques based on the test-driven analysis, process simulations, and data visualizations 
 Co-authored 2 top research papers and 2 patents on theoretical and quantitative analysis of new techniques 
 
 	 

Credit Card Fraud Detection                                                                                                            July. 2019  Present 
 Process 50 million transaction records in Spark and Cassandra distributed environment  
 Constructed real-time fraud monitoring infrastructures with Spark SQL, MLlib and Kafka  
 
Algorithmic Trading Bot                                                                                                                Mar. 2019 - May 2019 
 Created self-trainable trading agents using deep reinforcement learning architectures with TensorFlow 
 Achieved 5% net revenue over a 10-day trading period in a simulated stock market ",Data Scientist,emailed,indeed job post,resume
" Proficient in statistical or mathematical programming languages: Fortran, SAS, SQL, R, MATLAB,
GUASS, Python and Excel, Word, PowerPoint. 
 Experienced in economic/financial modeling and/or forecasting. 
 6+ years of experience performing advanced quantitative analyses. 
? Manipulating, analyzing and interpreting big data. (Data Mining & Analysis). 
? Defining problems, collecting data, establishing facts, and drawing valid conclusions. 
? Managing statistical . 
 3+ years experience of marketing analysis. 
 Able to  well under pressure and with limited resources 
 Able to effectively present information to top management, public groups, and/or boards of directors.
 
 Willing to  in cross-disciplinary teams, in a collaborative and transparent way.
Willing to relocate: Anywhere
Sponsorship required to  in the US
 Experience

Statistician
Kelley Blue Book
May 2016 to January 2017
 Tracked, analyzed and interpreted trends in traffic and marketing data in order to provide relevantconclusions and recommendations to management and core teams. 
 Developed Upfront statistical models utilizing typical methodologies based on data available fromdifferent sources. 
 Updated data and models for statistical modeling purposes. 
 Developed KPI tracker report and driver analysis to track results against forecasts and re-specifymodels when required.
Research Assistant
Economic & Policy Analysis Research Center - Columbia, MO
August 2010 to March 2016
Doctoral Research: initiate, design models and study teachers' retirement decision under different retirement policies. Innovate novel efficient and robust regression methodologies in the intersection of applied Math and Econometrics. 
 Cleaned, maintained and organized Missouri public schools dataset with over millions of records inboth Windows and Unix systems using SAS and SQL. 
 Analyzed and reported dataset to center director. 
 Collected and categorized retirement policies for different States. 
 Built quantitative models independently including Multivariate Probit model, Dynamic model,Bayesian statistics, Hazard model, Monte Carlo simulation and Structural model using Fortran, R and SAS.
Teaching Assistant
University of Missouri-Columbia - Columbia, MO
August 2009 to June 2010
 Led discussion sessions, hold office hour and coached students course materials 
 Assisted instructors with preparing and proctoring exams 
 Graded home and exams
Financial Data Analyst
Thomson Reuters Ltd, Co - Beijing, CN
August 2008 to August 2009
 Collected corporate latest actions data of all listed companies and funds in Greater China region. 
 Took charge of training all new employees and document ing procedures. 
 Managed the resolution process for all external customer related issues via CRM system. 
 Led various , including meeting events investigation, China automation investigation and
China share buyback study etc., collaborating with Corporate Actions team. 
 Solved internal queries and communicate with various regional project managers.
Management Trainee & Market Executive
Double A Public Ltd, Co
July 2006 to April 2008
 Designed market, conducted surveys, and analyzed raw data and report. 
 Managed Media Chosen and TVC campaign in China Market and follow up WIP & Strategic meetings. 
 Controlled global ATL budget, expenses and making comparison.
Research Executive
TNS Market Research Ltd, Co - Beijing, CN
April 2006 to August 2006
 Conducted qualitative research  and analyzed raw data: 
? Service Satisfaction Survey for British Consultant 
 Conducted quantitative research  and analyzed raw data: 
? New products market research for Nokia 
? Market share research for HomeWorld Supermarket
Project Assistant
AC Nielsen Ltd - Beijing, CN
November 2004 to December 2004
Collected and analyzed relevant information of the research project on Chinese tobacco industry


Ph.D. in Economic
University of Missouri - Columbia, MO
2019
M.A. in Mathematics?ABD)
University of Missouri-Columbia - Columbia, MO 2019
M.A. in Economics
Renmin University of China - Beijing, CN June 2006
B.A. in Economics
Renmin University of China - Beijing, CN June 2004


Economics, Python, R, Machine Learning",Data Scientist,emailed,indeed job post,resume
" 
 ?	 	 
	University of California, Berkeley 	 	                             8/2017-?	?8/2019 
Bachelor of Arts in Mathematics 
       Relevant Course: Linear Algebra, Ordinary Differential Equations, Real Analysis, Complex Analysis, Number  	Theory, Numerical Analysis, and Classical Geometries. 
	Santa Barbara City College 	                                                                                             ?	8/2013-5/2016?	 
Associate in Mathematics 
 
  Experience?	 	 
	Lecturer 	 ? Santa Barbara City College?	? ?Santa Barbara, CA                                               ?	7/2017-8/2017?	 
? Communicated complex ideas to a group of 30+ students. 
? Organized and planned lectures that required on-spot adjustments to accommodate the pace of student understanding.  
? Lectures included home questions, teaching new material, and engaging students in group . 
? Strong understanding of material in order to explain concepts clearly, effectively, and in different perspectives.  
 
	Math Tutor 	 ? Santa Barbara City College?	? ?Santa Barbara, CA                                          ?	1/2016-7/2017?	 
? Tutored 50+ diverse students in one-on-one, small group, and classroom sized settings. 
? Increased test scores by 20% 
? Sessions included home help, textbook or lecture note clarifications, a series of guided questions, encouragement of independent problem solving, and critical thinking. ? Subjects ranged from intermediate algebra to differential equations. 
 
	Sales Associate 	 ? In-N-Out Burger?	? ?Santa Barbara, CA                                                    ?	8/2012-8/2015?	 
? Provided exceptional customer service in a fast-paced  setting.  
? Required effective and clear communication between employees in a high demand environment. 
 
  Leadership?	 	    
	Vice President of Math Club 	 ? Santa Barbara City College?	? ?Santa Barbara, CA             ?	1/2016-7/2017?	  
? Coordinated a lecture series with STEM field s and math professors. 
? Talks entailed how math is related to different STEM fields, and math beyond the scope of community college classes. 
? Provided math tutoring to all members.  
 

  
?
?
?
?
MatLab 
Python 
Java 
MapleLab 
 
?
?
?
?
PowerPoint 
Excel 
Customer Service  
Public Speaking 
 
 ",Data Scientist,emailed,indeed job post,resume
"	 
 
California State University - San 
Bernardino  
San Bernardino, CA    2019 
Bachelor of Science: Computer 
Engineering  
Achievements: 
 Deans list for 6 quarters 
  
Programming Languages:  
 Python, C++, C, Java 
 HTML, CSS, JavaScript, PHP, SQL 
 Parallel programming 
 Object oriented languages 
 Assembly language  
 Verilog 
 
Experience using:  
 LabVIEW  
 VIVADO  
 MATLAB  
 Visual Studios  
 Spyder  
 Linux and Windows OS  
 NetBeans 
  
 
 
  
 Bachelor's degree in Computer Engineering 
 One year of lab instructor and teacher assistant for computer architecture course at CSUSB 
 Bilingual: Spanish and English 
 Efficient with algorithm analysis, software testing and debugging  
 Exceptional at leading and ing in teams 
 Self-starter with great time management and organizational  
 Excellent oral and written communication  
 Strong  ethic and thrives on challenges 
 Proficient on Microsoft Word, Excel, and PowerPoint 
 
SCHOOL RELATED EXPERIENCE 
Senior Engineering Team  - Team Leader 
     CSUSB                                                          09/2018 to 06/2019  
Lead a team of six to complete two  on smart home appliances using the Arduino, Raspberry Pi, different types of sensors, 3D-printing and mobile applications.  consisted of a smart home door lock and smart blinds. Conducted research, analyzed data, created a proposal, and wrote the requirements and system specification papers. Completed the final products with a PowerPoint presentation and a live demo presented to the professor and classmates. 
 
 EXPERIENCE 
Home Depot Inc - Sales Associate  
Hemet, CA                                                       05/2014 - Current  
Management rewarded with three promotions and maximum raises during annual evaluations. Established recognitions for Employee of the month and Homer Awards for exceeding ing expectations. Able to quickly learn Home Depot's multiservice computer applications. Took on responsibilities such as delivering orders to appropriate locations, managed multiple customer's orders and able to meet customer's requests.  
 
CSUSB Computer Engineering and Science - Teacher 
Assistant/Lab Instructor  
San Bernardino, CA                                          09/2018 - 06/2019  Supported classroom activities: tutoring, reviewing for exams, and guiding students on their lab assignments. Effectively completed grading home assignments,  and labs on their respectful due dates. Assisted over 150 students, and 100% of students completed their labs and  successfully. ",Data Scientist,emailed,indeed job post,resume
" Strong expertise on Hadoop big data, SQL and HQL. ed on Kafka p.o.c for end-to-endimplementation to receive real time data flow from AWS server. 
 Experience in data visualization, data analytics, data integration, data quality using Python. Used
MATPLOT, DJANGO libraries in Python to show data visualization and trends. 
 Used NLTK libraries to detect the customer satisfaction through their verbatim comments.
(Tokenization and standardization are part of this process). 
 Querying on large data sets to process and refine the data for down streams. Automated the auditsystem to recognize any errors on fly (Null percentage and validity). 
 Expertise in RDBMS, Agile, Scrumand waterfall methodologies. 
 CSS, HTML and JavaScript along with good knowledge of backend  including Python.
Tableau expertise with 12 months ofexperience. 
 Strong understanding of project life cycle and Software Development Life Cycle (SDLC). WordPress,
MS office , CMS and multiple platforms Windows andMac. Strong command on MS Excelwith
VLOOKUP, MATCH and INDEXfunctions. 
 Good industry knowledge, analyticaland problem solving  and ability to  well with in ateam as well as an individual. Expertise in transforming business requirements into analytical models, designingalgorithms, buildingmodels, developing data mining and reporting solutions that scales across massive volume of structured and unstructured data. 
 Hands on experience with big data  like Hadoop, Spark, Hive, Pig, Impala, Pyspark, SparkSql.  Good Knowledge in Proof of Concepts (PoC's), gap analysis and gathered necessary data for analysis from different sources, prepared data for data exploration using data munging. 
 Highly creative, innovative, committed, intellectually curious, business savvy with goodcommunication and interpersonal . 
 Experience in using various packages in Rand python like ggplot2, caret, dplyr, Rweka, gmodels,RCurl, tm, C50, twitteR, NLP, Reshape2, rjson, plyr, pandas, numpy, seaborn, scipy, matplotlib, scikitlearn, Beautiful Soup, Rpy2. 
 Extensive experience in Data Visualization including producing tables, graphs, listings using variousprocedures and  such as Tableau.
Willing to relocate to: CA - WA - AZ,NV,OR
 Experience

Data Engineer (Telemetry)
Cisco - San Jose, CA
June 2017 to Present
Description: Identifying, gathering and analyzing complex multi-dimensional Telemetry datasets utilizing a variety of  while parallelly implementing them using SCRUM frame. 
 
Responsibilities: 
 As a Data engineer ed on huge raw data sets (Telemetry data). 
 Data classification and text analysis using NLTK libraries in Python. Collecting and compiling thevarious datasets from various data generating sources (AWS, MYSQL servers, Oracle Database).  Ingesting the data by creating tables in Hive and achieving high-level optimization of the ingested data by altering and refining through HQL. 
 Resolved compatibility issues by ing on different areas of data architecture including dataingestion and pipeline design. ed on Machine learning and advanced data processing to see the customer trends (scrutinize) in cumulative. 
 Developed and implemented scripts using machine-learning algorithms like Nave Bayes Classifier
Algorithm, k means clustering, Random Forest classifier for regression, clustering etc. 
 Created many plots, bar charts, graphs, histograms of the complex data by visualizing the data usingmatplotlib in python. Maintain source code repository in subversion and handled branching, tagging & merging process. 
 Research, design and develop computer software systems, applications which require use ofadvanced computational and quantitative methodologies
Data Scientist
West Corporation - Omaha, NE
October 2015 to May 2017
Description: Lead in database maintenance and data visualization. Conducted end-to-end statistical data analysis and modeling, including querying, model building, forecasting, visualization, and implementation. 
 
Responsibilities: 
 Performed Data Profiling to learn about behavior with various features such as traffic pattern,location, time, Date and Time etc. Evaluated models using Cross Validation, Logloss function, ROC curves and used AUC for feature selection. 
 Detected the near-duplicated news by applying NLP methods(word2vec) and developing machinelearning models like label spreading, clustering. 
 Collected data needs and requirements by Interacting with the other departments. 
 Used Principal Component Analysis in feature engineering to analyze high dimensional data. 
 Used clustering technique K-Means to identify outliers and to classify unlabeled data. 
 Ensured that the model has low False Positive Rate. 
 Created and designed reports that will use gathered metrics to infer and draw logical conclusions ofpast and future behavior. 
 Used MLlib, Spark's Machine learning library to build and evaluate different models. 
 Created Data Quality Scripts using SQL and Hive to validate successful data load and quality of thedata. Created various types of data visualizations using Python and Tableau. 
 Developed Map Reduce pipeline for feature extraction using Hive. 
 Application of various machine learning algorithms and statistical modeling like decision trees,regression models, neural nets, SVM, clustering to identify Volume using scikit-learn package in python, Matlab. 
 Performed data analysis by using Hive to retrieve the data from Hadoop cluster, Sql to retrieve datafrom Oracle database. 
 Performed Data Cleaning, features scaling, features engineering using pandas and numpy packagesin python. 
 Implemented rule based expertise system from the results of exploratory analysis and informationgathered from the people from different departments. 
 Analyze traffic patterns by calculating auto correlation with different time lags. 
 Addressed over fitting by implementing of the algorithm regularization methods like L2 and L1.  Performed Multi nomial Logistic Regression, Random forest, Decision Tree, SVM to classify package is going to deliver on time for the new route. 
 Communicated the results with operations team for taking best decisions. 
 
Environment: Impala, Linux, Spark, Tableau Desktop, Python 2.x, CDH5, HDFS, Hadoop 2.3, Hive, SQL Server 2012, Microsoft Excel, NLP
Data Scientist/Data Analyst
Assurant Specialty Property - Santa Ana, CA December 2014 to September 2015
Description: Assurant partners with leaders in mortgage lending, manufactured housing, multifamily housing and other industries to protect client and consumer property. Used python data structures for data sampling and validation. 
 
Responsibilities: 
 Developed Python scripts to automate data sampling process. Ensured the data integrity by checkingfor completeness, duplication, accuracy, and consistency 
 ed on model selection based on confusion matrices, minimized the TypeII error 
 Generated data analysis reports using Matplotlib, Tableau, successfully delivered and presented theresults for C-level decision makers 
 ed on data cleaning and reshaping, generatedsegmented subsets using Numpy and Pandas inPython 
 Continuously collected business requirements during the whole projectlifecycle. 
 Generated cost-benefit analysis to quantify the model implementation comparing with the formersituation. 
 Identified the variables that significantly affect the target 
 Applied various machine learning algorithms and statistical modeling like decision tree, logisticregression, GradientBoostingMachine to build predictive model using scikit-learn package in Python  Conducted model optimization and comparison using stepwise function based on AIC value 
 Wrote and optimized complex SQL queries involving multiple joins and advanced analytical functionsto perform data extraction and merging from large volumes of historical data stored in Oracle 11g, validating the ETL processed data in target database 
 
Environment: Numpy, Pandas, Tableau 7, Python 2.6.8, Matplotlib, Oracle 10g, SQL,Scikit-Learn, MongoDB,
Data Architect/Data Modeler
Pitney Bowes Inc - Stamford, CT
November 2013 to November 2014
Description: PITNEY BOWES (PB) is the manufacturer of copiers, faxes and other office automation.
Pitney Bowes's operations are aligned under three lines of business, Global Mailing Systems (GMS) - It is the company's core mail automation and shipping business. Extracting data and making them refined for transfer and load while managing big data are some of my tasks. 
 
Responsibilities: 
 Develop Integrations jobs to transfer data from source system to Hadoop. 
 Installation of Talend Studio. 
 Technical design documents for Transformation processes. 
 Application of business rules on the data being transferred. 
 Task allocation for the ETL and Reporting team. 
 Communicate effectively with client and their internal development team to deliver productfunctionality requirements. 
 Architecting and design of data warehouse ETL processes. 
 Demo of POC built for the prospective customer and provide guidance and gather the feedback toback end ETL testing on SQL Server 2008 using SSIS. 
 Create Integration Jobs to backup a copy of data in net file system. 
 Design and implement the ETL Data model and create staging, source and Target tables in SQL  server database. 
 Gathering and analysis requirements definition meetings with business users and document meetingoutcomes. 
 
Environment: Hadoop, MS Office,Talend Studio, ETL, ODS, OLAP ,SQL Server 2008.
Data Analyst/Data Modeler
Nestle - IN
February 2011 to October 2013
Description: The Nestle is a Swiss transnational food and drink company. Building and maintaining the company website while parallelly visualizing the data using Tableau. 
 
Responsibilities: 
 Implemented a job which leads an electronic medical record, extract data into OracleDatabase andgenerate an output. Analyze the data and provide the insights about the customers using Tableau.  Designed, implemented and automated modeling and analysis procedures on existing and experimentally created data. 
 Created dynamic linear models to perform trend analysis on customer transactional data in Python.  Increased pace & confidence of learning algorithm by combining state of the art technology and statistical methods. 
 Parseddata, producing concise conclusions from rawdata in a clean, well-structured and easilymaintainable format. Developed clustering models for customer segmentation using Python. 
 Developed entire frontend and backend modules using Python on Django Web Frame. 
 Implemented the presentation layer with HTML, CSS and JavaScript. 
 Involved in writing stored procedures using Oracle. 
 Optimized the database queries to improve the performance. 
 Designed and developed data management system using Oracle.. 
 
Environment: Python 2.x, Tableau, Oracle, MySQL 5.x, ORACLE, HTML5, CSS3, JavaScript, Shell, Linux & Windows, Django.
Data Analyst
Syntel - Pune, Maharashtra
September 2010 to January 2011
Description: My main role is to analyze data, check their functionalities and build the requirements with in the given time frame. 
 
Responsibilities: 
 Applied Business Objects best practices during development with a strong focus on reusability andbetter performance. 
 Developed and executed load scripts using Tera data client utilities MULTI LOAD, FAST LOAD andBTEQ. 
 Responsible for development and testing of conversion programs for importing Data from text filesinto map Oracle Database utilizing PERL shell scripts &SQL*Loader. 
 Developed Tableau visualizations and dashboards using Tableau Desktop. 
 Used Graphical Entity-Relationship Diagramming to create new database design via easy to use,graphical interface. 
 Formatting the data sets read into SAS by using Format statement in the data step as well as ProcFormat. 
 ed with the ETL team to document the Transformation Rules for Data Migration from OLTP to
Warehouse Environment for reporting purposes. 
 Used Graphical Entity-Relationship Diagramming to create new database design via easy to use,graphical interface. 
 Co-ordinate with various business users, stakeholders and SME to get Functional expertise, designand business test scenarios review, UAT participation and validation of financial data. 
 Responsible for development and testing of conversion programs for importing Data from text filesinto map Oracle Database,utilizing,PERL,shell scripts&SQL*Loader. 
 
Environment: Business Objects, Oracle SQL Developer, PL/SQL, MS SQL Server, TOAD, Tableau, Informatica, SQL*PLUS, SQL*LOADER, XML.
Education

Bachelor's",Data Scientist,emailed,indeed job post,resume
"
* 8+ years of neuroscience research experience in which I lead and collaborated in the creative design and execution of stroke research aims, resulting in 30+ publications, oral presentations, and $1 million+ in NIH grant funding
* Passionate about the critical thinking, problem solving, and data analysis aspects of my academic experience and am transitioning into a data science industry career in which those attributes are more predominantly utilized
* Self-taught R and Python programming languages, IDEs such as RStudio, Jupyter, and Spyder, and database queries through Hadoop and SQL. Created my own website in which I independently analyzed publicly available datasets and posted results. Achieving industry data science credentials, including the Dell EMC Proven   Data Science Associate certification

 and Certifications
Dell EMC Proven 
Data Science Associate	June 2019

Loma Linda University School of Medicine; Loma Linda, CA
Doctor of Philosophy in Physiology	July 2012  June 2016
Dissertation: Mechanisms of Post-hemorrhagic Hydrocephalus Development after Germinal Matrix Hemorrhage

University of California Berkeley; Berkeley, CA
Bachelor of Arts Double Major: Molecular and Cell Biology; Economics	August 2006  May 2010



R, RStudio, Microsoft R
Python, Jupyter, Spyder, Komodo, Visual Studio Code
SQL, Microsoft Azure
Hadoop, Hive, Impala
Machine Learning
Data Analytics
Project management
Time management
Written communication
Oral communication
Critical thinking, decision-making, and problem solving
Grant writing
Business acumen
Leadership 
Mentorship and training
Gathering and interpreting data 
Rodent models of stroke
Aseptic rodent surgeries
Rodent neurobehavioral assessments
Histology
Immunohistochemistry
Diolistic staining
Golgi staining
Brightfield microscopy
Fluorescence microscopy
Confocal microscopy 
Stereology
Spectrophotometric assays
ELISA
Western blot
RT-qPCR
Zymography
Cell culturing
Flow cytometry
ImageJ
SigmaPlot / SigmaStat
GraphPad
Neurolucida 360
NeuronStudio
Microsoft Office  Word, Excel, PowerPoint
Bioinformatics
Statistics
Predictive Modeling
Website Design, WordPress

Memberships and Associations
* 
Cheeky Scientist Association
Society for Neuroscience
Golden Key International Honors Society
American Physiological Society
Tau Kappa Epsilon International Fraternity
California Golden Bears Alumni Association
* 


Albert Einstein College of Medicine, Bronx, NY
Postdoctoral Research Fellow							February 2017  June 2018
* Managed two separate  that investigated hippocampal neurogenesis and dendrite spine development in premature rabbits (delivered via C-section) as well as premature rabbits with germinal matrix hemorrhage
* Quantified hippocampal dendrite spines from 430+ histological z-stack images of Golgi-stained coronal brain sections, taken by brightfield microscopy and analyzed using Neurolucida 360. Also quantified hippocampal dendrite spines from 540+ histological z-stack images of diolistically stained coronal brain sections, taken by confocal microscopy and analyzed using NeuronStudio
* Stereologically quantified hippocampal dendrite branching and dendritic spines in 36+ histological brain sections. Also stereologically quantified immuno-stained markers of neurogenesis in 240+ histological coronal brain sections
* Performed BLAST searches and other bioinformatics search techniques to find antibodies specific for target sequences that were significantly homologous with rabbits 
MilliporeSigma / Randstad, Temecula, CA
Data Associate								November 2016  February 2017
* Data mined and calculated product formulations for 3000+ products with fellow EHSM team members
* Intensively searched databases, SOPs, manufacturing records, and other relevant resources to compile formulation data
Loma Linda University School of Medicine, Loma Linda, CA
Laboratory Manager, Pre-doctoral Student, and Visiting Student Researcher	July 2011  June 2016
* Lead the germinal matrix hemorrhage / intracerebral hemorrhage research groups and executed the specific research aims in our NIH R01 grant for germinal matrix hemorrhage, resulting in 5 publications related to the project. Communicated our findings in annual progress reports for the NIH
* Wrote, edited, and submitted two NIH R01 grants and one R21 grant. Wrote, edited, and submitted 30+ research / review manuscripts for publication. Peer-reviewed and evaluated 10+ grants and 30+ manuscripts
* Mentored and trained 10+ new lab members (a mix of students, post-docs, and neurosurgeons) in our hemorrhagic stroke models and basic laboratory techniques. Assisted with the design and implementation of their independent studies
* Performed over 500 stereotaxic rodent surgeries using aseptic technique to model glioblastoma multiforme, intracerebral hemorrhage, and germinal matrix hemorrhage, then assessed rodent neurofunctional deficits utilizing a battery of evaluations
Uber | Lyft | DoorDash | Postmates 
Independent Contractor	July 2016  November 2016 | July 2018 - Present
* Using the company smartphone app, picked up customers and/or their food orders and dropped off at their given delivery address

",Data Scientist,emailed,indeed job post,resume
" 
Full Stack LOCATION 	Los Angeles, CA 
Most used programming languages, frames, 
: 
   Amazon.com  	                        LOS ANGELES, CA                                                     
   Full-stack Developer                                                    Jun.2018  Jun.2019 ES6-7 JavaScript, Python, jQuery, React JS, Angular JS, Node 
JS, Vue JS, Redux, Router, Babel, Gulp, Webpack 
 Developed the Amazon affiliated referral program using ES6, React,  Most used UX/ UI  & testing : 

 	Angular, Node JS, Express, Routers, running real-time analysis 
 Specified user interface, system-level interfaces, deployment and  	neting/ security architecture, and RDBMS data modeling 
 Designed and implemented Restful APIs and GraphQL APIs by  using Express, Apollo as the client side 
 Automated test and deployment to AWS with AWS Cloudformation, 
 	EC2, S3, RDS, DynamoDB, Jenkins, and Ansible 
	Bank of America HQ 	 LOS ANGELES, CA  
	Full-stack Developer 	                     Jun.2017  May.2018 
 Programmed automation analyzation of public firms financial statements, debt repayment leverage (DSO & FCC), interpreted sensitivity analysis and asset investment project forecasts by using DCF and statistics correlation analysis 
 Built risks adjusted investment  for individuals with net worth more than 500k focused on the fixed income market (specifically treasury bonds, corporate bonds, mutual funds, and ETFs) 
 Performed equity research and conducted industry outlook through Bloomberg Terminal, WSJ and Seeking Alpha 
 Developed 3 marketing modules for asset portfolio accrual,  targeting money in motion, for 3 million dollars 
	SALEM PARTNERS, LLC 	NEW YORK, NY 
	Full-stack Developer Intern 	Feb.2017  May.2017?
 Programmed a tool to identify prospects for M&A opportunities and  the evaluation of strategic alternatives in preparation for  introductory presentations with investors and CEOs 
 Learned to build full-integrated model system for projection model,  DCF, LBO model, accretion/ dilution merger models, including  ability to run operational and capital structure sensitives within  models and data tables 
 Participated in 3 live transactions and 7 prospective deals across  the consumer, industrial, and real estate industries; including due  diligence sessions the drafting of confidential information 
HTML5, CSS3, SASS, LESS, Material Design, Flexbox, Grid 
System, Bootstrap4, Vuetify, Unit Test, Integration Test, Enzyme, Jasmine, Jest, Mocha  
Most used APIs, database and backend: 
RESTFUL APIs, Axios, GraphQL APIs, Apollo, PostgressQL 
Docker, Express, Mongo DB, MySQL, PHP, Laravel, AWS EC2 
Methodologies: Agile, Scrum, Kanban 

University of Southern California  
Viterbi School of Engineer  Computer Science 
Major GPA in Computer Science: 3.6 
LOS ANGELES, CA                          Aug.2013  May.2017 
Completed full-time web development intensive course + received in-depth training and feedback from  developers + built  using full-stack development  in fast paced environment
Interests
, computer algorithms, visual design, machine learning, sociology, photography",Data Scientist,emailed,indeed job post,resume
"? Around 4 years of experience in IT and Computer Science as Data Scientist with strong expertise, business experience, and communication  to drive high-impact business outcomes through data-driven innovations and decisions. 
? Experience with Statistical Analysis, Data Mining and Machine Learning  using Python and SQL.
 
? Data Driven and highly analytical with ing knowledge and statistical model approaches andmethodologies (Clustering, Regression analysis, Hypothesis testing, Decision trees, Machine learning), rules and ever evolving regulatory environment. 
? Good familiarity in entire Data Science project life cycle, including Data Acquisition, Data Cleansing,
Data Manipulation, Feature Engineering, Modeling, Evaluation, Optimization, Testing and Deployment. 
? Experience in using Statistical procedures and Machine Learning algorithms such as ANOVA,
Clustering, Regression and Time Series Analysis to analyze data for further Model Building. ? Experience in problem solving, data science, Machine learning, statistical inference, predictive analytics, descriptive analytics, prescriptive analytics, graph analysis, natural language processing, and computational linguistics; with extensive experience in predictive analytics and recommendation. 
? Hands on experience on clustering algorithms like K-means, Medoids clustering and Predictive andDescriptive algorithms. 
? Proficient knowledge on Mathematical Matrix Operations, Statistics, Linear Algebra, Probability,
Differentiation, Integration and Geometry. 
? Expertise in Model Development, Data Mining, Predictive Modeling, Descriptive Modeling Data
Visualization, Data Clearing and Management, and Database Management. 
? Experience using machine learning models such as random forest, KNN, SVM, logistic regression andused packages such as ggplot, dplyr, lm, rpart, Random Forest, nnet, NumPy, sci-kit learn, pandas, etc., in python. 
? Logistic Regression, SVM, Clustering, neural nets, Principal Component Analysis and goodknowledge on Recommender Systems. 
? Proficient in Statistical Modeling and Machine Learning techniques (Linear, Logistics, Decision Trees,
Random Forest, SVM, K-Nearest Neighbors, Bayesian, XGBoost) in Forecasting/ Predictive Analytics,
Segmentation methodologies, Regression-based models, Hypothesis testing, Factor analysis/ PCA, Ensembles. 
? ed and extracted data from various database sources like SQL Server, BigQuery, and SQLite. ? Proficient in advising on the use of data for compiling personnel and statistical reports and preparing personnel action documents, patterns within data, analyzing data and interpreting results. 
? Good knowledge on Deep Learning concepts like Multi-Layer Perceptron, Deep Neural Nets,
Artificial Neural Nets, Convolutional Neural Nets, Recurrent Neural Nets. 
? Hands on experience on Deep Learning Techniques such as Back Propagation, Choosing Activation
Functions, Weight Initialization based on Optimizer, Avoiding Vanishing Gradient and Exploding
Gradient Problems, Using Dropout, Regularization and Batch Normalization, Gradient Monitoring and
Clipping Padding and Striding, Max pooling, LSTM. 
? Experience in using Optimization Techniques like Gradient Descent, Stochastic Gradient Descent,Adam, RMS prop. 
? Experience in building models with Deep Learning frames like Tensor Flow and Keras. 
? Actively involved in all phases of data science project life cycle including Data Extraction, Data
Cleaning, Data Visualization and building Models. 
? Extensive hands-on experience and high proficiency in writing complex SQL queries like storedprocedures, triggers, joins and subqueries along with that used MongoDB for extraction data. ? Good knowledge of Hadoop Architecture and various components such as HDFS, Job Tracker, Task Tracker, Name Node, Data Node, Secondary Name Node, MapReduce concepts, and ecosystems including Hive and Pig. 
? Experience with data visualization using  like GGplot, Matplotlib, Seaborn, Tableau and using
Tableau software to publish and presenting dashboards, storyline on web and desktop platforms. ? Well experienced in Normalization & De-Normalization techniques for optimum performance in relational and dimensional database environments. 
? Regularly accessing JIRA tool and other internal issue trackers for the Project development. 
? Skilled in System Analysis, E-R/Dimensional Data Modeling, Database Design and implementing
RDBMS specific features. 
? Expertise in all aspects of Software Development Lifecycle (SDLC) from requirement analysis,Design, Development Coding, Testing, Implementation, and Maintenance. 
 
 AND : 
Languages Java, Python (NumPy, SciPy, Pandas, Genism, Keras, Seaborn) Scala, JavaScript, SQL, HTML, CSS, Kotlin, Perl. 
Databases MySQL, MongoDB, SQLite and big data. 
Data Mining and 
Dev  
 
MS Excel, Tableau, SQL Server, IDLE, Jupyter Notebook, Django, 
Flask, PyCharm, Visual Studio Code, Keras, Tensor Flow, Postman, Docker. 
 
Big Data  Hadoop, PySpark, Hive, HDFS, MapReduce, Pig, Kafka. 
 and Utilities Microsoft Office, Tableau, Microsoft Excel, Tensorboard, Matplotlib, NLTK, Numpy,
Pandas, scikit-learn, Node.js. 
Machine Learning 
Linear Regression, Logistic Regression, Gradient boosting, Random 
Forests, Maximum likelihood estimation, Clustering, Classification & 
Association Rules, K-Nearest Neighbors (KNN), K-Means Clustering, Decision 
Tree (CART & CHAID), Neural Nets, Principal Component Analysis, 
Weight of Evidence (WOE) and Information Value (IV), Factor Analysis, 
Sampling Design, Time Series Analysis, ARIMA, ARMA, GARCH, Market Basket Analysis, Text mining. 
 
Deep Learning 
Artificial Neural Nets, Convolutional Neural Nets, ResNet50, 
MobileNet, Recurrent Neural Nets, LSTM, Natural Language Processing, 
Transfer Learning, Embeddings, Residual Nets, Fine Tuning, GRU, 
SoftMax Classifier, Back Propagation, Choosing Activation Functions, 
Drop out, Optimization Algorithms, Vanishing and Exploding Gradient, 
Striding, Padding, Optimized weight Initializations, Gradient Monitoring and Clipping, Batch Normalization, Max Pooling. 
 
Cloud : Amazon Web Services (EC2, EBS, S3, VPC, RDS, SES), Google Cloud (App Engine,
Compute Engine, AI Platform). 
Methodologies and Version Control Agile, Scrum, Git.
 Experience

Data Scientist
Rosedale Federal - Rosedale, MD
September 2017 to Present
Description: Rosedale Federal Savings and Loan Association operates as a full-service bank. I  there as Machine Learning Engineer. I ed on a project to classify customer complaints and queries using Natural Language Processing and Deep Learning. I utilized various machine learning/statistical algorithms to identify the correlation between different features and utilized  like TFID and SHAP to visualize the relation between customer queries and responses. 
 
Responsibilities: 
? ed on huge data sets and developed predictive models with machine learning techniques 
? Used Natural Language Processing to perform complaint classification and labeling ? Used Pandas, NumPy, SciPy, Matplotlib, Seaborn, Scikit-learn in Python at various stages for developing machine learning model and utilized machine learning algorithms such as linear regression,
Naive Bayes, Random Forests, Decision Trees, K-means, & KNN 
? Evaluated models using Cross validation, Log loss function used to measure the performance andused ROC curves and AUC for feature selection. 
? Used Spark's Machine learning library to build and evaluate different models. 
? Used LSTM and RNN models to detect phishing websites using Keras and TensorFlow. 
? Addressed overfitting and underfitting by tuning the hyper parameter of the algorithm and by usingL1 and L2 Regularization 
? Trained machine learning models for fraud detection using customer transaction data. 
? Visualized data using different visualization libraries such as bokeh, seaborn and matplotlib 
 
Environment: Jupyter Notebook, Postman, Flask, Python, AWS, Visual Studio, Gitlab, Anaconda.
Data Scientist
InSure - Towson, MD
January 2016 to August 2017
Description: The INSuRE (Information Security Research and ) project aims to build research  and experience for graduate students through a research net between CAE-Rs (Centers of Academic Excellence in Research) in Information Assurance/Cyber Defense. Through the project, students engage in interdisciplinary, distributed-team research on tasks in the national information security domain. The students learn research by doing, building , expertise, and connections that will enable them to hit the ground running faster on information assurance research projects later in their careers. 
 
Responsibilities: 
? Developed Python modules, machine learning & predictive analytics for day to day businessactivities 
? Analyzed large data sets apply machine learning techniques and develop predictive models,statistical models and developing and enhancing statistical models by leveraging best-in-class modeling techniques 
? Conducting studies, rapid plots and using advanced data mining and statistical modeling techniquesto build a solution that optimizes the quality and performance of data 
? ed with parameter tuning and model evaluation techniques Confusion Matrix, Cross validation,AUC-ROC etc. Customer Profiling models using K-means and K-means++ clustering algorithms to enable targeted marketing 
? Implemented dimensionality reduction using Principal Component Analysis and k-fold crossvalidation as part of Model Improvement 
? Used LSTM and RNN models to detect malicious websites using Pandas, Keras and TensorFlow. ? Perform Exploratory analysis, hypothesis testing, cluster analysis, correlation, ANOVA, ROC Curve and build models in Supervised and Unsupervised Machine Learning algorithms, Text Analytics & Time Series Forecasting 
? Deployed the trained and optimized model to AWS EC2 instance using Flask production server to beserved as an API for other services. 
? Demonstrated experience in design and implementation of Statistical models, Predictive models,enterprise data model, metadata solution and data lifecycle management in both RDBMS, Big Data environments 
? Generated dashboards using Tableau to visualize the data and provide solutions to the existingproblems as well as insights to improve customer experience and organizational performance 
? ed with numerous data visualization  in python like matplotlib, seaborn, ggplot. 
 
Environments: Python, PyCharm, Jupyter Notebook, Spyder, Tableau, MySQL, Ubuntu server, Visual Studio Code.
Data Analyst and Machine Learning
BigScale Tech - Surat, Gujarat June 2014 to November 2014
Description: BigScale Tech is leading software company based in Surat, Gujarat, India. Specializing in custom software development in many different fields including Data Analysis, custom Software Development and Database Storage. I ed with the data analysis team at the company on several projects with the clients. 
 
Responsibilities: 
? Collected, Cleaned and Analyzed the data from different government websites and private fertilizercompanies to be modeled. 
? Designed, built, and implemented relational databases. 
? ed on huge data sets and developed predictive models with machine learning techniques. 
? ed with various regression algorithms like Random Forest Regression, Decision Tree regression,
XGBoost regression, etc. to forecast fertilizer requirement in rural farming. 
? Communicated effectively in both a verbal and written manner to client team. 
? Successfully interpreted data to draw conclusions for managerial action and strategy. 
? Completed documentation on all assigned systems and databases, including business rules, logic,and processes. 
? Maintained the data integrity during extraction, manipulation, processing, analysis and storage. 
? Dedicated time in keeping the code clean and organized while having the proper documentation inplace by logging all activity. 
? Constructed efficient data infrastructures that are easy to maintain and can be used effectively.Seamlessly spotting and resolving any issue within the infrastructure 
 
Environments: Python, Notepad ++, MySQL database, Ubuntu, Anaconda.",Data Scientist,emailed,indeed job post,resume
"
Health Services Research and Evaluation Intern
IEHP, Inland Empire - Rancho Cucamonga, CA
November 2018 to Present
 Conducted literature review on health disparities, social determinants of health, evaluation of health programs 
 interpreted statistical results for Health Evaluation presentations at American Public Health
Association (APHA) 2018 conference 
 Investigation into maternal health and potential programs that may help disadvantaged, low-incomepregnant women 
 Took part in research meetings with Health Services Research Director, analysts, evaluators, andprofessors to talk about survey data usage, data analysis
Resident Conference Assistant
UCR- Housing - Riverside, CA
June 2014 to June 2016
* ed on organizational  for the planning of present and future conferences for over threethousand residents 
* Maintained accurate conference files and documents for accurate billing information, conducted keyinventory and file audits 
* Performed general office duties, distributed mail, maintained conference files and documents foraccurate billing information, conducted key inventory and file audits 
* Provided administrative support to Conference Group Directors, RSO Administrators, Dining Services,
Custodial, Maintenance and Residence Life Staff
Resident Services Assistant
UCR- Housing - Riverside, CA
August 2013 to June 2016
* Led university housing tours on the biggest UCR welcoming events and throughout the year. * Ensured that UCR Housing Protocol was being followed by residents, wrote out incident reports, identified and reported maintenance issues on property. 
* Conducted monthly audits, room inventories, inspection reports, answered phones, and created andposted critical notices. 
* Monitored and maintained the residential computer labs and aided with general inventory stocking.


MPH in Epidemiology
Loma Linda University - Loma Linda, CA
September 2017 to June 2019
B.S. in Biology
University of California - Riverside, CA June 2016


SAS (1 year), R (1 year), Receptionist (3 years), SPSS (Less than 1 year), Microsoft Access (Less than 1 year), Microsoft Excel (4 years), Microsoft Word (10+ years)
Awards

Dean's Honors List
December 2016
Dean's Honors List June 2016
Groups

Loma Linda School of Public Health Association
September 2017 to Present
Member
LLU Writing Center Pilot Study Group
August 2018 to Present
Assist in writing center services by critiquing podcasts, seminars, and other  that may be provided to all students
Additional Information

Typing speed: 71",Data Scientist,emailed,indeed job post,resume
"Medical and entrepreneurial services are what I can provide. Additional information upon request. Extensive credentials.
Willing to relocate to: Beaumont, CA - Apple Valley, CA - Fontana, CA
Authorized to  in the US for any employer
 Experience

Anesthesia Technician
Loma Linda University Medical Center
2019 to Present
Primary responsibility is to assist the licensed anesthesia provider in clinical settings where anesthesia is administered at all sites to include MRI/CT/GI Labs/Cath Labs/Interventional 
Radiology. Prepared appropriate instruments, equipment and supplies for the administration of each age-specific type of anesthetic procedure. Can demonstrate practical knowledge and expertise in all general areas of anesthesia (general anesthesia, procedure-related sedation, and regional techniques). Assists with operating room turnover and patient flow. Achieves and maintains a trusting and credible ing relationship with anesthesiologists, nurse anesthetists and operating room administration and staff. Performed other duties as needed.
Anesthesia Technician
Victor Valley Global Medical Center 2014 to 2018
Assisting the Anesthesiologist with all indicated procedures. Assist the circulating nurse with all tasks intra-op within my scope of practice. Complete daily anesthesia machine check-outs and calibration. Assist intra-op with non-critical and critical monitor and/or anesthesia machine errors or faults. Reconciled incoming supplies and materials. Set-up and turn over all anesthesia supply carts for incoming cases. Maintain expired and non-expired materials. Stock all Operating Rooms fully with anesthesia related items before, during and at the end of the  day. Properly sterilized and packaged all anesthesia equipment after every case. Recorded temperature logs for fluids and warm blankets. Help others to adhere to policies and procedures. Complete all assigned tasks efficiently and courteously.
Partner/General Manager
Black Rose Marketing and Design - Apple Valley, CA
January 2016 to January 2017
Partner and Sales Executive of Black Rose Marketing and Design. Facilitated in acquiring new clients, provide face-to-face or video call consulting and customer-service. Established online presence across all clientele via. Social Media advertising and marketing, SEO, Web Page Optimization, Google Analytics, etc.
Squad Leader, Platoon Guide
US Army National Guard
2010 to 2015
Combat Medic / Health Care Specialist 2010 - 2015 Completed Basic Training with honors and early promotion PV1-PV2. Became Guide-on Bearer, Squad Leader, Platoon Guide. Completed Advanced Individual Training (Health Care Specialist) with honors.
Director
Marketing and Advertising
2013 to 2013
Answered phone calls, handled invoices and bills. Market and Advertised newly listed 
Residential and Commercial real estate. Syndicated  marketing materials to various websites. Listed real estate in local newspaper advertising. Prepped office for meetings, classes and events.
Finish (Trim) Plumber, Construction Apprentice
Hitt Plumbing Co
2006 to 2013
Answered phone calls, handled invoices and bills, reconciled company equipment and incoming materials. Prepped jobs for plumbing, demolition, construction debris clean up, delivered supplies to jobs. Assisted foreman with the current tasks of the day. Installed finish (trim) plumbing for new construction and/or remodeled homes and facilities to include commercial and residential.


BSN in nursing
Los Angeles Pacific University
2017 to Present
High school or equivalent
Granite Hills High School - Apple Valley, CA 2009


Patient care, Ultrasound, Excellent written, Written and verbal, Problem-solving, Team player, Excel, Microsoft word, Word, Publisher (2 years), Adobe (2 years)
Military Service

Branch: United States National Guard
Rank: SPC / PV4
Additional Information

 
Patient advocate. 
Well versed with Sonosite, Echo Machine and Ultrasound Guided procedures, such as regional blocks and line placements. Arterial Line setup and assist. 
CVP Line setup and assist. 
Swan Ganz setup and assist. 
HotLine setup proficient. 
Spinal Block assist proficient. 
Epidural assist proficient. 
Difficult Intubation proficient. 
Strong Computer  (Microsoft Word, Publisher, Excel / All Adobe Products). Proficient and balanced with medical treatment and patient care. 
Proficient and adapts well to trauma settings. 
Excellent written and verbal . 
A strong eye for detail. 
Productive in a high stress environment. 
Strong problem-solving . 
Driven individually, and as a team player. 
Can recognize personal weaknesses and strengths. 
Understands limitations established by and for corporate well-being.",Data Scientist,emailed,indeed job post,resume
" 
 
Personable Data Analyst with experience in ing with large data sets to provide custom reports and analytical support. Passionate about leveraging current  and learning new  to identify and contribute to business solutions. Highly interested in a parttime Data Scientist position to continue to grow Data Science et. Motivated to learn the necessary  to succeed at The Massage Negotiator. Experienced in leveraging SQL, Python and Excel for Data Analysis, Text Processing, Reporting and ETL processes. Maintains a high  ethic and is adept to ing independently or within a team setting to meet strict deadlines. Possesses excellent customer service and oral/written communication . 
  
Python 
SQL 
Excel / VBA 
Reporting 
Tableau 
SAS 
Java 
Experience 
Data Analyst  
ERI Economic Research Institute | Feb 2019  Present 
 Research and collect compensation data, conduct statistical analysis/modeling and communicate findings to senior team members. 
 Leverage Python, SQL and Excel to perform analysis for recurring & ad-hoc requests, maintain/clean databases and enhance current data management processes. 
 Perform text processing and analysis with python using Natural Language Processing and String Matching libraries.  
 Develop internal python GUIs and programs for web scraping and automation of ETL processes. 
 
Business Management Analyst  
Northrop Grumman | July 2018  Feb 2019 
 Perform and present analysis and prepare reports using Project Management software (SAP & MPM) and Excel to ensure that contracts are within negotiated and agreed-upon parameters. 
 Extract large amounts of data from databases to clean and transform data into presentable charts and reports. 
 Create Excel Macros using VBA to automate routine reports and streamline processes for analysis and reporting. 
 
Operations Data Analyst 
Irvine Company | Aug 2017  May 2018 
 Extensively utilized SQL Server, MS Query functions, vlookup, pivot tables, Tableau Server and SAP Business Objects Web Intelligence to provide analytical support. 
 Prepared and presented various quarterly, monthly, weekly and ad-hoc reports to executive management regarding Irvine Company Apartment data. 
 Prepared documentation and new user guide to assist in implementing new automated reports. 
 
Data Analyst (Part-time, Remote) 
Colibri Group | Oct 2018  Present 
 Extract data from various sources (Google Ads, Bing Ads, Google Analytics), transform in Excel and load data to be read into PowerBI and Tableau. 
 
Bachelor of Arts, Business Administration 
California State University, Fullerton | 2013  2017 
Emphasis: Information Systems 
 
Certification, Applied Data Science with Python University of Michigan | 2018 - 2019 
 
Computer Science Course 
Foothill College | 2019 
Courses: Intro to Programming  Python, Object Oriented 
Programming  Java, Object Oriented Programming  Python, 
Intermediate Software Development  Java,  
 ",Data Scientist,emailed,indeed job post,resume
"Significant experience turning data into easily accessible information to support decision making. Skilled in the use of open source  to manage data, conduct descriptive and inferential statistics, and present relationships between data in an intuitive graphical format. Concise, clear writer adept at explaining complex concepts to a general audience. Readily learn and adopt new . Tactfully engage end users to formulate answerable research questions that are important to them. Help users to plan and conduct data collections. 
 
Analysis, Visualization, Reporting 
flow Design and Prototyping 
Advanced Statistics and Modelling 
Budget and Personnel Management
Willing to relocate: Anywhere
 Experience

Associate Director, Institutional Research and Effectiveness
Harvey Mudd College
July 2018 to June 2019
Supported the continuous improvement efforts of the college by promoting the informed use of institutional data in evidence-based decision making. Managed entire student evaluation of teaching process from instrument design and creation, through data collection, analysis, and reporting of results for 250 courses per semester for both formative and summative purposes. 
? Wrote R program that tabulated survey results by student major (allowing for double and joint majors) to allow departments to focus on just the students they teach and improve their learning.
Staff to the Academic Deans Committee
The Claremont Colleges Academic Deans Committee - Claremont, CA
July 2012 to June 2019
Negotiated a new process with central business staff for the submission of intercollegiate budgets for presidential consideration to allow for more considered use of consortial resources. Had signature authority over their $100K off-budget expenditures and partnered with central business staff to move  onto the budget when possible to improve financial planning. 
? Wrote SQL queries to produce consortium-level views of registration data to inform the planning ofintercollegiate programs to optimize the use of shared resources. 
? Built searchable online archive of Committee minutes and documents to enhance institutionalmemory and learning.
Director, Academic Operations
Harvey Mudd College
July 2007 to June 2018
Researched causes of disparate academic performance and wrote a report used to demonstrate commitment to a culture of evidence in the reaffirmation of accreditation process. 
Supervised machine and electronic shops to promote safety and the prudent use of resources. ? Built and piloted online system for Student Evaluation of Teaching using Qualtrics to email students at a class time of the instructor's choosing to ensure high response rates and collect meaningful information to allow instructors to optimize courses for student learning. 
? Queried Student Information System to get data to analyze in order to study the effect of sex, ethnicity, and SES on student performance and wrote report that served as an appendix to the 2010 WASC self-study and motivated faculty to make courses more welcoming.
Associate Dean for Planning, IR and Assessment
Harvey Mudd College
July 2004 to June 2007
Oversaw reporting to governmental and accrediting agencies. Chaired college assessment committee. Co-chaired college diversity task force. Co-authored planning documents with the president of the college. Developed pilot portfolio assessment of student learning. 
? Built and deployed a LAMP server to collect, store, and manipulate administrative data resulting indecreased administrative overhead. 
? Collaborated with College President to study the financial impact of cross registration on theClaremont Consortium to inform strategic planning and staffing.


Doctor of Philosophy in 
Claremont Graduate University - Claremont, CA
Master of Arts in Philosophy
The Claremont Graduate School - Claremont, CA
Bachelor of Arts in Economics
Pitzer College - Claremont, CA


DATABASE, DBASE, SQL (10+ years), C++, GGPLOT2 (3 years), VBA, COGNOS (10+ years), LINUX (10+ years), SPSS (10+ years), STATISTICS (10+ years), VISUALIZATION, EXCEL (10+ years), PARADOX, Managerial, Budget Management (10+ years), R (9 years)
Links


Military Service

Branch: Army
Service Country: United States
Rank: Cadet
September 1980 to May 1985
Assessments

Data Analysis  Expert
July 2019
Interpreting and producing graphs, identifying trends, and drawing justifiable conclusions from data.
Full results: https://share.indeedassessments.com/share_assignment/yiec2z6gigfkth8f
Indeed Assessments provides  tests that are not indicative of a license or certification, or continued development in any  field.
Publications

Leibniz: His Philosophy and His Calculi https://pdfs.semanticscholar.org/4e92/1c48fa35049d40f50a5cb2fd626d29dd9779.pdf March 1999
Additional Information

COMPUTER  
 
? Operating systems: OSX, WIN, and LINUX 
? Programming language: C++ 
? Database programs: dBase, Filemaker, Paradox, and SQL 
? Database query : Cognos, Impromptu 8, SQL 
? Statistics and visualization software: ggmap, ggplot2, knitr, Lattice, R, SPSS, TeX 
? Spreadsheets: Excel with VBA, Google Sheets",Data Scientist,emailed,indeed job post,resume
" Experience

HotBox Pizza
September 2017 to Present
 Made pizzas for high-volume catering clients as well as individuals just wanting dinner 
 Responsible for taking orders both over the phone and in person 
 Communicated with entire crew to ensure that no mistakes would be made for high-volume orders 
 Balanced being a full-time student with ing a fast-paced job 25 hours per week 
 
Key : 
 : Microsoft Excel, Microsoft Word, R, Python 
 : Strong  Ethic, Team, Multitasking, Detail-Oriented 
 Academic: Pure and Applied Mathematics, Statistics, Finance, Independent Learning 
 
Relevant Course: 
Real Analysis Differential Equations Abstract Algebra 
Statistical Analysis Probability Linear Algebra 
Financial Mathematics Corporate Finance Accounting


Bachelor of Science in Mathematics in Mathematics
Purdue University - West Lafayette, IN
May 2021",Data Scientist,emailed,indeed job post,resume
"Willing to relocate: Anywhere
Authorized to  in the US for any employer
 Experience

Linux System Administrator/ Jr Devops engeneer
Data Service Group Inc - Delaware County, PA
June 2016 to Present
Experienced DevOps /Red hat certified Linux systems Administrator with over five years of experience in the IT field with excellent reputation for resolving problems, improving customer satisfaction, and driving 
overall operational improvements. Consistently saved costs while increasing profits. I am a motivated
IT  with hands on systems administration, System configuration, Enthusiastic team player , always looking for innovative and efficient engineering solutions, Energetic self-starter capable of learning quickly 
with minimal guidance. I am seeking to progress my career in the Information  sector where
I 
will use my  and experience in system maintenance and  troubleshooting to contribute to an active growth and productivity of the company. I am authorized to  in the United States for any employer.


PROBATOIRE
FULTANG


MYSQL, Javascript, CSS, Windows 2008, Automation, System Administrator, Bash Shell Scripting,,
Version Control Systems: Git, bitbucket, Continuous Integration: Jenkins, Nexus, Automation/ Deployment: Ansible,, Virtualization: Oracle Virtual Box,, Platforms: Linux Centos 6&7 RHEL 6&7, and Windows Server 2008, Storage: LVM, NAS, Ticketing: Jira/kanban board/Confluence/, Database: MySql, Apache, web Server, error.log,, Automation, Bash, Hardware, content,, Customer service,
Database Management, Firewalls, Help Desk, HTTP, inventory, LAMP, Managing, memory,, NAS,, NFS,
Net installation, net, Process management, Redhat Linux, SAMBA,, scrum, SSH, servers, Shell Scripting, shell scripts, SMTP, System Administrator, Tech Support, Troubleshoot, troubleshooting, upgrades,, Windows Server, Patching, Git",Data Scientist,emailed,indeed job post,resume
" Being a Passionate Data enthusiast and have ing experience as  qualified DataScientist in Statistical modeling, Machine Learning, Data Visualization and Data mining with large sets of both Structured and Unstructured Data. 
 Experience in feature extraction, creating Regression models, Classification, Predictive data modelingand Cluster analysis. 
 Strong experience in implementing Supervised Machine Learning Algorithms like Linear Regression,
Logistic Regression, Linear Discriminant Analysis (LDA), Decision Tree, Random Forest, Support Vector
Machines (SVM), Naive Bayes, K-Nearest Neighbor. 
 Extensive experience in providing Machine Learning and Data Mining solutions to various businessproblems based on requirements using Python 
 Strong expertise in implementing Unsupervised Machine Learning Algorithms like Hierarchicalclustering, K-means clustering, Probability Clustering, Density-Based Clustering. 
 Proficient in using Python libraries like Pandas, NumPy, Scikit-learn, Seaborn, Scipy for developingvarious machine learning models. 
 Around 3 years of experience in developing Deep Learning models like Conventional Neural Net(CNN), Artificial Neural Net, Multilayer perception's (MLPs), Recurrent Neural Nets (RNN) for recommended systems. 
 As a Data scientist actively involved in all phases of project life cycle including Data Extraction, Data
Cleaning, Data Visualization and building Models. 
 Strong experience in Software Development Life Cycle (SDLC) including Requirement Analysis,
Design Specification and Testing in both Waterfall and Agile methodologies. 
 Implementation experiences in Machine Learning and deep learning including Regression,Classification, Neural net, object tracking and Natural Language Processing (NLP's) using packages like Tensor Flow, Keras, NLTK and Spacy. 
 Proficient Mathematical knowledge on Matrix Operations, Statistics, Probability, Linear Algebra,
Differentiation, Integration and Geometry 
 ed with various data visualization  of python like Matplotlib, Seaborn, ggplot, pygal andusing of Tableau. 
 Hands on Experience in using GIT Version Control System. 
 Proficient with excellent initiative and innovative thinking  and ability to guide teammates tobreakdown large and complex issues to simplified versions for easy execution.
 Experience

Data Scientist
Whole Foods Market, Austin
October 2018 to Present
 Performed Customer segmentation based on customers behavior, demographics, transactions byusing customer specific details like age, income and created multiple customer classes. 
 Analyzed the customers purchase data and product trends to recommend the types of products forcustomers based on their behavior tracked through customer accounts. 
 Explored and created different new data sets to  with and implement few data science  flowplatforms for future applications. 
 Constructed customer classes with historical, demographic and behavioral data as features usingRandom Forest Classifier and Logistic Regression to help marketing team understand purchase pattern of customers. 
 Predicted sales and profits using machine learning and deep learning strategies. 
 Assisted marketing team to devise business strategy to target customers with discount coupons,deals and offers to improve customer purchases and maintaining stock at stores. 
 Communicated with management to discuss insights obtained from data, assisted in makingbest business decisions and reduced customer churn by 15% in few months of implementation by extracting value from data. 
 Applying clustering algorithms like partitioning clustering, fuzzy clustering, density-based clusteringmethods to group the data on their similar behavior patterns. 
 Identified distinct patterns in which customers respond to offers and clustered their actions usingK-means, K-means++ Clustering, Hierarchical Clustering and segmented them into different groups, helped marketing team to further analyze behavioral patterns of customers. 
 Created Customer Lifetime Value (CLV) from the customers data by using Multi-Linear Regressionalgorithm, identified high and low value segments and helped organization to understand customers and improve customer service to retain customers. 
 Performed personal and food sales Predictive Modeling by using decision trees and regressions inorder to get the risk involved by giving individual scores to the customers. 
 Proposed marketing strategies to target potential customers using their first three months data andfrom regression model, we evaluated CLV for every new customer. 
 Investigated large datasets to handle missing values, cleaned messy datasets and applied featurescaling to standardize range of independent variables. 
 Researched predictive models including Logistic Regression, Support Vector Machine (SVC) and re-enforcement learning to prevent retail fraud. 
 Improved model performance by tuning hyper-parameters using optimization techniques like Gridsearch, Random search and Bayesian optimization and increased model efficiency by XG-Boosting  Validated models using Cross validation, loss function to measure model performance and created Confusion Matrix, Receiver Operating Characteristic (ROC) and Cumulative Accuracy  (CAP) curves. Addressed over-fitting and under-fitting by tuning hyper parameters using L1 and L2 Regularization 
 Applied dimensionality reduction technique like Principal Component Analysis (PCA) to extractrelevant optimal features from high dimensional data. 
 Visualized results using Matplotlib, Seaborn libraries of scikit-learn and used Tableau to presentresults on dashboards for team members, Management and other relevant departments in company.  Forecast the company's short-term and long-term growth in terms of revenue, number of customers, various costs, stock changes etc., using machine learning algorithms.
Data Scientist
Adidas, Oregon
July 2017 to September 2018
 Developed predictive solutions to support online shopping using machine learning algorithms suchas Linear Regression, Logistic Regression, Naive Bayes, Decision Trees, Random Forest, Support Vector
Machine in Python. 
 ed on data cleaning, data preparation and feature engineering with Python, including NumPy,
SciPy, Matplotlib, Seaborn, Pandas, and Scikit-learn. 
 Responsible for data identification, collection, exploration, cleaning for appropriate modeling.  ed on NLTK library in python for doing sentiment analysis on customer product reviews and other third-party websites using web scrapping. 
 Performed sentiment analysis of customer reviews and classified each review into good, bad andneutral class to understand pulse of customers about business. 
 Implemented Time Series analysis on sales data to consider what measures to be taken for improvethe Sales. 
 Used MySQL and created SQL tables and involved in data loading and writing SQL UDFs. 
 Conducted analysis in assessing customer behaviors with clustering algorithms such as K-Means
Clustering and Hierarchical Clustering. 
 Evaluated parameters with K-Fold Cross Validation, Grid search methods to optimize performance ofmodels. 
 Along with data analytics and Excel data extracts, Implemented Agile Methodologies, Scrum storiesand sprints in a Python based environment. 
 ed on .csv, .json, .excel different types of files for the data cleaning and data analysis.  Performed Time Series Analysis on animal medicine and vaccine product sales data in order to extract meaningful statistics and other characteristics of the data to predict future values based on previously observed values. 
 ed in Tableau environment to create weekly, monthly, daily reports using tableau desktop &publish them to server. 
 ed on Excel using pivots, conditional formatting, large record sets, data manipulation andcleaning. 
 Used GIT HUB as version control software to manage the source code and to keep track of changesto files which is fast and light weight system.
Data Scientist
First Data - GE
February 2016 to June 2017
 Analyzed the data using various machine learning algorithms to segregate all transactions made bycustomers depending on the amount and total transactions. 
 Extracted Tera bytes of both structured and unstructured data by using SQL queries and performeddata mining tasks including handling missing data, data wrangling, feature scaling. 
 Developed an easy to use documentation for the frames and  developed for adaption byother teams. 
 Implemented Porter Stemmer (Natural Language Tool Kit) with NLP bag of words model using Count
Vectorizer class to process text data. 
 Created predictive model using LSTM, Recurrent Neural Nets (RNNs) and studied reviews,obtained feedback on customer service to help employer reduce customer churn. 
 Experimented with other classification models like Random Forests, Logistic Regression and Nave
Bayes to classify customers reviews. 
 Extracted data from web using Web Scraping, Text mining and processes data into tab separated fileto separate reviews by tab in data. 
 Cleaned dirty data and prepared data for feature extraction using Count Vectorizer of sci kit-learnfeature extraction library. 
 Automated customer service by creating chat box which responds to customer queries using deeplearning and text processing with nltk of NLP library. 
 Evaluated model performance by creating confusion matrix, classification report and accuracy score.Improved model performance by k-fold cross validation and XG-Boosting and achieved model accuracy of 92%. 
 Developed recommended systems using Apriori Principle Algorithm, for mining frequent item setsand relevant association rules to operate database containing a lot of transactions. 
 Built machine learning algorithms to forecast the company's short term and long term growth interms of revenue, number of customers, stock changes and other. 
 Demonstrated experience in design and implementation of Statistical models, Predictive models,enterprise data model, meta data solution and data life-cycle management in both RDBMS, Big Data environments. 
 Presented simple visualization of results using seaborn visualization libraries of Python.  Used python for statistical operations on the data and seaborn, ggplot for visualizing the data regarding the sales and customers.
Data Analyst
Karvy capital Ltd - Hyderabad, Telangana May 2013 to December 2015
 Acquired data from primary or secondary data sources and maintain databases/data systems. 
 Established new client data preparing them for entry into new platform. 
 Loaded data by converting CSV file into corresponding database tables. 
 ed with management team to create prioritized list of needs for each business segment.  Monitored and resolved issues of data flow on daily basis. Also created views for reporting team to use data for marketing numbers on daily basis. 
 Collaborated with reporting team to resolve data discrepancies and logical data corrections which areoccurring throughout reports. 
 Generated Tableau ad-hoc reports using excel sheet, flat files, CSV files. 
 Designed, built, and implemented relational databases 
 Used data mining techniques for outlier detection and created algorithm to connect patternsbetween customer trends. 
 Created Software solutions in Software development lifecycle (SDLC) and Agile methodologiesenvironment. 
 Performed computational tasks on data by creating pig, hive and Map reduce scripts to access andtransform data in HDFS. 
 Developed and implemented metadata models for reporting functionalities and developedautomated process for data corrections. 
 Developed SQL, NoSQL and PL/SQL scripts to extract data from database and for testing Purposes.  Reviewed logical model with application developers, ETL team, DBAs, and testing team to provide information about data model and business requirements. 
 Identified and logged defects if/when test fail, using SQL to narrow down root cause of problem forefficient investigation by development team and log accordingly. 
 Used advanced Excel functions to generate spreadsheets and pivot tables.
Education

M.S.
Southern Arkansas University


Boosting, Decision trees, Hierarchical clustering, K-means, K-nearest neighbor, Linear discriminant analysis, Lda, Linear regression, Logistic regression, Machine learning, Principal component analysis,
Random forests, Support vector machines, Clustering, Python, Sql, Java, Algorithms, Statistics, Operations
Links


Additional Information

TECHNICAL : 
Languages Python, SQL, Java 
Mathematical  Statistics and Probability, Linear Algebra, Matrix Operations, Calculus 
Machine Learning Algorithms 
Linear Regression, Logistic Regression, Linear Discriminant Analysis (LDA), Decision Trees, Random
Forests with Adaboost and Gradient Descent Boosting, Support Vector Machines (SVM's), Naive Bayes,
K-Nearest Neighbor, Hierarchical clustering, K-means clustering, Probability Clustering, Density-Based Clustering. 
 
Machine Learning Techniques 
Principal Component Analysis, Data Standardization Techniques, L1 and L2 regularization, Hyperparameter tuning, Resampling Techniques like SMOTE, Cluster Centroid Methods, Feature selection and Feature Engineering, Cross Validation Methods(K-fold).",Data Scientist,emailed,indeed job post,resume
"
Experience 
 	Mustang Analytics LLC Data Scientist and co-founder   Sept 2018  Present 
 Provided and presented deliverables to company executives 
 Created a brand and established a company 
 Consulted with clients on a number of data analytics issues 
 	Risk Analyst Summer at Goldman Sachs  	 	June 2017 - Aug 2017 
 Evaluate and assessing daily risk in the market 
 Predicted changes in the market and when underperformance could occur  
 ed within a large team to analyze a shock situations effect on the market 
Data Analysis  
 Wrist Wearable data analysis to predict an individuals activity using only directional movements (January 2018  June 2018) 
 IPO Financial investment project using web scraping and machine learning to find the best long term and short term investments (May 2018 - Present) 
 Performed web scraping and topic modeling techniques on presidential candidate speeches to find differences between syntax (December 2017) 
 Building an NFL prediction model and 4th down analysis using a database of NFL plays with SAS software (January 2017) o http://support.sas.com/resources/papers/proceedings17/2023-2017.pdf 
	Data Scientist Internship at VISA  	 	 	June 2016 - Sept 2016 
 ed to find revenue leakage and find ways to maximize profits 
 Analyzed datasets containing over 160 million records & 120 variables 
 
California Polytechnic State University, San Luis Obispo 
 Bachelors: Statistics (Minor: Data Science & Computer Science)  Graduated June 2018 
 GPA: 3.2  
 Certified: SAS 9 Advanced Programming Certified 
 
Computer  
 Python, R, Spark, Map Reduce, SAS, C, Java, SQL, Pandas, Numpy, Scikit-learn 
Other Experience  
 NBA Hackathon Finalist  	 	 	 	Sept. 2017 
 Finalist: SAS Student Symposium  	 	 	April 2017 ",Data Scientist,emailed,indeed job post,resume
" 
* Dedicated IT  with around7 years of experience in Data Analysis, DataProfiling, DataIntegration, Data Visualization, Predictive modeling, MetadataManagement,Business requirement analysis, Quality assurance, Design, Development, and Testing to further the success of various organizations business goals and s.
* Extensive experience in Machine Learning solutions to various business problems and generating data visualizations using Python and R.
* Hands on experience with in extracting and modeling datasets from variety of data sources like Hadoop using Big data  such asKafka, Pig, Hive, Spark,Sqoopfor adhoc analysis.
* Advanced knowledge of statistical techniques in Sampling, Probability, Correlation, Multivariatedataanalysis, PCA, Time-series analysis and application of Statistical Concepts using SAS and SPSS.
* Hands on experience in implementing Web Services  like RESTAPI.
* ing SQL knowledge and experience ing with relational databases and optimizing the queries with a variety of databases such as PostgreSQL, MySQL, Microsoft SQL Server and Oracle.
* Experience ing with NoSQL databases such as MongoDB.
* Expertise in programming languages like Python, Scala, R, SAS,Java and JavaScript.
* Experience with AWS  like Redshift, S3, EC2, SageMaker&EMR.
* Extensive experience in ing with Tableau 9.0 and 10.0 Desktop along with Tableau Server.
* Excellent Software Development Life Cycle (SDLC) with good ing knowledge of testing methodologies, disciplines, tasks, resources and scheduling.
* Experience in Cloud  such as Google Cloud Platform and SAP
* Extensive ing knowledge in Microsoft Excel for Data Analysis.

Domain
 Used
Machine Learning
Classification,Regression, Clustering, Association, Logistic Regression, DecisionTrees,RandomForest, Nave Bayes, K-NearestNeighbors(K-NN),Kernel SVM.
Big Data
Pig, Hive, Spark, HBase, Kafka, Sqoop, Cassandra, MongoDB.
Databases
Oracle, MySQL, MS SQL Server, Postgres.
Analytics
Python, R, SAS, SPSS, Excel.
Visualization
Tableau and PowerBI
Operating Systems
Windows, Unix/Linux, Mac
Design Methodologies
Agile, Scrum, Waterfall
IDEs
IntelliJ IDEA, Pycharm, Jupyter, Spyder 
			
 EXPERIENCE
Credit Suisse, Raleigh - NC			
Data Scientist				Feb 2019 - Till date
* Developed a Rule based engine that helps the business in identifying the potential conflicts during the business initiation phase using Python.
* ed on configuring Apache Kafka to publish real-time business data to various topics.
* ed on Kafka messaging systems for extracting the user input data that feeds to the Rules engine.
* Created Restful Web Services that integrate with Apache Kafka and web interface using Flask.
* Develop a user interface for the client to display the algorithm results for easy access of data using React JS and Bootstrap.
* Involved in fixing and testing the python functions using unit testing.	
* Improved the ETL flow by building a python script that transfers data from MongoDB to Oracle database.
Environment: Python, Kafka, React, Flask, MongoDB, Oracleand Git.
MetLife, Charlotte - NC				
Data Scientist										June 2018 - Jan 2019
* Involved in tuning the Propensity models predicting the customer behavior to help Marketing and Finance team in analyzing various data sources to promote the growth of insurance products.
* Involved in extracting data from various sources and performed data cleansing, data integration, data transformation, data mapping and loading data into Hadoop with Apache Spark using PySpark and SparkSQL.
* Wrote SparkSQL scripts to Query multiple tables, performed joins and create tables to load data into Hive.
* Applied Text Mining techniques to clean the marketing survey data using NLP techniques such as lemmatization and porter stemmer.
* Wrote Hashing algorithm to secure PII data, and stored the data in the Hadoop.
* ed on SAP Cloud services and ingested data from Oracle and MySQL data bases using Sqoop.
* Prepared analysis reports and dashboards for the stakeholders using Tableau.
Environment: Python, Spark, NLP, Hive, HDFS, SSIS, Oracle, SAP cloud, MySQL, Tableau and Git.
JPMorgan Chase				
Data Scientist									Apr2017-Mar2018
* Provided recommendations by evaluating the customer life value to the business.
* ed on Data cleaning, Data preparation and Feature engineering with Python, including NumPy, SciPy, Matplotlib, Seaborn, Pandas, and Scikit-learn. 
* Hands on experience in Dimensionality Reduction, Model selection and Model boosting methods using Principal Component Analysis (PCA), K-Fold Cross Validation and Gradient Tree Boosting.
* Advanced knowledge of statistical techniques in Sampling, Probability, Multivariatedataanalysis, PCA, and Time-seriesanalysis using SAS.
* Responsible for creating Hive tables, loading the structured data resulted from Map Reduce jobs into the tables and writing hive queries to further analyze the logs to identify issues and behavioral patterns.
* Performing statistical data analysis and data visualization using R and Python.
* Performed analysis of implementing Spark uses and wrote spark sample programs using PySpark. 
* Implemented data refreshes on Tableau Server for biweekly and monthly increments based on business change to ensure that the views and dashboards were displaying the changed data accurately. 
Environment: SQL, Informatica, SAS, Hive, Tableau, Python, GIT, Google cloud platform and Tableau.
Wells Fargo, India					
Data Analyst/Scientist									Feb 2015-Aug 2016
* Involved in evaluating customer credit data and financial statements in order to determine the degree of risk involved in lending money.
* Developed predictive solutions to support commercial banking team using machine learning algorithms such as Linear Regression, Logistic Regression, Naive Bayes, Decision Trees, Random Forest, Support Vector Machine in Python.
* Conducted analysis in assessing customer behaviors with clustering algorithms such as K-Means Clustering and Hierarchical Clustering.
* ed on data cleaning, data preparation and feature engineering with Python, including NumPy, SciPy, Matplotlib, Seaborn, Pandas, and Scikit-learn.
* Implemented Agile Methodologies, Scrum stories and sprints in a Python based environment, along with data analytics and Excel data extracts. 
* Used Hive and created Hive tables and involved in data loading and writing HiveUDFs. 
* Experience designing and optimizing complex SQL queries involving table joins using MySQL.
* ed in Tableau environment to create weekly, monthly, daily reports using tableau desktop & publish them to server.
* ed on importing and exporting data from Oracle into HDFS using Sqoop.
* ed on Excel using VLOOKUP, pivots, conditional formatting, large record sets, data manipulation and cleaning.
* Used GIT as version control software to manage the source code and to keep track of changes to files which is fast and light weight system.
Environment: Python, MySQL, SAS, HDFS, Hive, Excel, Tableau and GIT.
Wells Fargo, India					
Data Analyst										May 2013-Jan 2015 
* ed for Risk management team in identifying the risk involved in the Mortgage process by evaluating the customer and property records.
* Involved in addressing a wide range of challenging problems using techniques from applied statistics, machine learning and data mining fields.
* Experience in descriptive statistics and hypothesis testing using Chi-square, T-test, Pearson correlation and Analysis of variance (ANOVA).
* Advanced knowledge of statistical techniques in Sampling, Probability, Multivariate data analysis, PCA, and Time-series analysis using SAS.
* Analyzed Relational & Non-relational data using MySQL.
* Contributed , Projectmanagement, and Business management functions to push the business forward with innovative solutions.
* Performed data acquisition and exploratory data analysis in R.
* Visualized team metrics and communicated to the higher management using PowerBI.
Environment: R, MySQL, PowerBI and GIT. 
Bankatlal Institute of Information , Hyderabad, India
Data Analyst											Aug 2011-Apr2013
* As part of a project, ed on HTML, CSS and JavaScript to develop a web application for online applications
* Involved with Data Analysis primarily Identifying Data Sets, Source Data, Source Meta Data, Data Definitions and
* Data Formats
* Involved in development of General Ledger module, which streamlined analysis, reporting and recording of
* accounting information
* Managed connectivity using JDBC for querying/inserting and data management including triggers and stored
* procedures
* Involved in maintaining student records in the database using Oracle
* Resolved the data related issues such as: assessing data quality, data consolidation, evaluating existing datasources using MS EXCEL
Environment: HTML, CSS, Oracle, MS Excel, Tableau
ACADEMIC PROJECTS
Big Data Modeling & Management
* As part of course analyzed streaming data in real-time from a weather station and created plots.
* Used Map-Reduce algorithms on Twitter data and Performed words count to study the presidential elections.
Big Data Integration & Processing
* As a final project, ed on Soccer world cup data to study the behavior of fans across the globe with the help of Twitter.
* Using Population census data performed statistical calculations using Splunk and accessed Postgres database tables and manipulated data frames using Spark.
* Involved in the retrieval of NoSQL data using MongoDB and performed data aggregation of Data Frames.
Statistical Thinking for Data Science
* ed with Machine Learning Algorithms: Neural Nets, Deep Learning, Net analysis, Supervised Learning, and Unsupervised Learning.
* Importing data into Python from files native to other software such as Excel spreadsheets, Stata, SAS and MATLAB files from relational databases such as SQLite&PostgreSQL.
* Reshaping and tidying data using techniques such as pivoting and melting.
* Performed Time-series analysis and A/B testing.
* Used Machine Learning components: Classification, Regression, and Clustering.
R Programming for Data Science
* Characterized relationships graphically, in the form of  statistics, and through simple linear regression models.
* Used Regression Trees, Decision Trees, Random Forest, Ensemble Methods and conducted performance metrics analysis to find the best model to forecast currency trend and generate detailed reports in RStudio.
* Usedappropriate libraries to cleandata using(tidyr, dplyr), visualizedata(ggplot2), and finally tune and evaluate models.
Database Management System
* Developed a car rental system using Java and MySQL as a part of Database Design courseproject to perform CRUD operations.
Project and Change Management
* As part of the team project, ed on doctors appointment website using Microsoft project 2013. Created business requirement documents, business use cases and system use cases. Conducted Cost-Benefit analysis, GAP Analysis, SWOT Analysis to validate the compatibility of the system infrastructure with the new business requirements.



",Data Scientist,emailed,indeed job post,resume
"		 

Analyst with proven ability to succeed as part of team or individually, driven to providing actionable machine learning solutions along with excellent presentation .


Fowler College of Business, San Diego State University			                        Aug 2017 - May 2019           
Master of Science in Information Systems 						                             GPA: 3.55
Relevant Courses: Machine Learning, Business Analytics, Big Data, Relational Database Management, Statistics, Spark
Anna University					   	                  	             	           Aug 2006 - May 2010
Bachelor of Engineering in Computer Science						    	                  GPA: 3.1

 
* Programming Languages: Python, R, SQL, Matplotlib, NumPy, Pandas. Database: Sybase, SQL Server
* Big Data: Spark, PySpark, SparkML. Machine Learning Methodology: Supervised and Unsupervised Learning
* Analytical models: Linear Regression, Logistic Regression, PCA, SVM, Classification, Predictive Modeling
* : R Studio, Jupyter Notebook, Putty, AWS Spark, Microsoft Excel, Scikit-Learn, SciPy. 
* Others: Exploratory Data Analysis, Data visualization, NLP, Statistical Analysis, Regression Analysis, Correlation

 EXPERIENCE
Research Assistant (Data Scientist), San Diego State University, San Diego, US	          Feb 2019  May 2019
Churn rate prediction for Telecom Industry

* Cleansed data by applying various techniques like, missing value treatment, outlier treatment, data normalization.
* Identified group of customers who are likely to churn through exploratory analysis. Visualized the findings.
* Found out factors/variables that are responsible for churning by performing dataset slicing and dicing.
* Leveraged class imbalances technique from Python for dataset balancing and improved the accuracy in churn prediction by 50%.
* Increased revenue of the business by $6000 on monthly basis.
* Provided recommendations for customer engagement.  
Environment: Python 3.5, NumPy, Pandas, Matplotlib, Seaborn, Microsoft Excel, Microsoft PowerPoint.

Data Analyst/Software Engineer, HCL , Bangalore, India		            Jul 2010  Sep 2013        
Client: Deutsche Bank/State Street Bank
Project: Loan Default Prediction						          	           

* Coordinated and communicated with other departments in collecting client requirements.
* Conducted exploratory data analysis and discovered patterns on borrowers who are likely to default the loan using NumPy, Pandas and visualized the findings using Seaborn and Matplotlib. 
* Merged different data sets into a single data set and read data from various data sources like CSV, text, excel and json formats.
* Performed pre-processing steps like imputing missing values, removing highly correlated variables and converting categorical variables to dummy variables.
* Employed feature selection method to select best features to prevent the curse of dimensionality.
* Involved in model building process and incorporated cross-validation technique to avoid over-fitting and implemented classification algorithms like Logistic Regression, Decision Trees, Random Forest, Support Vector Machines.
* Validated the machine learning classifiers using AUC ROC curve and the accuracy. The accuracy of the best model is 82.7% using Decision Trees.
* Presented the findings to the  team.

Environment: Python, Pandas, NumPy, Scikit-Learn, Seaborn, PCA, Linear models, Non-linear models, Ensemble models.


Project: Portfolio Investment Banking System				             				             
* Involved in all phases of the SDLC (Software Development Life Cycle) from analysis, design, development, testing, implementation and maintenance with timely delivery against aggressive deadlines.
* Developed stored procedures, written complex SQL queries using multiple table joins, sub-queries with knowledge of optimal software performance techniques and generated reports for business shareholders and customers.
* Migrated code from testing to production environment ensuring accuracy.
* Monitored systems post go live. Proactively delivered solutions for continuous improvement. 
* ed in scrum team setting as a developer with very good understanding of each of the scrum roles.
* As a scrum team player, liaised with product manager and product owner to understand client requirements and helped develop software that exceeds clients expectations.
* Collaborated with team members to eliminate unwanted dependencies of autosys batch jobs.
* Automated reports to end clients using TCL scripts and job scheduler that saved manual efforts of 72 hours/month.
* Improved the performance of stored procedure by using techniques like bulk insertion of data from the source to the database.	
Environment: Sybase, TCL scripting, SQL, python scripting, Putty, UNIX/Linux. 

		                                               
ACADEMIC 
1) Prediction of new user reservation using Airbnb: (4-person project).                                  Jan 2019  May 2019
 used: R, ggplot2, dplyr, PowerPoint, Neural Nets & Support Vector Machine (SVM), Boosting             
* Reduced dimension of the dataset from 10 million to 100,000s by using data wrangling technique.
2) Insights on donation made in 2016 presidential campaign						    Apr 2019
* Discovered valuable insights on total donation made to each campaign by all contributors and to that small contributors using Spark, Amazon Web Services cloud (AWS EMR and AWS S3).
3) Detection of insults in social commentary: (SciPy, Pandas, Nave Bayes, SVM, Logistic Regression)    Jun 2018
* Handled end-to-end machine learning pipeline for sentiment classification.
4) Designed ER model for Bakery Management and implemented database solution using MS SQL.  Mar 2018






",Data Scientist,emailed,indeed job post,resume
"
 
* Around 8 years of hands on experience and comprehensive industry knowledge of Machine Learning, Statistical Modeling, Deep Learning,DataAnalytics,Data Modeling, Data Architecture, Data Analysis, DataMining, Text Mining & Natural Language Processing (NLP), Artificial Intelligence algorithms, Business Intelligence.
* Having good experience in Analytics Models like Decision Trees, Linear & Logistic Regression, Hadoop (Hive, PIG), R, Python, Spark, Scala, MS Excel, SQL and PostgreSQL,Erwin.
* Strong knowledge in all phases of the SDLC (Software Development Life Cycle) from analysis, design, development, testing, implementation and maintenance. 
* Experienced in Data Modeling techniques employing Data Warehousing concepts like star/snowflake schema and Extended Star.
* Expertise in applyingDataMining techniques and optimization techniques in B2B and B2C industries.
* Expertise in writing functional specifications, translating business requirements to  specifications, created/maintained/modified database design document with detailed description of logical entities and physical tables.
* Excellent knowledge of Machine Learning, Mathematical Modeling and Operations Research. Comfortable with R, Python, SAS and Weka, MATLAB, Relational databases. Deep understanding & exposure of BigDataEco-system.
* Expertise inDataAnalysis,DataMigration,Data Profiling, DataCleansing, Transformation, Integration, DataImport, andDataExport through the use of multiple ETL  such as Informatica Power Center.
* Proficient in Machine Learning, Data/Text Mining, Statistical Analysis&Predictive Modeling. 
* Good Knowledge and experience in deep learning algorithms such as Artificial Neural net (ANN), Convolutional Neural Net (CNN) and Recurrent Neural Net (RNN), LSTM and RNN based speech recognition using Tensor Flow.  
* Excellent knowledge and experience in OLTP/OLAP System Study with focus on Oracle Hyperion Suite of , developing Database Schemas like Star schema and Snowflake schema (Fact Tables, Dimension Tables) used in relational, dimensional and multidimensional modeling, physical and logical Data Modeling using Erwin tool.
* Used Cognitive Science in Machine Learning for Neurofeedback training which is essential for intentional control of brain rhythms.
* Experienced in building data models using machine learning techniques for Classification, Regression, Clustering and Associative mining. 
* Good Knowledge on Natural Language Processing (NLP) and Time Series Analysis and Forecasting using ARIMA model in Python and R.
* Enabling rapid insights generation from adverse event Data via cognitive  to increase the translational research capabilities
* ing experience in Hadoop ecosystem and Apache Spark frame such as HDFS, MapReduce,HiveQL, SparkSQL, PySpark.
* Very good experience and knowledge in provisioning virtual clusters under AWS cloud which includes services like EC2, S3, and EMR.
* Proficient in data visualization  such as Tableau, Python Matplotlib, R Shiny to create visually powerful and actionable interactive reports and dashboards.
* Expertise in building, publishing customized interactive reports and dashboards with customized parameters and user-filters using Tableau(9.x/10.x).
* Experienced in Agile methodology and SCRUM process.
* Strong business sense and abilities to communicate data insights to both  and non clients.
* Proficient in Python, experience building, and product ionizing end-to-end systems.
* Strong programming expertise (preferably in Python) and strong in Database SQL.
* Solid coding and engineering  preferably in Machine Learning.
* Exposure to python and python packages.
* Be a valued contributor in shaping the future of our products and services.




Bachelor of Computer Science 

 

Databases
MySQL, PostgreSQL, Oracle, HBase, Amazon Redshift, MS SQL Server 2016/2014/2012/2008 R2/2008, Teradata
Statistical Methods
Hypothetical Testing, ANOVA, Time Series,Confidence Intervals, Bayes Law, PrincipalComponent Analysis (PCA), Dimensionality Reduction, Cross-Validation, Auto-correlation
Machine Learning
Regression analysis, Bayesian Method, Decision Tree, Random Forests, SupportVector Machine, Neural Net, SentimentAnalysis, K-Means Clustering, KNN andEnsemble Method
Hadoop Ecosystem	
Hadoop 2.x, Spark 2.x, MapReduce, Hive, HDFS, Sqoop, Flume
Reporting 
Tableau Suite of  10.x, 9.x, 8.x which includes Desktop, Server and Online, Server Reporting Services(SSRS)
Languages
Python (2.x/3.x), R, SAS, SQL, T-SQL
Operating Systems
PowerShell, UNIX/UNIX Shell Scripting , Linux and Windows
Data Analytics 
Python (numpy, scipy, pandas, Gensim, Keras), R (Caret, Weka, ggplot).
Data Visualization
Tableau, Visualization packages, Matplotlib, Seaborn, ggplot2, Microsoft Office.
R Package
dplyr, sqldf, data table, Random Forest, gbm, caret, elastic net and all sortof Machine Learning Packages.
:


Role: Sr.Data scientist/Machine learning Engineer
Client: Best Buy, Minneapolis, MN  May 2018-Till Date

Description:Best Buy Co., Inc. is an American multinational consumer electronics retailer headquartered in Richfield, Minnesota. It was originally founded by Richard M. Schulze and James Wheeler in 1966 as an audio specialty store called Sound of Music. In 1983, it was re-branded under its current name with an emphasis placed on consumer electronics.Best Buy is the largest specialty retailer in the United States consumer electronics retail industry.

Responsibilities:
* Utilized Spark, Scala, Hadoop, HBase, Kafka, Spark Streaming, MLlib, R a broad variety of machine learning methods including classifications, regressions, dimensionally reduction etc. and Utilized the engine to increase user lifetime by 45% and triple user conversations for target categories. 
* Participated in features engineering such as feature intersection generating, feature normalize and label encoding with Scikit-learnpre-processing.
* Performing statistical analysis on textual data. Building Machine learning/ Deep Learning models in the domain of Natural Language
* Used Python 3.X (numpy, scipy, pandas, scikit-learn, seaborn) and Spark2.0 (PySpark, MLlib) to develop variety of models and algorithms for analytic purposes.
* Application of various Machine Learning algorithms and statistical modeling like decision trees, regression models, neural nets, SVM, clustering to identify Volume using the scikit-learn package in python, Matlab.
* Create and build Dockers images for prototype deep learning models running on local GPU.
* Created Data Quality Scripts using SQL and Hive to validate successful data load and quality of the data. Created various types of data visualizations using Python and Tableau.
* Developed and implemented predictive models using machine learning algorithms such as linear regression, classification, multivariate regression, Naive Bayes, RandomForest, K-meansclustering, KNN, PCA and regularization for Data Analysis.
* Performed Data Collection, Data Cleaning, Data Visualization and developing Machine Learning  
Algorithms by using several packages: Numpy, Pandas, Scikit-learn and Matplotlib.
* Implemented various data pre-processing techniques to manipulate the unstructured, structured data  
and imbalanced data like SMOTE.  
* Clustered customers' actions data by using K-means clustering and Hierarchical clustering, then  
segmented them into different groups for further analyses.
* Built Support Vector Machine algorithms for detecting the fraud and dishonest behaviors of customers  
by using several packages: Scikit-learn, Numpy, Pandas in Python.
* Designed and developed NLP models for sentiment analysis. 
* Led discussions with users to gather business processes requirements and data requirements to develop a variety of Conceptual, Logical and Physical Data Models. Expert in BusinessIntelligence and Data Visualization : Tableau, Microstrategy.
* Developed and evangelized best practices for statistical analysis of Big Data.
* Designed and implemented system architecture for Amazon EC2 based cloud-hosted solution for client. 
* Developed deep learning algorithm that generated hedging strategies providing 15% ROI per month with a standard deviation of 2.7%(results based on testing strategies on real data for 3 months)
* Designed the Enterprise Conceptual, Logical, and Physical Data Model for Bulk Data StorageSystem using Embarcadero ER Studio, the data models were designed in 3NF.
* ed on machine learning on large size data using Spark and MapReduce. 
* Collaborated with data engineers and operation team to implement ETL process, wrote and optimized SQL queries to perform data extraction to fit the analytical requirements.
* Performed data analysis by using Hive to retrieve the data from Hadoop cluster, SQL to retrieve data from RedShift.
* Explored and analyzed the customer specific features by using SparkSQL.
* Performed data imputation using Scikit-learn package in Python.
* Let the implementation of new statistical algorithms and operators on Hadoop and SQL platforms and utilized optimizations techniques, linear regressions, K-means clustering, Native Bayes and other approaches. 
* Knowledge of Information Extraction, NLP algorithms coupled with Deep Learning.
* Developed Spark/Scala,SAS and R programs for regular expression (regex) project in the Hadoop/Hive environment with Linux/Windows for big data resources.
* Conducted analysis on assessing customer consuming behaviours and discover value of customers with RMF analysis; applied customer segmentation with clustering algorithms such as K-MeansClustering and Hierarchical Clustering.
* Implement deep learning algorithms to identify fraudulent transactions
* Built regression models include: Lasso, Ridge, SVRand XGboost to predict Customer Life Time Value.
* Built classification models include: Logistic Regression, SVM, Decision Tree, RandomForest to predict Customer Churn Rate.
* Used F-Score, AUC/ROC, Confusion Matrix, MAE, RMSE to evaluate different Model performance.

Environment: AWS RedShift, EC2, EMR, Hadoop Frame, S3,HDFS, Spark(Pyspark, MLlib, Spark SQL), Python 3.x (Scikit-Learn/Scipy/Numpy/Pandas/Matplotlib/Seaborn),Tableau Desktop (9.x/10.x), Tableau Server (9.x/10.x), Machine Learning (Regressions, KNN, SVM, Decision Tree, Random Forest, XGboost,LightGBM, Collaborative filtering, Ensemble),Deep Learning, Teradata, Git 2.x, Agile/SCRUM

Role: Data scientist/Machine learning Engineer
Client: Johnson and Johnson, Raritan, NJ				Jan 2017  Apr 2018

Description:
Johnson & Johnson is an investment holding company with interests in health care products. It engages in research and development, manufacture and sale of personal care hygienic products, pharmaceuticals and surgical equipment. The company operates through the following business segments.
Responsibilities:
* Tackled highly imbalanced Fraud dataset using undersampling, oversampling with SMOTE and cost sensitive algorithms with Python Scikit-learn.
* Wrote complex Spark SQL queries for data analysis to meet business requirement.
* Developed MapReduce/Spark Python modules for predictive analytics & machine learning in Hadoop on AWS.
* Building Optimization models using Machine Learning, Deep Learning algorithms.
* ed on data cleaning and ensured Data Quality, consistency, integrity using Pandas, Numpy.
* Participated in feature engineering such as feature intersection generating, feature normalize and label encoding with Scikit-learn preprocessing.
* Improved fraud prediction performance by using random forest and gradient boosting for feature selection with Python Scikit-learn.
* Performed feature engineering, performed NLP by using some techniques like Word2Vec, BOW (Bag of Words), Tf-Idf, Word2Vec, Doc2Vec.  
* Performed Nave Bayes, KNN, Logistic Regression, RandomForest, SVMandXGboost to identify whether a loan will default or not.
* Implemented Ensemble of Ridge, Lasso Regression and XGboost to predict the potential loan default loss.
* Used various Metrics (RMSE, MAE, F-Score, ROC and AUC) to evaluate the performance of each model. 
* Performed data cleaning and feature selection using MLlib package in PySpark and ing with deep learning frames.
* Actively involved in all phases of data science project life cycle including Data Extraction, Data Cleaning, Data Visualization and building Models.  
* Experience in ing with languages Python and R.
* Developed text mining models using Tensor Flow&NLP (NLTK, SpaCy and CoreNLP) on call transactions & social media interaction data for existing customer management.  
* Experienced in Agile methodology and SCRUM process. 
* Experience in Extract, Transfer and Load process using ETL  like Data Stage, Data Integrator and SSIS for Data migration and Data Warehousing projects.  
* Experienced in Data Integration Validation and Data Quality controls for ETL process and Data Warehousing using MS Visual Studio, SSAS, SSIS and SSRS.  
* Used big data  Spark(Pyspark, SparkSQL and MLlib) to conduct Realtime analysis of loan default based onAWS.
Environment:MS SQL Server 2014, Teradata, ETL, SSIS, Alteryx, Tableau (Desktop 9.x/Server 9.x), Python3.x(Scikit-Learn/Scipy/Numpy/Pandas), Machine Learning (Nave Bayes, KNN, Regressions, Random Forest, SVM, XGboost, Ensemble), AWS Redshift, Deep Learning, Spark(PySpark, MLlib, Spark SQL), Hadoop 2.x, Map Reduce, HDFS, SharePoint.

Role: Data Scientist
Client: RetailMeNot INC, Austin, TX                                         Nov 2015  Dec2016

Description:RetailMeNot, Inc. is a leading digital savings destination connecting consumers with retailers, restaurants and brands, both online and in-store. The company enables consumers across the globe to find hundreds of thousands of digital offers and discounted gift cards to save money while they shop or dine out.
Responsibilities:
* Gathered, analyzed, documented and translated application requirements into data models and Supports standardization of documentation and the adoption of standards and practices related to data and applications.
* Participated in Data Acquisition with Data Engineer team to extract historical and real-time data by using Sqoop, Pig, Flume, Hive, MapReduce and HDFS.
* Automated csv to chatbot friendly Json transformation by writing NLP scripts to minimize development time by 20%.
* Wrote user defined functions (UDFs) in Hive to manipulate strings, dates and other data.
* Performed Data Cleaning, features scaling, features engineering using pandas and numpy packages in python.
* Applied clustering algorithms i.e. Hierarchical, K-means usingScikit and Scipy. 
* Created logical data model from the conceptual model and it's conversion into the physical database design using ERWIN. 
* Mapped business needs/requirements to subject area model and to logical enterprise model. 
* ed with DBA's to create a best fit physical data model from the logical data model
* Redefined many attributes and relationships in the reverse engineered model and cleansed unwanted tables/ columns as part of data analysis responsibilities. 
* Enforced referential integrity in the OLTPData Model for consistent relationship between tables and efficient database design. 
* Developed the data warehouse model (star schema) for the proposed central model for the project. 
* Created 3NF business area data modeling with de-normalized physical implementation data and information requirements analysis using ERWIN tool.
* ed on the Snow-flaking the Dimensions to remove redundancy. 
* ed in using Teradata14  like Fast Load, Multi Load, T Pump, Fast Export, TeradataParallel Transporter (TPT) and BTEQ.
* Helped in migration and conversion of data from the Sybase database into Oracle database, preparing mapping documents and developing partial SQL scripts as required. 
* Generated ad-hoc SQL queries using joins, database connections and transformation rules to fetch data from legacy Oracle and SQL Server database systems.

Environment: Machine learning(KNN, Clustering, Regressions, Random Forest, SVM,Ensemble), Linux, Python 2.x (Scikit-Learn/Scipy/Numpy/Pandas), R, Tableau (Desktop 8.x/Server 8.x), Hadoop, Map Reduce,HDFS, Hive, Pig, HBase,Sqoop, Flume,Oracle 11g, SQL Server 2012.


Role: BI Developer/Data Analyst
Client: Deutsche Bank, New York City, NYMay 2014  Oct 2015

Description: Deutsche Bank is a leading global investment bank with a strong and profitable private clients franchise. The Project was to implement machine learning techniques and develop statistical models to identify loan default pattern and predict potential default loss for the company.

 Responsibilities:

* Used SSIS to create ETL packages to Validate, Extract, Transform and Load data into Data Warehouse and Data Mart.	
* Maintained and developed complex SQL queries, stored procedures, views, functions and reports that meet customer requirements using Microsoft SQL Server 2008 R2. 
* Created Views and Table-valued Functions, Common Table Expression (CTE), joins, complex subqueries to provide the reporting solutions. 
* Optimized the performance of queries with modification in T-SQL queries, removed the unnecessary columns and redundant data, normalized tables, established joins and created index. 
* Created SSIS packages using Pivot Transformation, Fuzzy Lookup, Derived Columns, ConditionSplit, Aggregate, Execute SQL Task, Data Flow Task and Execute Package Task.
* Migrated data from SAS environment to SQL Server 2008 via SQL Integration Services (SSIS).
* Developed and implemented several types of Financial Reports (Income Statement, Profit& Loss Statement, EBIT, ROIC Reports) by using SSRS. 
* Collaborated with database engineers to implement ETL process, wrote and optimized SQL queries to perform data extraction and merging from SQL server database.
* Created Complex ETL Packages using SSIS to extract data from staging tables to partitioned tables with incremental load.  
* Gathered, analyzed, and translated business requirements, communicated with other departments to collected client business requirements and access available data.  
* Migrating data from Legacy system to SQL Server using SQL Server Integration Services 2012.
* Used C# scripts to map records.  
*  Involved in writing complex SQL Queries, Stored Procedures, Triggers, Views, Cursors, Joins, Constraints, DDL, DML and User Defined Functions to implement the business logic and created clustered and non-clustered indexes.  
* Created and modified Stored Procedures, Functions, and Indexes.  
* Developed SQL Scripts to Insert/Update and Delete data in MS SQL database tables.  
* Created various ad-hoc SQL queries for customer reports, executive management reports and types of report types like tables, matrix, sub reports etc.  
* Designed and developed new reports and maintained existing reports using Microsoft SQLReporting Services (SSRS) and Microsoft Excel to support the firm's strategy and management. 
Created sub-reports, drill down reports,  reports, parameterized reports, and ad-hoc reports using SSRS.
* Used SAS/SQL to pull data out from databases and aggregate to provide detailed reporting based on the user requirements. 
* Used SAS for pre-processing data, SQL queries, Data Analysis, generating reports, Graphics, and Statistical analyses.
* Provided statistical research analyses and Data Modeling support for mortgage product.
* Perform analyses such as regression analysis, logistic regression, discriminant analysis, cluster analysisusing SAS programming.

Environment: SQL Server 2008 R2, DB2,Oracle,SQL Server Management Studio, SAS/ BASE, SAS/SQL, SAS/Enterprise Guide, MS BI Suite(SSIS/SSRS), T-SQL, SharePoint 2010, Visual Studio 2010, Agile/SCRUM

Role: Data Analyst
Client: Exceloid Soft Systems, India  Jan 2013  Apr 2014

Description: By implementing Exceloid's made-for-future technological strategies excellent ing with Exceloid Soft Systems in the initial days of our retail journey in India. I think they are among the best Openbravo specialist we ed with in India.
 Responsibilities:
* Wrote SQL queries for data validation on the backend systems and used various  like TOAD&DBVisualizer for DBMS(Oracle).
* Perform Data analysis, Backend Database testing, Data Modeling and Developing SQL Queries to solve problems and meet user's need for Database management in Data Warehouse.
* Utilize object-oriented languages, concepts, database design, star schemas and databases.
* Create algorithms as needed to manage and implement proposed solutions. 
* Participate in test planning and test execution for functional, system, integration, regression, UAT (User Acceptance Testing), load and performance testing.
*  with test automation  for recording/coding in Database, and execute in regression testing cycles. 
* Transferred data from various OLTP data sources, such as Oracle, MS Access, MS Excel, Flat files, CSV files into SQL Server.
* ing with Databases DB2, Oracle DM, SQL Server for Database testing and maintenance. 
* Involved in writing and executing User Acceptance Testing (UAT) with end users.
* Involved in Post- Implementation validations after the changes have been to the Data Marts. 
* Chart out Graphs, and Reports alike in QC to point out the percentage of Test Cases passed, and thereby to point out the percentage of Quality achieved and uploading the status daily to ART reports an in-house tool. 
* Performed extensiveDataValidation,DataVerification againstDataWarehouse.
* Used UNIX to check the Data marts, Tables and Updates made to the tables. 
* Writing advanced SQL Queries to query the data from Data marts and Landings to verify the changes has been made. 
* Involved in Client requirement gathering, participated in discussion & brain storming sessions and documented requirements. 
* Validating and profilingFlat FileDatainto Teradata tables using UNIX Shell scripts.
* Actively participated Functional, System and User Acceptance testing on all builds and supervised releases to ensure system / functionality integrity. 
* Closely interacted with designers and software developers to understand application functionality and navigational flow and keep them updated about Business user sentiments. 
* Interacted with developers to resolve different Quality Related Issues.
* Wrote and executed manual test cases for functional, GUI, and regression testing of the application to make sure that new enhancements do not break ing features 
* Writing and executing Manual test cases in HP Quality Center.
* Wrote test plans for positive and negative scenarios for GUI and functional testing
* Involved in writing SQL queries and stored procedures using Query Analyzer and matched the results retrieved from the batch log files 
* Created Project Charter documents & Detailed Requirement document and reviewed with Development & other stake holders. 

Environment: Subversion, TortoiseSVN, Jira, Agile-Scrum, Web Services, Mainframe, Oracle, Perl, UNIX, LINUX, Shell Scripts, UML, Quality Center, RequisitePro, SQL, MS Visio, MS Project, Excel, Power Point, Word, SharePoint, Win XP/7 Enterprise.

Role: Data Analyst/Data Modeler
Client: ZEN3 Info Solutions, India                             May 2011  Dec 2012


Description: Zen3 is a leading software solutions group developing innovative solutions for media, travel and  industries.
  Responsibilities:
* Data analysis and reporting using MY SQL, MS Power Point, MS Access and SQL assistant.
* Involved in MY SQL, MS Power Point, MS Access Database design and design new database on Netezza which will have optimized outcome.
* Used DB2 Adapters to integrate between Oracle database and Microsoft SQL database in order to transfer data.
* Designed the data marts using the Ralph Kimball's DimensionalData Mart modeling methodology using ER Studio. 
* Involved in writing T-SQL, ing on SSIS, SSRS, SSAS, Data Cleansing, Data Scrubbing and Data Migration.
* Used Normalization methods up to 3NF and De-normalization techniques for effective performance in OLTP systems. 
* Initiated and conducted JAD sessions inviting various teams to finalize the required data fields and their formats. 
* Involved in designing and implementing the Data Extraction (XML DATA stream) procedures. 
* Created base tables, views, and index. Built a complex Oracle procedure in PL/SQL for extract, loading, transforming the data into the warehouse via DBMSScheduler from the internal data. 
* Involved in writing scripts for loading data to target data Warehouse using BTEQ, Fast Load, MultiLoad.
* Create ETL scripts using Regular Expressions and custom  (Informatica, Pentaho, and Sync Sort) to ETL data. 
* Developed SQLService Broker to flow and sync of data from MS-I to Microsoft's master database management (MDM).  
* Extensively involved in Recovery process for capturing the incremental changes in the source systems for updating in the staging area and data warehouse respectively 
* Strong knowledge of Entity-Relationship concept, Facts and dimensions tables, slowly changing dimensions and Dimensional Modeling (Star Schema and Snow Flake Schema).
* Involved in loading data between Netezza tables using NZSQL utility. 
* ed on Data modeling using Dimensional Data Modeling, Star Schema/Snow Flake schema, and Fact & Dimensional, Physical & Logical data modeling. 
* Generated Stats pack/AWR reports from Oracle database and analyzed the reports for Oracle8.x wait events, time consuming SQL queries, table space growth, and database growth.

Environment: ER Studio, MY SQL, MS Power Point, MS Access, MY SQL, MS Power Point, MS Access, Netezza, DB2, T-SQL, DTS, Informatica MDM, SSIS, SSRS, SSAS, ETL, MDM, 3NF and De-normalization, Teradata, Oracle8.x, (Star Schema and Snow Flake Schema) etc.








",Data Scientist,emailed,indeed job post,resume
"Willing to relocate to: Los Angeles, CA - Laughlin, NV - Fort Mohave, AZ
 Experience

Consultant Business Intelligence and Pricing Analyst
Panasonic - Lake Forest, CA
September 2018 to July 2019
 Developed and led the initiative for the monthly executive report, which provided data-driveninsights to support decision-makers by utilizing Python libraries and MS Power BI for data visualization.
 
 Collaborated with cross-functional teams to understand the data challenges of the organization anddevelop solutions, which increased ancillary revenue by approximately 20%.
Consultant Data and Post Market Analyst
Johnson & Johnson - Orange County, California, US
July 2017 to September 2018
 Analyzed large dataset related to post-market complaints and implementing statistical models todiscover potential anomalies to ensure compliance with all applicable corporate and federal agencies using MS SQL, Excel, and Python. 
 Improved operational efficiency, which reduced costs by automating 40% of ad-hoc activities,reporting, and data visualization by utilizing Tableau and SSRS.
Business Insight Analyst
ATV Inc., Los Angeles County - Los Angeles, CA December 2016 to June 2017
 Performed thorough research and analysis on market pricing to determine the best target price forproducts and services using Excel, Python, SQL, and other proprietary . 
 Automated ETL process by 80% using Python scripts and SQL Server stored procedures.
Customer Project / Program Manager III
Hewlett Packard Enterprise - Hong Kong, HK
July 2012 to June 2015
 ed with one of the largest conglomerates in Australia and New Zealand to ensure thatorganizational s are achieved. 
 Managed project scope, schedule, resources, and costs using the most appropriate verificationtechniques. 
Supv, ITO Svc Delivery Consultant II 
 Managed a team of ETL analyst and database administrators using appropriate  and techniqueswhich resulted to consistently exceeding operational performance goals, and quality expectations. 
 Delivered SIP (Service Improvement Plans) to advance the quality of service and service deliveryeffectiveness by identifying opportunities from a large dataset and develop data-driven strategies.
Programmer Analyst III
Amkor  Inc - Manila
December 2006 to June 2012
 Used Oracle PL/SQL, VB.NET, and Webmethods to design and develop ETL solutions. 
 Pioneered in Customer Focus -SME Team, who performed data analysis, disaster recovery,and problem resolution for high priority incident. Reduced ticket resolution time to 15%. 
 Conducted and facilitated  training about the implementation and maintenance of newlydeveloped ETL application across the Global team.


Master of Business Administration in Business Statistics
Westcliff University - Irvine, CA January 2018 to Present
Graduate Certificate in Data Science and Predictive Analytics
University of California Irvine - Irvine, CA September 2016
Bachelor of Science in Computer Application
De La Salle University March 2006


Database, Db2, Mysql, Oracle, Sql, Elasticsearch, Informatica, Power bi, Teradata, .net, C/c++, C+
+, Html, Python, Visual basic, Visual basic 6, Xml, Xslt, Cognos, Ssrs, Business Intelligence, Excel, Microsoft Office, Powerpoint, access, testing, Visio, MS Office, SAP
Links


Certifications/Licenses

ITIL v3",Data Scientist,emailed,indeed job post,resume
"Self-motivated, harding  with strong creative and strategic problem-solving capabilities, communication , analytical  and strong commitment to detail.  
Effective resource planning, project planning, decision making, results delivery, and team building .  
Effectively manage stress and communicate openly in high-pressure situations.  
Flexible, fast-learning and multitask capable with skillful attention to detail and accuracy.
Authorized to  in the US for any employer
 Experience

Data Analyst/Data Manager
CDG, A Boeing Company - Rancho Cucamonga, CA July 2018 to Present
 We provide support to Boeing engineers and suppliers in need of data governance assistancethrough CSDT, the web-based company-wide data warehouse. 
 Update CSDT, REDARS release and ENOVIA with supplier drawings and documents, verifyingmetadata in system matches real-time documentation through CSDT and PDM, as provided 
 Provide updates and verification on important dates and users to certify that all data is documentedaccurately and in a timely fashion, as needed.  
 Solely developed and implemented the Data Management SharePoint site for the EngineeringServices group in a matter of weeks.
 Specialist/Scientist 2
Southern California Edison - Pomona, CA October 2016 to November 2017
 Analysis, data corrections, retriggering assets to field , correcting maintenance plans/orders -mainly related to data discrepancies prohibiting inspectors from being able to complete orders  Create missing measurement points, create maintenance plans, generate E6 notifications, upload inspections w/o creating E1 notifications or upload against removed equipment, set DLFL NOCO on E6 notifications with removed equipment, location corrections, and research uploading errors for resolution 
 Create orders at floc level for assets that did not get inspected due to CMS issue. 
 Client support related to data associated to all inspection programs. Application support related todata discrepancies (CMS, EMobile, DM, AUD, GESW, FIM, SAP). 
 Query/correct flocs assigned to Superior Floc ""OH"" and ""UG"" 
 Object type changes, correct maintenance plans (ODI and Intrusive) and assist with creatingmaintenance plans. 
 Compare SAP location group table to FIM location group extract. Make determination for new Grids/FIMs and decommission removed Grids/FIMs no longer on FIM. May need to move assets from one group to another and create new ODI/AGP MPs. 
 Key element of the team that built model for equipment failure to increase reliability.  Utilized ArcGIS in project to predict risk of failure of equipment, as well as analyzing data discrepancies. 
 Experience using AutoCad in business environment.
Analyst-Business 2
Southern California Edison - Pomona, CA
October 2015 to October 2016
 Key team member in development, analysis, implementation, and reporting on reliability basedmaintenance and compliance pilot where I was tasked with providing analysis and strategic recommendations to peers and senior organizational leaders. 
 Key analyst in developing new and innovative statistical analytics and modeling for various high-  for implementation across T&D business lines. 
 Developed and implemented statistical and data analytical presentation for chief executive officers. 
 Responsible for analyzing data and metrics that were submitted to the CPUC. 
 Significant team member in root cause analytical strategy involving high-visibility reliability metricsin which I was responsible for presenting our findings to departmental executives in a comprehensive and coherent manner. 
 Responsible for creating and maintaining ODI Obstruction and ODI Access compliance-relateddatabases, ensuring that all Maintenance inspections are completed in a timely and efficient manner.  Identified opportunity for process improvement with Scheduled Outage report and used that as an occasion to develop fellow team member in understanding and implementation of the report.  Effectively integrate  across business lines to develop and execute business plans, manage information, and provide exceptional customer service to my clients. 
 Responsible for managing the  under the scope of the  Management team by creatingand maintaining multiple data sources that help manage the  of the District and Contractor related .
Analyst-Program/Project 2
Southern California Edison - Pomona, CA November 2014 to October 2015
 Developed a new monthly Utilization report using Telogis, SAP, Microsoft Access, and Microsoft Excel.
 
 Developed two new reports that help track the number of ODI Access and ODI Obstructionsinspections created since May 2014 using SAP, BEx Analyzer, Microsoft Access and Microsoft Excel. 
 Key associate in the development of the new 2015 Metrics book, including automation and design in
Microsoft Excel and Adobe PDF. 
 Designed and created the new graphical interface in the 2015 Metrics book. 
 Instrumental in the development of a high-level pilot program that will potentially help the companyreduce risk, including sifting through large data sets from SAP and BEx, running queries, formulas and automation in Microsoft Access and Microsoft Excel. 
 Continuously share data processes and report processing with peers by creating job aids andautomating the reports prior to handing them off. 
 Integral part of Data Validation team where I build queries in Microsoft Access to analyze large datasets pulled from SAP and BEx Analyzer, taken the lead on some milestones for the team, reviewed and updated peer's , and identified trends in data discrepancies. 
 Provided support to the newly developed Metrics database by trouble shooting issues, validatingcode and functionality, and identifying gaps in data flow to Metrics template.
Analyst-Program/Project 1
Southern California Edison - Pomona, CA
January 2014 to November 2014
- SW 
 Using BEx Analyzer, SAP & Excel, I create Ad-Hoc Reports out of confidential data that is thenprovided to the Law & Claims Department in a timely manner. 
 Generate and create reports in Excel that are then arranged in PowerPoint to provide information forthe Circuit Reliability Project, used in several areas of SCE management. 
 I routinely track and prioritize Record Corrections in SAP. I analyze the data, apply inspections,research the Maintenance Plan, research mapping in FIM and eWorld prior to making recommendations as to further course of action. 
 Research, develop and test job aids to assist co-ers in completing  in the most effectivemanner and making recommendations of ways to be more concise, effective and efficient. Utilize spreadsheets, databases, and material in Telogis, SAP, Excel, and Access. 
 Develop queries and maintain databases in Access for the E1P1 Report and Supervisor Field Report.  Provide effective support to co-ers and peers with little or no supervision, freeing PM&A management and staff to focus attention on other necessary  within the organization. 
 Experience reviewing, updating and preparing reports such as the E1P1 report, using Access, Excel,SAP and BEx Analyzer. 
 Analyze data in SAP in order to resolve problems and situations that arise in the field while ingon notifications for PM&A. 
 Review and assess  orders in SAP/BI Query to provide accurate data to ODI Supervisors for DailyPending Report. 
 Simultaneously  on notifications, claims, data requests, run reports and assist other analystswithin the PM&A  group. 
 Demonstrated the ability to effectively integrate  from the ODI Daily Pending report that I run toprocessing E2 notifications to gathering data for Claims. 
 Communicate clearly with different  groups to validate the data input into SAP and mapping.  Take initiative to effectively increase productivity by creating automations for the  that I  on using VBA. 
 Provide assistance without hesitation to other personnel when needed. 
 Complete ODI and UDI inspections, resulting in accurate data entered into database to aid PM&A intracking and managing process. 
 Analyze and track data for processing E2 notifications, Claims processing, ODI/AGP Daily Pending
Reports and E1P1 Report using SAP, Excel and Access. 
 Experience creating and maintaining documents and files for use in monitoring, tracking andtrending costs, budget variance and resources for T&D Dashboard for PM&A management using Excel formulas and functions. 
 Experience monitoring, tracking and trending resources, schedules and status for ODI Supervisorsusing Excel, SAP and BI Query. 
 Simultaneously create ODI Daily Pending Report, review T&D Dashboard, process E2 notifications,create Ad-Hoc reports for the Law & Claims Department, create PowerPoints for the System Reliability group and support various analysts within the PM&A group. 
 Create Ad-Hoc reports for the Law & Claims Department and provide daily compliance reports for theODI supervisors. 
 Support PM&A analysts by reviewing and updating job aids using Excel, VBA, Telogis, Access, Wordand other online resources. 
 Provide daily, bi-weekly and quarterly reports to assist T&D management in various groups. 
 Gather documents and research data using databases, files, spreadsheets and mapping for Claimsprocessing, notifications, E1P1 Report, field supervisor report, Fleet Frequency of Use report, ODI Daily
Pending Report and Circuit Reliability Report. 
 Perform analysis, data entry and record preparation for E2 notifications, claims and F.O.Ps. 
 Experience researching and reconciling online records using Telogis, eWorld and FIM for Fleet
Frequency of Use report, processing E2 notifications and processing Claims. 
 Experience using Word, Excel, and PowerPoint to create job aids and various reports, includingClaims, Circuit Reliability, etc. 
 Understanding of OMS readings that assist in the compilation of the data for the Circuit Reliabilityproject. 
 Experience ing with Transmission Resource Planning and Cost Efficiency database. 
 Ability to interface effectively and collaborate with peers while ing on multiple . 
 Routinely exceed deadlines and expectations set for  and reports. 
 Experience ing with T&D policies, procedures, and practices. 
 Experience following Edison safety protocols and safe  practices. 
 Demonstrated strong ethics, personal mastery and interpersonal .
Education

Master of Science degree in Statistics
Cal Poly Pomona 2018
Bachelor of Arts degree in Mathematics
CSU Fresno 2013
Sierra High School 2007


Analyzer (4 years), Excel. (4 years), Microsoft Access (4 years), SAP (4 years), Arcgis (1 year), Autocad (Less than 1 year), Apple, Esri Arcgis, Excel, SQL, Business Intelligence, access
Additional Information

Computer Application : 
 Microsoft Excel 
 Microsoft Access 
 Microsoft PowerPoint 
 Microsoft Word 
 Microsoft Outlook 
 Microsoft Visio 
 SnagIt 
 SAP 
 VBA 
 Lotus Notes 
 BEx Analyzer (Business Analyzer-BI) 
 Telogis 
 Facilities Inventory Mapping (FIM) 
 eWorld 
 SQL 
 OMS 
 CSS 
 SAS 
 R 
 Minitab 
 SPSS",Data Scientist,emailed,indeed job post,resume
"
Remote Statistician and Data Scientist Bremerton, WA

I have my Master of Applied Statistics and am seeking remote employment as a statistician or data scientist. Through my schooling I have experience in multiple statistical methods including Linear Regression, Multiple Linear Regression, Analysis of Variance, Multiple Analysis of Variance, Experimental Design. I am trained and experienced in Minitab, SAS, and R programming, and I have the ability to learn any statistical analysis software required for .  
I am a former math educator at the public secondary level and collegiate level. My experience in  helps me communicate statistical results in terms that are understandable to people from all fields of expertise. I also have extensive experience with creating and giving presentations and communicating with my management and customers. I am confident that I am a valuable asset to any data analysis team.
Authorized to  in the US for any employer
 Experience

Math Tutor
Leslie's Tutoring
October 2016 to Present
 Support students in secondary level schools in academic and organizational areas 
 Help students increase self-confidence regarding their ability to succeed 
 Designed, Administered, and Interpreted a statistical analysis comparing perceptions of the successof tutoring at the center
Adjunct Faculty
NORTHWEST COLLEGE OF ART & DESIGN - Tacoma, WA
May 2017 to August 2018
Instructed QP 351, Quantitative Reasoning, a core math requirement course for students pursuing a Bachelor of Fine Arts. 
 Designed a comprehensive curriculum that focused on students' using collegiate level mathematics to improve decision making and knowledge in daily life.
Administrative Assistant
Peninsula Electric Corp. - Poulsbo, WA September 2016 to August 2018
 Completed weekly payroll for all employees 
 Invoiced customers, received payments, entered inventory using Quickbooks software 
 Designed and administered monthly safety training program
Math Teacher
Sangaree Middle School - Ladson, SC
July 2015 to January 2016
 Taught 7th Grade and 7th Grade Accelerated Math 
 Created weekly teaching plans within peer teacher groups 
 Analyzed student data to adjust instruction
Math Teacher
Emmett School District - Emmett, ID
January 2012 to June 2015
 Taught 7th Grade, 7th Grade Accelerated, 8th Grade, 8th Grade Geometry at the Middle School 
 Taught Algebra 1 and Geometry at the High School 
 Actively taught using AVID techniques in conjunction with Common Core math curriculum 
 Member of the Emmett Middle School AVID CommitteeCore Subject Teacher 
 Aligned Emmett Math curriculum with Common Core standards in a district curriculum initiative 
 Created vertical alignment with curricular expectations for all middle grades through Algebra 1  ed on creating unified final exams and unit tests to create consistency in al expectations  
 Organized the end of year Awards Assembly for Emmett Middle School 2012-2015 
 Founded and Coached a Middle School Academic Team


Master's in Applied Statistics
Pennsylvania State University-Main Campus - State College, PA
January 2017 to May 2019
Bachelor's in Mathematics Secondary 
Boise State University - Boise, ID August 2008 to December 2011


SAS (2 years), R (1 year), Microsoft Office, Organizational , Communications, Powerpoint,
Teaching, Filing, problem solving, Typing, Data Presentation, Excel, MS Office
Certifications/Licenses

Driver's License
Assessments

High School Classroom Management  Highly Proficient
August 2019
Minimizing classroom disruption and engaging students.
Full results: https://share.indeedassessments.com/share_assignment/drsvobncaf5wgvlx
Logic & Verbal Reasoning  Expert
August 2019
Understanding the meaning of text, and identifying the relationships among words or concepts.
Full results: https://share.indeedassessments.com/share_assignment/55w3hznjs8o4bkuz
Verbal Communication  Expert
August 2019
Speaking clearly, correctly, and concisely.
Full results: https://share.indeedassessments.com/share_assignment/enouluzkbhlhrkim
Problem Solving  Expert
August 2019
Analyzing relevant information when solving problems.
Full results: https://share.indeedassessments.com/share_assignment/l9qhy5pgvj74e5rp
Critical Thinking  Expert
August 2019
Using logic to solve problems.
Full results: https://share.indeedassessments.com/share_assignment/vsei0-w54o3-av0f
Data Analysis  Expert
August 2019
Interpreting and producing graphs, identifying trends, and drawing justifiable conclusions from data.
Full results: https://share.indeedassessments.com/share_assignment/vfnyqiubwki52n80
 Support  Highly Proficient
August 2019
Applying protocols to identify errors and solutions in order to maintain system function.
Full results: https://share.indeedassessments.com/share_assignment/tegnzuaok0tc5hug
Research  Expert
August 2019
Following protocols, interpreting statistics and graphs, identifying errors, and choosing research methodology.
Full results: https://share.indeedassessments.com/share_assignment/d3iqhuhhdd-dwzyp
Management & Leadership : Planning & Execution  Expert
August 2019
Planning and managing resources to accomplish organizational goals.
Full results: https://share.indeedassessments.com/share_assignment/n-7zuzc-tuibibet
Indeed Assessments provides  tests that are not indicative of a license or certification, or continued development in any  field.",Data Scientist,emailed,indeed job post,resume
"To continue learning and innovating the world in science, medicine, and humanities as a developing human for the benefit of the world. 
The opportunity to gain meaningful experience is always possible with the right environment. As I explore career paths and creative ways of learning, I find meaningful  in seeing mission statements fulfilled and visions being executed.
 Experience

Activities Director
HUMANITARIAN EFFORTS - San Diego, CA
August 2017 to Present
San Diego, CA, USA; Los Angeles, CA, USA: San Jose, Costa Rica; Azores, Portugal 
 
 Nazarene Youth International: Annual District Youth Camp - Activities Director 
 Loves Program:  and Community - Portugal Missions Trip 
Achievement 
 City Height's Health Disparities Clinic: Medical Volunteer
RESEARCH LABORATORY MANAGER
Abilities - Loma Linda, CA
June 2018 to June 2019
91232 
Laboratory: in Vivo surgical procedures, IHC, Cryosectioning, Advanced Microscopy Imaging 
 Project Manager for NIH R21 Grant, Two MD-PHD graduate  
 Neuroscience Research in Vagal nerve stimulation, Traumatic Brain Injury, Hypoxia 
Research : SPSS, Data Analysis, Prism, 
 Created research culture to market healthy place environment and productivity 
Python, PubMed, NCBI 
 Collaborated with Biochemistry, Pediatrics, Neurosurgery departments with other 
Health Equity Mentor Program
UCSD School of Medicine
May 2018 to May 2018
Science  Mentor
PLNU - San Diego, CA
May 2018 to May 2018
San Diego, USA 
 Ocean Discovery Institute: Discovery Fellowship Program - Science  Mentor 
The Best Oral Presentation - May 2018 
WCBSUR Conference, California, USA
STUDENT MANAGER
place Culture - San Diego, CA
August 2014 to May 2018
71622 Social Media incorporation, Emotional Intelligence, 
Leadership 
 Oversaw GDP, purchasing logistics, organization and marketing of Dining Program 
 Marketed and Developed new dining programs for student well-being 
Office Application: Microsoft Office, Grant 
 Collaborative integration of campus coffee shop, caf, and dining hall Budgets, Inventory, and Project Management


CERTIFICATE
Chaffey College - San Diego, CA August 2019
B.S Honors in Scholar
Point Loma Nazarene University
August 2014 to May 2018
Links",Data Scientist,emailed,indeed job post,resume
"? Highly productive and effective Financial Analyst with over four years of experience ing in thefinancial field. 
? Exceptional command of language and communication , both written and oral. 
? Unique ability to give full attention to people, utilize effective listening  and ask appropriatequestions. 
? Strong analytical and reasoning  and ability to identify the strengths and weaknesses ofalternative solutions. 
? Extensive experience in developing constructive, cooperative,  ing relationshipswith others and maintaining them over time.
Authorized to  in the US for any employer
 Experience

QUANTITATIVE ANALYST
Quantopian - Boston, MA
July 2016 to Present
Building low latency liquidity taking or market making strategies from end-to-end. 
? Developing mathematical models to solve difficult stochastic problems. 
? Analyzing convergence and boundedness properties of algorithms and estimates. 
? Translating your models to fast computational methods. 
? Collaborating with researchers and developers to implement all of the above.
SR. FINANCIAL ANALYST
Houghton Mifflin - Boston, MA December 2014 to June 2016
Performed complex accounting functions for business units 
? Coordinated, prepare and review month-end closing process, accounting entries and documents ? Developed annual budgets - coordinated the development, implementation and control of budgets by preparing, balancing, compiling, and entering budget data 
? Created quarterly forecasts 
? Performed statistical analysis of revenue and expense versus budgets 
? Monitored and evaluated budgets and forecast 
? Analyzed financial results to reconcile variances and develop qualitative explanations, and identifypotential risks to IT plans. 
? Interacted with vendors and support deal negotiations using statistical regression analysis forpredictability 
? Liaised with Legal and HMH executives in efforts to help achieve goals and mission of firm.
FINANCE ASSOCIATE
Bain Capital - Boston, MA
November 2014 to December 2014
Two month contracted position. 
? ed with the Financial Analyst team to transition smoothly from Great Plains to day, andwrote VBA scripts and macros to further aid the team in minimizing the time of completion. 
? Performed Balance Sheet and Income statement validation and variance analysis and provided auditsupport.
FX & SECURITIES TRADER
FINRA SERIES - Boston, MA
July 2014 to November 2014
Executed purchases and sales of investment products in a timely and efficient manner. 
? Explained trading procedures to reps and other customers, answers questions and provides quotes. 
? Researched using Statistical software equity price movements for future price action. 
? Ensured that all trades meet internal and external compliance standards. Checks to verify properlicensing of outside reps. 
? Developed proprietary application MSITEQ to analyze market volatility.
FINANCIAL ACCOUNTING REPORTING Specialist
BNY Mellon - Everett, MA
December 2012 to July 2014
Supported complete, accurate, and timely valuation of financial statements for daily and monthly values. Utilized VBA to produce accurate and timely Net Asset Values and month-end close consolidation. 
? Supported a mix of large complex institutional client relationships requiring the understanding of alldaily, weekly and monthly standard and customized client accounting and reporting requirements. ? Frequent/daily contact with entry/mid-level client staff with respect to cash, accounting, and reporting of daily and monthly client activities and initiatives. 
? Answered all client processing, accounting and reporting questions, either directly or throughresearch with other functional areas, which required a solid understand of all functional groups that support CARS. ? Provided  support to clients and interpreted business performance drivers and opportunities for enhancements.


Bachelor of Science in Economics
Salem State University 2012


ACCESS (Less than 1 year), EXCEL (Less than 1 year), FINANCIAL MODELING (Less than 1 year),
HYPERION (Less than 1 year), MS EXCEL (Less than 1 year), Python, C, R
Additional Information

?  : MS Excel (Pivot Tables, Macros, VBA script, If and Analysis, (V, H)LOOKUP), Access and PowerPoint, 
Financial Modeling SAP R3, SAP BW, BPC, Hyperion, SAP,",Data Scientist,emailed,indeed job post,resume
"Business Data Analyst who maximizes productivity, profitability, and competitiveness by transforming  plans into powerful business solutions. Distinguished for both  and business acumen, and for an ability to bridge communication gaps between , business, and executive audiences. Earned undergraduate degree in petroleum engineering and master degree in business data analytics. Expertise in Data Modeling, Data Mining/manipulation, SQL, python, R language, Data Visualization, and Data Warehousing with experience in handling multiple  and meeting tight deadlines.
 Experience

Data Analyst
Dude Solutions Inc - Grand Junction, CO
September 2018 to Present
 Streamline the reporting process through tuning SQL and SAS scripts and automate the dailyreporting process 
 Handle data manipulation and data cleaning on all utility data for successful database population.  Provide assistance to implementation teams with initial account configuration by utilizing, creating, and maintaining import templates, utility data migration tool sets, and data loads. Actively participate, when needed, in client facing calls to set proper expectations and establish needs regarding client data. 
 Optimize production databases by ing with implementation teams to address data problems.  Perform pre-and-post job inspections, consistently complete preventative maintenance procedures, and maintain support equipment as well as rig up and down service line equipment at the well site.  Utilize expertise in transforming all 3rd party utility data received from client in a consumable format for the purposes of importing into Dude Solutions Software (Energy Manager) from existing SQL databases. 
 Demonstrate strong problem-solving, critical thinking, and organizational . 
 Efficiently communicate the entire process successes and failures to internal and externalstakeholders to identify potential areas of business improvement, which require cross-departmental collaboration. 
 Document and monitor large utility data sets for producing business intelligence (BI) solutions thatidentify relationships and trends in utility data. Help team to prioritize  according to business value. 
 Perform database administration duties: backups, restoration, performance monitoring, and tuning. 
 Provide data solutions such as business partner data pulls, report design, and data visualizations.  Leverage advanced statistical and data analytic techniques to support/implement new data efforts by analyzing and producing comprehensive reports that provide competitive and actionable information.  Use optimization  at a quickening pace for preventative maintenance forecasting, integrated business planning, business partner collaboration, and risk analytics. Ensure the merging of large utility data sets. 
 Manage project through status meetings, weekly reports, identifying risks, and tracking issues 
 Served customers and businesses by running and viewing reports 
 Designed analytical solutions 
 Execute and validate test cases 
 Responsible for specifications, implementations, and analytics 
 Prepared business models, flowcharts, and diagrams 
 Efficiently collaborate with team members to develop and recommend new business intelligenceservices.
Maintenance Assistant
Halliburton - Fort Lupton, CO
July 2017 to October 2017
 Performed basic safety/repair procedures using processes defined for assigned jobs. Strictlymonitored preventive/predictive maintenance procedures on the Halliburton and other pumping equipment. 
 Completed preventative maintenance procedures, performed pre-and-post job inspections, andmaintained support equipment as well as rig up and down service line equipment at the well site.  Under general supervision, inspected, maintained, diagnosed, troubleshot, and repaired a wide variety of high-pressure pumping equipment, which included: the Halliburton HT-150, HT-400, HT-2000, Grizzly (HQ-2000), and Bearcat as well as a variety of other manufacturers pumping equipment. 
 Managed the process for maintenance inspections, repairs, and tests required by the company andcustomers. Performed basic troubleshooting on some mechanical, hydraulic, and pneumatic systems.
Information  Tech
University of Wyoming - Laramie, WY
July 2014 to May 2017
 Provided  assistance and support for incoming queries and issues related to computersystems, software, and hardware. Deeply experienced in responding to queries either in person or over the phone. 
 Diagnosed computer errors and provided  support. Trained end-users on how to setup anduse new . Performed basic administration of Windows OS and iOS - fixing any related issues. 
 Troubleshot software, net issues, and hardware i.e. desktops, laptops, printers, and otherperipherals. Continuously backed-up and restored the organization's data files and electronic systems.
 
 Installed, configured, and upgraded PC software and operating systems. Performed setup ofenterprise LAN and WAN. Deeply experienced in installing and using anti-spyware and anti-virus software.


Master's in Science in Business Data Analytics
Maryville University - Saint Louis, MI December 2018
Bachelor of Science in Petroleum Engineering
University of Wyoming - Laramie, WY May 2017


Database, Relational database, Sql, Power bi, Saas, Tableau, Ios, Python, Vba, Data warehouse, Matlab, Linux, Sap, Power BI, Excel, Microsoft office, Powerpoint, Word, MS Office
Additional Information

Business Data Analyst who maximizes productivity, profitability, and competitiveness by transforming  plans into powerful business solutions. Distinguished for both  and business acumen, and for an ability to bridge communication gaps between , business, and executive audiences. Earned undergraduate degree in petroleum engineering and master degree in business data analytics. Expertise in Data Modeling, Data Mining/manipulation, SQL, python, R language, Data Visualization, and Data Warehousing with experience in handling multiple  and meeting tight deadlines. 
 
 Predictive/Exploratory Model Design 
 Advanced Graph Database and SQL  
 Excellent Communication  
 Expert in Data Mining, Query Optimization, Scripting, and ing with Large Datasets 
 Data Analytical Mindset 
 Problem-Solving  
 Advanced Data Warehousing  
 Advanced Visualization  (Tableau and Microsoft Power BI) 
 
 Exceptional Interpersonal and Communication  - Proficient in maintaining long-term businessrelationships while successfully interfacing with a wide range of people from diverse backgrounds.  Problem Solving - Proven ability to troubleshoot and develop both creative and innovative solutions to project challenges. Successfully manages change for improved performance and greater team efficiency. 
  Ethic and ism - Solid  standards and excellent record of dependability.
Maintains a clear focus on achieving bottom-line results and team integrity while ensuring progress. 
 
  
 
Advanced Microsoft Power BI Skill Advanced R language Skill 
Advanced Python Language Skill Advanced SQL Database Skill 
Advanced Graph Database Skill (Neo4j Software) Advanced Azure Server Skill 
Advanced Experience in SQL Query and Data Warehouse Advanced Tableau skill 
Advanced SAP IS - Oil and Gas (Downstream) Skill Expert VBA Excel Skill 
Proficiency in relational database  and  
Advanced  in Petrel E&P Software and MATLAB 
Experienced ing with several operating systems (iOS, Windows, and Linux) 
Experience with SaaS or analytics software 
Advanced  in Microsoft Office applications (Word, Excel, PowerPoint, Project, etc.)",Data Scientist,emailed,indeed job post,resume
"  with around 7 years of experience in all phases of diverse  specializing in Data Science, Big Data, Azure Machine Learning, Google Cloud and Tableau, using Cloud based infrastructure. 
 ed on analyzing large datasets on distributed databases and developing Machine Learningalgorithms to gain operational insights and present them to the leadership. 
 Extensively ed on Data preparation, exploratory analysis, Feature engineering using supervisedand unsupervised modeling. 
 Experienced the full software life cycle in SDLC, Agile and Scrum methodologies. 
 Expert in using of statistical  and programming languages (R, Python, Java, SQL, UNIX) 
 Adapted statistical programming languages like R and Python. 
 Well-versed with Linear/non-linear, regression and classification modeling predictive algorithms. 
 Actively involved in model selection, statistical analysis using SAS and Gretl statistical tool. 
 Created dashboards as part of Data Visualization using Tableau. 
 Proficiency in using Spark for Bigdata processing in the Hadoop/DataProc/ EMRE ecosystem.  Performed preliminary data analysis using descriptive statistics and handled anomalies such as removing duplicates and imputing missing values using Talend tool. 
 Performed Dimensionality reduction using principal component analysis, auto encoders, and t-SNE.  Validate the consolidated data and develop the model that best fits the data. Interpret data from multiple sources, consolidate it, and perform data cleansing using R/Python/Spark.  Performed multiple Data Mining techniques and derive new insights from the data. 
 Creative problem-solver with strong analytical, leadership, and communication . 
 Proficient in Python, R, Scala, Java, SQL, and C. 
 Experienced in Machine Learning, Data mining with large datasets of Structured and Unstructured
Data, Data Acquisition, Data Validation, and Predictive Modeling. 
 Experience in building Machine Learning, Sequential Modeling, Natural Language Processing (NLP). 
 Use of Analytical : Bayesian Analysis, Inference, Time-Series Analysis, Regression Analysis,
Linear models, Multivariate analysis, Sampling methods, Forecasting, Segmentation, Clustering,
Sentiment Analysis, Part of Speech Tagging, and Predictive Analytics. 
 Experienced in formulating and solving discrete and continuous optimization problems. 
 Able to research statistical machine learning, supervised learning, and classification methods.  Strong mathematical and statistical modeling and computer programming  in an innovative manner. 
 Use of Various Analytics : Classification and Regression Trees (CART), Support Vector Machine
(SVM), Random Forest, Gradient Boosting Machine (GBM), Principal Component Analysis (PCA),
Regression, Nave Bayes, Support Vector Machines. 
 Experienced in AWS cloud computing, Spark, and capable of ing with large datasets. 
 Delivered presentations and highly  reports; collaboration with stakeholders and cross-functional teams, advisement on how to leverage analytical insights 
 Development of clear analytical reports which directly address strategic goals. 
 Identified and learn applicable new techniques independently as needed. 
 Able to  comfortably and effectively within an interdisciplinary research environment. 
 Experienced with validation of machine learning ensemble classifiers. 
 Utilized the online datasets to implement machine learning models using Spark ML for buildingprototypes.
Willing to relocate: Anywhere
Sponsorship required to  in the US
 Experience

Data scientist
Walmart - Jersey City, NJ
September 2017 to September 2017
Responsibilities: 
* Identified relevant data sources and sets to mine for client's business needs from large structuredand unstructured datasets and variables. 
* Identify and integrate new datasets from various data sources including Oracle, DB2, SQLServer,AWS, Azure by employing languages such as SPARK, HDFS, Hive and PIG Latin by ing closely with the data engineering team to strategize and execute the development of models 
* Performed data cleaning including transforming variables and dealing with missing value andensured data quality, consistency, integrity using Pandas, NumPy. 
* ed on Multiple datasets containing two billion values which are structured and unstructured dataabout web applications usage and online customer surveys. 
* Performed Data cleaning process applied Backward - Forward filling methods on dataset for handlingmissing values. 
* ed on different data formats such as JSON, XML and performed machine learning algorithms inPython. 
* Design built and deployed a set of python modelling APIs for customer analytics, which integratemultiple machine learning techniques for various user behavior prediction and support multiple marketing segmentation programs. 
* Supported client by developing Machine Learning Algorithms on Big Data using PySpark to analyzetransaction fraud, Cluster Analysis etc. 
* Used classification techniques including Random Forest and Logistic Regression to quantify thelikelihood of each user referring. 
* Designed and implemented end-to-end systems for Data Analytics and Automation, integratingcustom, visualization  using R, Tableau, and Power BI. 
* Implemented a Python-based distributed random forest via PySpark and MLlib. 
* Performed data wrangling methods using R programming and data visualizations using Tableau. 
* ed on Amazon Web Services (AWS) cloud services to do machine learning on big data. 
* Experience ing with Big Data  such as Hadoop - HDFS and MapReduce, Hive QL, Sqoop, Pig
Latin and Apache Spark (PySpark) 
* Collaborating with the project managers and business owners to understand their organizationalprocesses and help design the necessary reports. 
 
Environment: Python, R, SQL, HADOOP (HDFS) Horton s, AWS, SPARK, Hive, Tableau, MySQL
Data Scientist
Anthem or United Health Groups - Atlanta, GA
June 2015 to November 2016
Responsibilities: 
 Translated business questions into research s, design and conduct analyses, developfindings and synthesize recommendations to deliver valuable, relevant, and actionable insights.  Strong track record of contributing to successful end-to-end analytic solutions (clarifying business s and hypotheses, communicating project deliverables and timelines, and informing action based on findings) 
 Used Pandas, NumPy, Scikit-Learn in Python for performing exploratory analysis and developingvarious machine learning models such Random forest. 
 The missing data in the dataset is handled using Imputer method in SkLearn library. 
 Performed categorical variable analysis using python Label Encoder, fit transform, One Hot Encodermethods in sklearn library. 
 Responsible for design and development of advanced R/Python programs to prepare to transformand harmonize data sets in preparation for modeling. 
 Defined a generic classification function, which takes a model as input and determines the Accuracyand Cross-Validation scores. 
 Advanced SQL ability to efficiently  with very large datasets. Ability to deal with non-standardmachine learning datasets. 
 Built forecasting models in Python using Gradient Boost Regression Trees. Forecasted the revenuefor future 
 ed with applied statistics and applied mathematics  for performance optimization. 
 ed with K-Means clustering and Hierarchical clustering algorithm to do segmentation of stores.  Collected various store attributes and added them into our segmentation model in order to better classify different segments using clustering algorithms. 
 Used cross-validation to test the models with different batches of data to optimize the models andprevent over fitting. 
 Analyzed the SQL scripts and designed the solution to implement using PySpark and developedscripts as per the requirement. 
 ed with Tableau in order to represent the data in visual format and better describe the problemwith solutions. 
 
Environments: Python, PyCharm, Jupyter, Notebook, Spyder, R, Tableau, MySQL
Data Analyst
Discover - Brookfield, WI
October 2013 to June 2015
Responsibilities: 
 Designed, implemented and automated modeling and analysis procedures on existing andexperimentally created data. 
 Increased pace & confidence of learning algorithm by combining state of the art statistical methods. 
 provided expertise and assistance in integrating advanced analytics into ongoing business processes
 
 Parsed data, producing concise conclusions from raw data in a clean, well-structured and easilymaintainable format. 
 Collaborated with data engineers and operation team to implement the ETL process, wrote andoptimized SQL queries to perform data extraction to fit the analytical requirements. 
 Performed univariate and multivariate analysis on the data to identify any underlying pattern in thedata and associations between the variables. 
 ed on feature generating, PCA, feature normalization and label encoding with Scikit-learnpreprocessing. 
 Used Python (NumPy, SciPy, pandas, Scikit-learn, seaborn) and R to develop a variety of models andalgorithms for analytic purposes. 
 Analyzed the partitioned and bucketed data and compute various metrics for reporting 
 Used SQL alongside a variety or reporting  - BusinessObjects, Power BI, Tableau - to developoperational and visual reports for KPI monitoring. 
 Data analysis and visualization using (Python, R) 
 Collaborate with UI engineers, project managers, and designers to develop web portal thataggregates reports from various sources and . 
 Focused on front end features, browser manipulation, and cross-browser compatibility. 
 Used Agile Scrum for BI  across different clients, which allowed for production prototyping,rapid deployment and transparency. 
 
Environment: Tableau, SQL, Java, HTML, Oracle, Agile, Hadoop.
Data Analyst
VEDA Solutions PVT LTD - Hyderabad, Telangana
September 2012 to October 2013
Description: Built a new team and managed in designing cost effective A/B tests to determine high performance marketing campaigns and contributed to increase in sales by 20% and reduced the promotional cost by 35%. 
 
Responsibilities: 
 ed on various phases of data mining, data collection, data cleaning, developing models,validation, and visualization. 
 Captured Modelling requirements from Senior Stakeholders to fetch functional requirements for SAS/R, Python. 
 Performed Statistical Analysis and Hypothesis Testing in Excel by using Data Analysis Tool. 
 Integrated data from disparate sources, mined large data set to identify patterns using predictiveanalysis 
 Conducted intermediate and advanced statistical analysis, such as linear regression, ANOVA, time-series analysis, classification models, and forecasting future sales. 
 Applied concepts of probability distribution and statistical inference on the given dataset to unearthinteresting findings using comparison, T-test, F-test, R-squared, P-value etc. 
 Used various libraries and developed various matching learning algorithms using Pandas, NumPy,
Seaborn, Scipy, matplotlib, Scikit-learn in python. 
 Identified patterns, data quality issues, and opportunities and leveraged insights by communicatingopportunities with business partners. 
 Designed Predictive analysis algorithms using Historical Data. 
 Utilized machine learning algorithms as well as implemented algorithms such as Decision Tree, linearregression, multivariate regression, Naive Bayes, Random Forests, K-means, & KNN. 
 ed on Python modules for machine learning and predictive analytics. 
 ed on Reporting tool to Test, Validate Data Integrity of Reports. 
 
Environment: Python, IPython, Scikit-Learn, MySQL, SQL, NoSQL, Data Modelling, Data Warehouse, Hadoop (MapReduce, HBase, Hive), Gradient Boost, Random Forest, Neural Nets, Sklearn etc.
Education

Bachelor's


PYTHON (4 years), SQL (4 years), Hadoop (2 years), HADOOP (2 years), MACHINE LEARNING (2 years)
Additional Information

 : 
Programming languages Python, Java, R. C 
Data  and Frames s Hadoop (Apache Spark, Pig, Hive, MapReduce), D3.js, Tableau, MATLAB,
R-Studio 
Data Science 
Machine Learning, Deep Learning, Natural Language Processing, Computer Vision, Neural Nets, Machine Perception, Keras, TensFlow, PyTorch 
 
Operating systems Windows, Linux, Mac 
Databases MySQL, MS SQL Server, NoSQL 
Cloud Technologies AWS, Azure. 
Languages Python 3.7, SQL, R 3.6, Java, JavaScript, PL/SQL",Data Scientist,emailed,indeed job post,resume
"Motivated, team-oriented and responsible Data Scientist with 2 years of experience in analyzing market data by using machine learning techniques. Strong knowledge and experience in Predictive analytics, Machine Learning, Deep Learning, Data Visualization and Statistical Modeling.
 Experience

Data Scientist
Akuna Capital - Chicago, IL
January 2018 to Present
Responsibilities: 
 Optimized the existing trading strategies and participated in developing market prediction andtrading strategies using machine learning algorithms such as Linear regression, K-Nearest Neighbors
(KNN), Decision trees, Boosting and Recurrent Neural Nets (RNN). 
 Optimized the existing portfolio optimizer with historical data to improve the performance ofportfolios and asset selection. Designed the new portfolio optimizer with optimization machine learning algorithms for the improvement of portfolio construction. 
 Ranked and selected candidate features by using Principal Component Analysis (PCA). 
 Participated in developing quantitative models using statistical and machine learning algorithms todescribe market behavior. 
 Developed trade signal based on prediction model to guide the trade direction. 
 Evaluated the executed trading strategies based on the trade results for the future optimization.  ed with analytics and trader team to further improve and refine quantitative . 
 Acquisition of new data sets, wrangling and maintenance of new and existing data sets.


Master of Statistics in Statistics
North Carolina State University - Raleigh, NC May 2016 to June 2018
Bachelor of Science in Mathematics in Mathematics
Zhejiang University - Hangzhou, CN
September 2011 to August 2014


Decision trees, Linear regression, Logistic regression, Machine learning, Neural nets, Random forests, Svm, Support vector machines, Natural language processing, Nlp, Natural, Python, Keras, Matplotlib, Numpy, Pandas, Tensorflow, Ms access, Ms sql server, Sql server, Excel, SQL, Business Intelligence, testing, access
Additional Information

  
Programming Languages: Python, R, SAS, SQL 
Packages: Pandas, NumPy, Scikit-learn, SciPy, Seaborn, Matplotlib, NLTK, TensorFlow, Keras 
Databases: MS SQL Server, MySQL, MS Access 
Machine Learning : Linear Regression, Logistic Regression, Decision Trees, Naive Bayes, KNearest Neighbors, Random Forests, Support Vector Machines (SVM), Convolutional Neural Nets
(CNN), Recurrent Neural Nets (RNN), Generative Adversarial Net (GAN), 
XGBoost, Natural Language Processing (NLP)",Data Scientist,emailed,indeed job post,resume
"   
: 	 	 	 	 	 	 	 	 	 	 	 
 A highly motivated, detail-oriented and performance-driven  with relevant years of industrial experience in Python development. Seeking full time opportunity in an organization where I can effectively utilize my Analytical, Machine Learning, Software Development, Problem solving and NLP  relevant in moving the organization forward. 
:   
      University of Illinois at Chicago, Chicago, IL  	 	 	 	 	            	                                   May 2019 Master of Science (MS)  Computer Science (Teaching Assistant) 	 	 	 	 	              GPA - 3.44/4 
Computer Algorithms, Machine Learning, Data Mining, Neural Nets, NLP, Computer Vision, Recommendation Systems 
      SRM University, India   	 	 	 	 	 	 	 	  	                   June 2013 
	Bachelors  Computer Science   	              	 	 	 	 	                             GPA: 8.42/10 
Data Structures, Analysis of Algorithms, Database, Software Engineering, Web Programming, Java 
 :                                                                                                  
Programming Languages: Python (Numpy, Pandas, NLTK, OpenCV, Matplotlib, Scikit-learn, TensorFlow), Matlab, C++, R 
Database : SQL, PostgreSQL 
Other : Django, Flask, REST, Git, Hadoop, Tableau, AWS, ETL, Snowflake 
Data Science: Predictive Modeling, Linear/Logistics regression, Bagging, NLP  and corpora (NLTK, Stanford CoreNLP), Random forest, Bayesian Learning, SVM, PCA, Clustering, Neural Nets (CNN, RNN, LSTM) Operating Systems: Linux, Unix, Mac OS X, Windows 
CORE :          ? Machine Learning ? ETL and Snowflake Datawarehouse ? Neural Nets/Deep Learning ? Statistical Analysis ? Data Structures & Algorithms ? Data Mining & Text Mining 
* Natural Language Processing (NLP) 	? Database 
 DATA SCIENCE ACADEMIC RESEARCH EXPERIENCE:   	 	 	 	 	 	 AUG 2017-May 2019 ? Sentiment Analysis of Twitter Corpus - Python, Scikit-learn, NLTK, NLP, Pandas, Numpy, TensorFlow  	  o Cleaned, structured, analysed the data and generated features followed by application of ML and DL models such as Nave Bayes, SVM, K-nearest neighbours, Decision trees, Random Forest, Logistic Regression, PCA, CNN & LSTM 
o Build a classifier to determine the sentiment of the corpus  means positivity and negativity with 84% accuracy 
* Housing Sale Price prediction based on 79 initial features Python, NLTK, Scikit-learn, pandas, Numpy, TensorFlow o Cleaned and pre-processed the raw data, performed statistical analysis and applied feature engineering o Predicted Sale Prices to minimize the Cost function using Machine learning models & Neural Nets o Explored Linear regression with regularization methods, Random Forest and Deep Learning models 
* Designed Automatic License Plate Recognition Using Computer Vision and Neural Net o It takes the image of a vehicle as the input and outputs the characters written on its license plate 
o Utilised various Image processing algorithms like Connected Components, Contour detection, thresholding, 
       Filtering techniques, Edge detection and detection of boundary box ? Question/answering system in NLP, employing only text using category database o Found the category of each question using Named Entity Recognition and other rules o Created Parse tree and dependency tree of questions to understand and generalise the pattern o Develop a SQL Query to query relevant answer from the corresponding category database 
* Designed a neural net in Python without using pre-defined libraries with 400 hidden neurons for digit classification using the backpropagation algorithm on MNIST data set and achieved accuracy of more than 95% INDUSTRIAL EXPERIENCE:  
CCC Information Services, Chicago, IL 	 	 	              	 	                                   Jun 2018-Aug 2018 Data Science Intern - Python (OpenCV, Pandas, Numpy, scikit-learn), Linux, Machine Learning, Neural Nets (CNN) ? Automated data collection, pre-processing and cleaning for Neural net application 
* Used Image processing to automate the panel segmentation process for any car image by outlining the panels 
	Quadeye Securities  Financial Firm, India  	 	 	 	 	 	 	 Aug 2016Aug 2017 
Software Developer - Python, JSON, REST, Linux, UNIX, SQL, GIT, Django, Pandas, Numpy, AWS 
* Automated the monitoring process and error tracking process across 45 servers for a high frequency trading company 
	Maplelabs by Xoriant Solutions, India   	 	 	 	 	                                    Jul 2015Jun 2016 
Software Engineer - Python, Django, JSON, REST, SQL, Linux, Unix, GIT, ETL 
* Proficiently developed neting application to automate the installation and upgradation of software images and configuration files on switches, connected in different topologies 
	Wipro , India  	 	 	 	 	 	 	 	 	    Dec 2013Jul 2015 
Project Engineer  International Clients  Python, Greenplum, SQL, C, Linux ? Provided support and handled issues using logs of Greenplum DB by Pivotal ",Data Scientist,emailed,indeed job post,resume
"I am an ambitious individual who is excited to start a career in an analyst position. I excel in statistical programming, including data analysis, data visualization, and predictive modeling using R and RStudio. I am excited to put these  to use, help a company with their data needs, and to pursue a career in the statistics field.
Willing to relocate to: Ontario, CA - Lake Elsinore, CA - Irvine, CA
Authorized to  in the US for any employer
 Experience

Notetaker for Disability Services
Disability Services - Riverside, CA January 2017 to Present
 Acted as a liaison between the students and the professors ensuring the students had what theyneeded to be successful 
 Attended select classes for individuals with disabilities in order to take notes on lectures 
 Kept confidentiality between myself and the students to provide normalcy in their 
Teacher's Assistant/Grader
California Baptist University - Riverside, CA January 2018 to May 2018
 Graded home and quizzes and input the scores to the grading system so students could keeptrack of their grades 
 Frequently discussed with the professor about assignment deadlines and rubrics
Hardlines Team Member
Target - Corona, CA
July 2016 to November 2016
 Assisted customers with finding the correct merchandise 
 Collaborated with a team to effectively (re)stock and organize the products resulting in more sales 
 Accelerated the checkout process by helping at the registers and providing cashier services


Bachelor of Science in Applied Statistical Analysis in Actuarial Science
California Baptist University - Riverside, CA
September 2015 to May 2019


Microsoft Office (7 years), Python (Less than 1 year), R and R Studio (3 years), Latex (1 year), Excel, Powerpoint
Links


Additional Information

Some of my strengths are: 
Hard-ing  
Discipline  
Leader 
Team player  
Organization  
Attention to Detail 
Data Visualization 
Data Analytics  
Quick learner  
Predictive modeling 
Good Communicator 
Data Interpretation",Data Scientist,emailed,indeed job post,resume
" Experience

Research Analyst
North East Independent School District - San Antonio, TX
August 2019 to Present
 Review external research requests 
 Digitize data reports using Tableau-dashboards/stories 
 Create surveys using Qualtrics
Junior Data Analyst (Contract Position-Remote)
Pactera  International Limited
June 2019 to August 2019
 Utilized the tool Address Point Judge to analyze large quantities of address data. 
 Verified accuracy while demonstrating the ability to understand, follow, and apply software developerguidelines.
Statistical Consultant
Sportavida LLC - San Antonio, TX
February 2019 to March 2019
 Prepared descriptive statistics and graphics for a season of college football data using SASprogramming. 
 Prepared regression analyses showing relationships between measures such as a fatigue biomarkerand biomarkers indicative of general stress (cortisol levels) and specific tissue stress (muscle, softtissue, cardiac).
Graduate Teaching Assistant I
University of Texas at San Antonio-Department of Data Management and Statistics - San Antonio, TX January 2016 to December 2017
 Held office hours for undergraduate students each week. 
 Maintained email correspondence with professors and undergraduate students on a daily basis. 
 Graded home/ and exams and submitted grades in Blackboard. 
 Proctored exams and aided professors in the department with exams when needed. 
 Helped fellow teaching assistants with grading and inputting grades into Blackboard. 
 Assisted undergraduate students with questions regarding statistics home and .
Data Maintenance - San Antonio, TX
August 2014 to September 2015
Oblate Missions-Missionary Association of Mary Immaculate, San Antonio, TX 
? Retrieved donor account numbers for pledges by searching for and matching donor names andaddresses. 
? Maintained a database of donor accounts by inputting, removing, and revising requests fromreturned donation pledges. 
? Corrected mailing addresses and created mailing labels in Microsoft Word. 
? Balanced pledges using StudioEnterprise software and generated basic excel spreadsheets in orderto keep track of data entry errors.
Office Clerk
Denton, Navarro, Rocha, Bernal, Hyde & Zech, P.C - San Antonio, TX
April 2014 to July 2014
Recorded reimbursable office expenses using QuickBooks. 
? Resolved error issues with Sage Timeslips and QuickBooks by troubleshooting with support. 
? Scanned, copied, and emailed legal documents to secretaries and the office administrator. ? Determined postage for outgoing legal mail and delivered it to the post office at the end of each day.


Master of Science in Applied Statistics in Applied Statistics
University of Texas at San Antonio - San Antonio, TX
August 2018
Bachelor of Arts in Mathematics in Mathematics
St. Mary's University - San Antonio, TX
August 2014",Data Scientist,emailed,indeed job post,resume
"Data Mining Analyst and SAP application engineer in a mid-size e-Commerce company more than 11 years, one year analysis and prediction experience with Python, A resourceful engineer with strong understanding of machine learning approaches and big data ecosystems. Proactive in learning new concepts and apply in , highly self-motivated, able to  well with a team as well as independently.
Willing to relocate: Anywhere
Authorized to  in the US for any employer
 Experience

Data Mining Analyst
Newegg Inc. - Industry, CA
September 2018 to Present
1) Monitor operating metrics 
2) Explore and analyze the business data sets 
3) Development and optimize predictive models, 
4) Build visualization to illustrate trends 
5) Customer Segmentation for market promotions and A/B test 
6) With SMOTE to process the imbalanced data for PO anti-Fraud
SAP Application Engineer
Newegg Inc. - Industry, CA
November 2007 to August 2018
1) Configure SAP ERP/CRM system, maintain SAP server, troubleshoot SAP issue 
2) Handle change request and user role in FI/CO,MM/SD and HR module, support for end-user 
3) Monitor and optimize SAP production system performance, enhance system security 
4) Research SAP HAHA and S/4 application, deploy testing environment 
5) Upgrade SAP hardware and software component 
6) Implement data interface between SAP and external system 
7) Data volume management, archive transaction data 
8) Support and compliant with SOX ITGC audit activities 
9) SAP R/3 system backup/restore, system copy and client copy 
10) SAP Concur integrate with ECC


Bachelor's in Information and Computing Science
Xiangtan University - Xiangtan, Hunan
September 1999 to June 2003
Bachelor's


SAP (10+ years), Data Mining, machine learning (1 year), Python, Java R, SQL, Tableau, Scikit-Learn, Hadoop, Spark (1 year)
Certifications/Licenses

SAP Certified  Associate - System Administration(Oracle DB) with SAP NetWeaver 7.0 EhP2
May 2014 to Present
LPIC-1
October 2015
Linux System Administration",Data Scientist,emailed,indeed job post,resume
"Willing to relocate: Anywhere
Authorized to  in the US for any employer
 Experience

Data Scientist
BESHTON SOFTWARE INC - Santa Clara, CA
May 2019 to Present
1. Cardiovascular Disease(CVD) Prediction 
Implemented data cleaning and standardization for datasets consisting of 70,000 records of patients data in 12 features, including medical history and patients behavior. 
Performed exploratory data analysis(EDA) to detect outliers, and presented to non-experts 
Build binary classifier with Python to predict presence or absence of cardiovascular disease, achieving 73% accuracy 
  
2. Document Classification 
Build multinomial classifier with Python to classify legal documents (with 41 classes). 
Extracted features from documents with cutting edge method such as LDA, TF-IDF and Word2vec. Trained the extracted data with various models including Random Forest and Softmax, achieving 65% accuracy. 
         
3. Rating Behavior Prediction 
Implemented data extraction and cleaning from Guokr using Python- Over 100,000 questions, answers and rating results. 
Extracted features for text mining and performed different models with TensorFlow for users rating prediction. 
Built algorithm for multilayer random-grouping and decision-making strategy using Python
Kaggle Competition: House Prices Prediction
Kaggle
January 2019 to February 2019
Used Python to standardize and clean datasets with 79 explanatory variables describing every aspect of residential homes. Used MXNET to build a multilayer perceptron model to predict house prices, achieving 12% RMSE 
Kaggle Competition: Leaf Classification with Neural Net
Kaggle
October 2017 to December 2017
Used TensorFlow to build a Multinomial Logistic Regression with a single layer for leaf classification and extend the case to a multilayer Convolutional Neural Net.  
Used Python to prepare and clean data for machine learning, implement Random Forest and K-Nearest Neighbors algorithm on the datasets and compare these models. 


Master of Science in Information Systems
Claremont Graduate University - Claremont, CA
June 2018
Master of Science in Mathematics
Claremont Graduate University - Claremont, CA December 2015
Bachelor of Science in Financial Management
Southwestern University of Finance and Economics
June 2013


Matlab (2 years), Python (2 years), Sql (2 years), access, Excel, Business Intelligence, MS Office, testing
Additional Information

Machine learning: TensorFlow, MXNET, NLP, AWS, Google Cloud,  
Programming: Python, R, MySQL, Tableau 
Mathematics: Statistics, Time Series, AB Testing, ",Data Scientist,emailed,indeed job post,resume
" 
Experienced data scientist currently co-founding a startup building a platform, including full lifecycle of data science: data management, data mining, model training, predicting, implementing and reusing. Skilled in predictive analytics and prescriptive analytics using Python, SQL, and Spark Pipeline.  
 
Languages:   	 
Python, R, SQL 
Data/Modeling : 
Pandas, Numpy, Scipy, NLTK, SparkML, Scikit-learn, Keras, Regular Expression (RE)  
Algorithms:   	 
Classification, Regression, Clustering, Neural Nets, Time series 
Visualization:  	 
Tableau, Gephi, Matplotlib, Seaborn, Plotly 
Other:  	 	 
AWS (EC2, RDS, Beanstalk, Pipeline), Git, T-test, Experimental Design 
   
 
EXPERIENCE 

Data Scientist & Co-Founder| Eiffo Analytics LLC | Jersey City, NJ 	        Dec 2018  Present 
Providing data science as a service, empowering all level  with automatic data solution, helping data scientist implement and manage predictive models. Won the third place in Viva  2019. https://www.eiffo-analytics.com/ 
 Designing and building automated end-to-end data pipelines including processing, modeling and deploying, etc. 
 Improving data reliability, efficiency and quality by data type recognition and data enrichment. 
 Implementing data science code on Django frame. Constructing automatic deploying pipeline on AWS. 
 Reducing model training time from 136s to 10s using parallelized computing and Spark.   
 Building a reinforcement learning mechanism to continuously improving performance and optimizing machine learning. 
 Leading 3 data scientists ing on pipelines for classification, regression, fraud detection and time series.  
 
Data Scientist Intern | Linchpin Inc. | New York, NY 	    Sep 2018 - Dec 2018 
 Contributed to the risk management system for insurance by brainstorming potential risk factors for restaurants.  
 Identified 10 new data points; collected data and enriched the databases by 200% using APIs with python. 
 Improved user experience by a fuzzy search tool; generated applications by querying data using SQL with Python.  
 
Research Assistant | Stevens Institute of Tech | Hoboken, NJ 	             May 2018 - Dec 2018 
 Participated in ongoing research to predict frauds based on 10-k financial reports using NLP and neural nets.  
 Parsed and analyzed over 100,000 text files using natural language processing (NLP) and RE.  
 Trained bassline classification models (Nave Bayes, SVM, CNN) using python with Scikit-learn and Keras.  
 Constructed a nested word vector, after data cleaning, for the usage of RNN and attention model.   
Graduate Assistant | Stevens Institute of Tech | Hoboken, NJ 	Jan 2018  May 2019 
 ed as a graduate teaching assistant for 2 professors in 3 semesters; instructed and helped over 200 students.  
 Developed a case study <Advertising Analytics - Click Prediction> with Spark pipelines using SparkML and SparkSql.  
 Coached on Statistics course teaching experimental design, A/B test, and hypothetical test.    
 
 

 
KKBox's Churn Prediction 	 	 	 	 	 	 	 	 	 	 	  Fall 2018  
 
 Analyzed customer behavior and predicted subscription churn, which will reduce 90% risk of losing existing customers.  
 Built a ML Pipeline for Xgboost classifier and increased performance by 8% by hyperparameters tuning. 
 
Fraud Detection on IEEE Membership  	 	 	 	 	 	 	               Spring 2018 
 
 Selected by IEEE in partnership with Stevens to build a fraud detection system alerting them on membership abuse. 
 Parsed 1T log files (Unstructured data) in combined with metadata (Json) to determine patterns and abnormal behaviors.  
 Created a dashboard using Tableau to visualize and analyze the abnormal pattern; presented to help decision making.  
 
Site Selection of Chase Bank Branch 	 	 	 	 	 	 	 	 	 Fall 2017 
 Suggested on site selection strategy based on 12 key metrics using machine learning models with Python. 
 Collected and constructed the dataset from scratch; built 7 classification models with highest accuracy of 90%.  
 
 

Stevens Institute of  |Hoboken, USA | GPA:4.0/4.0                                            	  May 2019 
Master of Science in Business Intelligence & Analytics (Data Science) | Outstanding Academic Achievement Award  Relevant Course: Machine Learning, Data Mining, Big Data, Cognitive Computing, Deep Learning, Databases  
 
Hubei University of Economics | Wuhan, China | GPA:3.69/4.0  	 	 	 	 	          June 2016 
Bachelors in Financial Management | Award: First Prize Scholarship (top 1%) ",Data Scientist,emailed,indeed job post,resume
"Around 8 Years of experience and comprehensive industry knowledge of Machine Learning, Statistical
Modeling, Data Analytics, DataModeling, Data Architecture, Data Analysis, Data Mining, Text Mining &
Natural Language Processing (NLP), Artificial Intelligence algorithms, Business Intelligence, Analytics
Models (like Decision Trees, Linear & Logistic Regression, Hadoop (Hive, PIG), R, Python, Java, Spark,
Scala, MS Excel, SQL and Postgre SQL, Erwin. 
Having a good experience in Big Data, Hadoop, No SQL database (MongoDb, HBase), Data
Warehousing, Business Intelligence, Data Analytics & ETL concepts. 
Excellent knowledge of Machine Learning, Mathematical Modeling and Operations Research.
Comfortable with R, Python, Java, SAS and Weka, MATLAB, Relational databases. Deep understanding
& exposure of Big Data Eco-system. 
Expert in creating PL/SQL Schema objects like Packages, Procedures, Functions, Subprograms,
Triggers, Views, Materialized Views, Indexes, Constraints, Sequences, Exception Handling, Dynamic
SQL/Cursors, Native Compilation, Collection Types, Record Type, Object Type using SQL Developer. 
Have hands on experience in Hadoop, Hive, Hbase, Map Reduce, Pig, Oozie, R, Sqoop, Flume,
Zookeeper, Ambari, YARN, Tezand SAP Hana. 
Strong experience and knowledge in Data Visualization with Tableau creating: Line and scatter plots,
Bar Charts, Histograms, Pie chart, Dot charts, Box plots, Time series, Error Bars, Multiple Charts types, Multiple Axes, subplots etc. 
Strong experience in Big Data  like Spark, SparkSQL, PySpark, Hadoop, HDFS, Hive. 
Proficient at building robust Machine Learning, Deep Learning models, Convolution Neural Nets
(CNN), Recurrent Neural Nets (RNN), LSTM using Tensor Flow and Keras. 
Adept in analyzing large datasets using Apache Spark, PySpark, Spark ML and Amazon Web Services (AWS). 
Highly skilled in using Hadoop (pig and Hive) for basic analysis and extraction of data in the infrastructure to provide data summarization and Proficient in HiveQL, SparkSQL, PySpark. 
In depth knowledge in using of spark machine learning library MLlib. 
Hands on experience integrating R with Hadoop ecosystem using rhdfs, hiver, rhbase packages. 
Hands on experience in SAP Hana, HDFS and R integration. 
Hands on experience in making Web application in R using shiny package. 
Hands on experience in PHP, Python, MySql, PostgreSQL, and MongoDB. 
Knowledge on Scala, Spark andJaql. 
Having experience in Data Quality Management to get, clean, process, and cross-verify the data in multiple sources. 
Domain Knowledge on E-Commerce, E-Learning, Travel, Health Care and Gaming. 
I am an Active Team Player, Quick Learner, Planned and Committed Personality. 
Experience in Amazon - EMR, EC2 and S3 cloud services. 
Involved in installing/configuring Hadoop 1.0 and its Eco system  in CentOS6.x. ed with the admin team in upgrading Hadoop 1.0 to 2.0 using Apache Ambari 2.0.1 and configured with HUE. 
ed up to 20 nodes, with dedicated nodes for namenode, Jobtracker, Secondary node. 
Handled a data load up to 20 TB. 
Extracted data from log files into HDFS using Flume. 
Developed Oozie flow for scheduling and orchestrating the ETL process. 
Extracted data from SAP Hana, MsSql and MySqlintoHDFS using Sqoop. 
Experienced in Teradata RDBMS using Fast load, Fast Export, Multi load, T pump and Teradata SQL
Assistance and BTEQ Teradata utilities. 
Expertise R user with knowledge of statistical programming languagesSAS. 
Created and ed on Sqoop (version 1.4.3) jobs with incremental load to populate Hive External tables. 
Developed Hive (version 0.10) scripts for end user / analyst requirements to perform ad hoc analysis. 
Very good understanding of Partitions, Bucketing concepts in Hive and designed both Managed and
External tables in Hive to optimize performance. 
Solved performance issues in Hive and Pig scripts with understanding of Joins, Group and aggregation and how does it translate to MapReduce jobs. 
Used Tez execution to speed up the query execution time in Hive. 
Good experience with both MapReduce 1 (Job Tracker) and MapReduce 2 (YARN) setups. 
Good experience in monitoring and managing cluster using Ambari through Nagios and Ganglia. 
Experience with ing in Agile/SCRUM software environments. 
Highly motivated team player with excellent interpersonal , effective communication, analytical and presentation .
 Experience

Sr. Data Scientist/Machine Learning Engineer
Allianz life - Minneapolis, MN August 2018 to Present
Description:Allianz Life Insurance Company of North America (Allianz) is a leading provider of retirement solutions, including fixed and variable annuities and life insurance for individuals. Guarantees are backed by the financial strength and claims-paying ability of the issuing company. Variable annuity guarantees do not apply to the performance of the variable subaccounts, which will fluctuate with market conditions. 
Responsibilities: 
? Massively involved in Data Architect role to review business requirement and compose source totarget data mapping documents. 
? Expertise and experience in domains like Retail Solutions, Finance, Healthcare, Banking, Digitaladvertisement and e-commerce. 
? Responsible for the dataarchitecture design delivery, data model development, review, approval and
Data warehouse implementation. 
? Set strategy and oversee design for significant data modelling , such as Enterprise Logical
Models, Conformed Dimensions, and Enterprise Hierarchy. 
? Analyzed existing Conceptual and Physicaldatamodels and altered them using Erwin to supportenhancements. 
? Applied feature engineering according to feature importance by Random Forest and correlationamong features by L1 and L2 algorithms. 
? Designed the LogicalDataModel using Erwin with the entities and attributes for each subject areas. 
? Architectural Design in BigData, Hadoop  and provide for a designer that is an idea-driven. 
? Skilled in Data chunking, Data profiling, Data Cleansing, Data mapping, creating flows and Data
Validation using data integration  like Informatica during the ETL and ELT processes. 
? Used Python 3.X (numpy, scipy, pandas, scikit-learn, seaborn) and Spark 2.0 (Scala, PySpark, MLlib)to develop variety of models and algorithms for analytic purposes. 
? Developed Map Reduce jobs written in java, using Hive for data cleaning and pre-processing. ? Used big data  Spark (Pyspark, SparkSQL, Mllib) to conduct real time analysis of loan default based on AWS. 
? Developed and configured on InformaticaMDM hub supports the MasterDataManagement (MDM),
BusinessIntelligence (BI) and Datawarehousing platforms to meet business needs. 
? Loaded data into Hive Tables from HadoopDistributed File System (HDFS) to provide SQL access onHadoop data 
? Used AgileMethodology of Data Warehouse development. 
? Design and implement data ingestion techniques for real time and batch processes for structuredand unstructured data sources into Hadoopecosystems and HDFSclusters. 
? Designed and developedarchitecture for data servicesecosystem spanning Relational, NoSQL, andBigData . 
? Responsible for data identification, collection, exploration, and cleaning for modeling, participate inmodel development 
? Implemented multi-datacenter and multi-rack Cassandra cluster. 
? Created HBase tables to load large sets of structured, semi-structured and unstructured data comingfrom NoSQL and a variety of portfolios. 
? Involved in data model reviews as dataarchitect with business analysts and business users withexplanation of the data model to make sure it is in-line with business requirements. 
? Created Entity relationships diagrams, data flow diagrams and enforced all referential integrityconstraints using Rational Rose 
? ed with the ETL team to document the SSIS packages for data extraction to Warehouseenvironment for reporting purposes. 
? Developed data Mart for the base data in Star Schema, Snow-FlakeSchema involved in developingthe data warehouse for the database. 
? Involved in Dataloading using PL\SQLScripts and SQLServer Integration Services packages ? Established data governance, monitoring of DataQuality and clear documentation for facile implementation. 
? Involved in the validation of the OLAP, Unittesting and System Testing of the OLAP Report
Functionality and data displayed in the reports. 
? Generated ad-hocSQLqueries using joins, database connections and transformation rules to fetchdata from Teradata database. 
? Created HBase tables to load large sets of structured, semi-structured and unstructureddata comingfrom UNIX, NoSQL and a variety of portfolios. 
? ed on AmazonRedshift and AWS and architecting a solution to load data creates data modelsand run BI on it. 
? ed on AWS and architecting a solution to load data creates data models and run BI on it. 
? Created UNIXscripts for file transfer and file manipulation 
? Directed to Create Dashboards based on the business requirement using SSRS/Cognos and helpeddevelopment team in knowledge about the requirement. 
? Handled importing of data from various data sources, performed transformations using Hive,
MapReduce, loaded data into HDFS and Extracted the data from Oracle into HDFS using Sqoop 
? ed with various Teradata15  and utilities like Teradata Viewpoint, MultiLoad, ARC,
TeradataAdministrator, BTEQ and other Teradata Utilities. 
? Involved in several facets of MDM implementations including DataProfiling, Metadataacquisition anddata migration. 
? Developed predictive models using Decision Tree, Random Forest, Nave Bayes, Logistic Regression,Cluster Analysis, and Neural Nets to predict analytical Online Advertising Pricing Model to maximize client's net revenues, predict accurate Revenue per Click estimates and build a fraud traffic detection system to flag potential bot sessions that cause inflated billings to the client's customers. ? Extensively used AginityNetezza  bench to perform various DML,DDL operations on Netezza database. 
? Created DDLscripts using Erwin and source to target mappings to bring the data from source to thewarehouse. 
? Lead database level tuning and optimization in support of application development teams on an ad-hoc basis. 
 
Environment: Erwin r9.6, Python, SQL, Oracle 12c,Netezza, SQL Server, Informatica, Java, SSRS, PL/ SQL, T-SQL, Tableau, MLlib, regression, Cluster analysis, Scala NLP, Spark, Kafka, MongoDB, logistic regression, Hadoop, Hive, Teradata, random forest, OLAP, Azure, MariaDB, SAP CRM, HDFS, ODS, NLTK, SVM, JSON, Tableau, XML, Cassandra, MapReduce, AWS, Assistant 15.0, Flat Files.
Sr. Data Scientist/Data Analyst
State of MA Boston - Boston, MA
May 2017 to July 2018
Description:Mass.gov is committed to achieving meaningful accessibility to this online environment for all users, including users with disabilities. We follow specific Commonwealth enterprise standards designed to meet the needs of our citizens with disabilities. The Commonwealth enterprise standards are generally based on standards used by the federal government for technology accessibility for people with disabilities, as well as web content accessibility guidelines developed by the World Wide Web Consortium (W3C). 
Responsibilities: 
? Evaluating the data analytics opportunities to improve the efficiency of claims handling process likeFraud Detection 
? Utilized various data analysis and data visualization  to accomplish data analysis, report designand report delivery. 
? Trained different ML algorithms models, including Logistic, Tree-Based, SVM, Knn and GBM, multipletimes after repeat model evaluations by confusion matrix and cross-validation aimed at finding out optimal parameter and hyper parameter to ensure prediction accuracy. 
? Create statistical models based on researched information to provide conclusions that will guide thecompany and the industry into the future. 
? Implemented PySpark jobs for batch processing to handle massive volume of data from various datasources - Bloomberg, Government publications, unstructured news articles, etc. and data persisted in
HDFS. Configured a CI/CD pipeline in Kubernetes and DockerSwarm. 
? Performed data cleaning and feature selection using MLLib package in PySpark and ing withdeep learning frames such as Caffe, Neon etc. 
? Taking care of missing data after import and encoding the categorical data, when needed. ? Splitting the data into training set, test set and scaling the data in training set and test set, if necessary. 
? Creatively communicated and presented models to business customers and executives, utilizing avariety of formats and visualization methodologies. 
? Impact of marketing tactics on sales and then forecast the impact of future sets of tactics. 
? Developed Scala and SQL code to extract data from various databases 
? Used R and python for Exploratory Data Analysis and Hypothesis test to compare and identify theeffectiveness of Creative Campaigns. 
? Used Scala, Python, R and SQL to create Statistical algorithms involving Linear Regression, Logistic
Regression, Random forest, Decision trees, Support Vector Machine for estimating the risks. 
? Developed statistical models to forecast inventory and procurement cycles. 
? Created and designed reports that will use gathered metrics to infer and draw logical conclusions ofpast and future behaviour. 
? Created pipelines for data ingestion and from various channels, through the scripts written inHive&Java. 
?  with a range of proprietary, industry standard, and open source data stores to assemble andorganize and analyze data. 
? Mapped customers to revenue to predict the revenue (if any) from a new prospective customer. 
? Visualizations, Summary Reports and Presentations using R and Tableau. 
? Uploaded data to HadoopHive and combined new tables with existing databases. 
? Involved in converting Hive/SQL queries into Spark transformations using SparkRDDs, and Scala. 
? Developed pyspark code and Spark-SQL/Streaming for faster testing and processing of data. 
? Supported Map Reduce Programs those are running on the cluster. 
? Created Data Quality Scripts using SQL and Hive to validate successful data load and quality of thedata. 
? Scheduled jobs and flow scheduler to manage Hadoop jobs. 
? Loaded the aggregated data into Data Mart for reporting, dash boarding and ad-hoc analysis using
Tableau and developed a self-service BI solution for quicker turnaround of insights. 
? Maintained SQL scripts to create and populate tables in data warehouse for daily reporting acrossdepartments. 
 
Environment: R 3.x, Python 2.x, Tableau 9, SQL Server 2012, Spark/Scala, SBT, Hive, Sqoop, Spark ML.
Sr. Data Scientist
Coventry Health Care - Downers Grove, IL
January 2016 to April 2017
Description:Coventry Health Care, Inc. is a diversified national insurer in the United States. Based in North Bethesda, Maryland, Coventry operates health plans, insurance companies, net rental and ers' compensation services companies. 
Responsibilities: 
? Participated in all phases of project life cycle including data collection, data mining, data cleaning,model building and validation, as well as report creating. 
? Utilized MapReduce and PySpark programs to process data for analysis reports. 
? ed on data cleaning to ensure data quality, consistency, and integrity using Pandas/Numpy. ? Performed data pre-processing on messy data including imputation, normalization, scaling, feature engineering etc. using Scikit-Learn. 
? ed on different data formats such as JSON, XML and performed ML algorithms in Python. ? Conducted exploratory data analysis using PythonMatplotlib and Seaborn to identify underlying patterns and correlations between features. 
? Built classification models based on Logistic Regression, Decision Trees, Random Forest Support
Vector Machine, and Ensemble algorithms to predict the probability of absence of patients. 
? Applied various metrics like recall, precision, F-Score, ROC, and AUC to evaluate the performance ofeach model and k-fold cross-validation to test the models with different batches of data to optimize the models. 
? Involving in creating data frames in Hadoop system, Spark using PySpark and then applying Hive/
SQL queries into Spark transformations using Spark RDDs, Python libraries. 
? Utilized PySpark, Spark Streaming, MLlib, in Spark ecosystem with a broad variety of machinelearning methods including classifications, regressions, dimensionally reduction etc. 
? Implemented and tested the model on AWSEC2, collaborated with development team to get the bestalgorithm and parameters. 
? ed on Nave Bayes algorithms for Agent Fraud Detection using R. 
? Performed data visualization and design dashboards with Tableau and generated complex reportsincluding chars, summaries, and graphs to interpret the findings to the team and stakeholders. 
 
Environment: Python (Scikit-Learn/Keras/Scipy/Numpy/Pandas/ Matplotlib/Seaborn), Machine Learning
(Linear and Non-linear Regressions, Deep Learning, SVM, Decision Tree, Random Forest, XGboost, Ensemble and KNN), MS SQL Server 2017, AWS Redshift, S3, Hadoop Frame, HDFS, Spark (Pyspark, MLlib, Spark SQL), Tableau Desktop and Tableau Server.
Data Analyst/Data Modeler
Aetna - San Diego, CA
March 2014 to December 2015
Description:Aetna is committed to providing individuals, employers, health care professionals, producers and others with innovative benefits, products and services. Aetna is now a subsidiary company of CVS Health Corporation. 
Responsibilities: 
? Analyzed and reviewed functional specifications and requirements to determine best data designapproach and translate business requirements into data models. 
? Created models for various schemas and created the metadata in order to deploy the models intomicro strategy to be able to reuse the definitions enterprise wide. 
? Performed data Ingestion for the incoming web feeds into the Data lake store which includes bothstructured and unstructured data. 
? Implemented Predictive analytics and machine learning algorithms to forecast key metrics inthe form of designed dashboards on to AWS (S3/EC2) and Django platform for the company's core business. 
? Create the architectural artifacts for the Enterprise Data Warehouse and the Operational Dashboard,such as Entity Relationship Diagrams (ERD), the DDL scripts, the Conceptual Data Model, and technical as well as business documents. 
? Conducted data profiling to insure that the available data could support business needs. edwith the developers on resolving the reported bugs and various technical issues. 
? Involved in requirements gathering activities analyze, and document business processes andfundamentals, and strategic data needs. 
? Created data source views from MYSQL and HADOOP data sources. 
? Migrated our retired systems to leverage new systems and customized according to the businessrequirement. 
? Enforced database naming standards and maintained user domains. 
? Supported data conversion activities and coordinated the resolution of conversion and datamigration issues. 
? Created and maintained Data Dictionary and pursued to reach consensus. 
? Created data lineages and mappings for Data Lake schemas. 
? Ensured Error logs and audit tables are generated and populated properly. 
? Involved in troubleshooting, resolving and escalating data related issues and validating data toimprove data quality. 
? Tracking and reporting the issues to project team and management. 
? Created mapping for horizontal data lineages for various systems. 
? Contribute in the development of knowledge transfer documentation. 
? Used Python, R and SQL to create Statistical algorithms involving Linear Regression, Logistic
Regression, Random forest, Decision trees for estimating the risks. 
? Managed change requests by following change request management process for the project. ? Involved in preparing a simple and detailed user guide and training manual for the application and for an intended novice user. 
Environment:Erwin 9.64, MS Access, Micro strategy, MySql, Erwin, Oracle10g, HeidiSql, Hadoop, Toad 12.5,MS Visio, SVN
Data Analyst
Symbiosys  - Visakhapatnam
December 2012 to February 2014
Description: Symbiosys  was founded in 2001 and is a 100% export oriented unit, registered in the Visakhapatnam Special Economic Zone (VSEZ). We provide high-quality services and solution to our clients worldwide. 
 
Responsibilities: 
? Extensively ed on Informatica PowerCenter Transformations such as Source Qualifier, Lookup,
Filter, Expression, Router, Joiner, Update Strategy, Rank, Aggregator, Sequence Generator etc. ? Proficiency in using Informatica PowerCenter tool to design data conversions from wide variety of sources. 
? Expertise in transforming business requirements into analytical models, designing algorithms,building models, developing data mining and reporting solutions that scale across a massive volume of structured and unstructured data. 
? Proficient in using Informatica flow manager, flow monitor to create, schedule and controlflows, tasks, and sessions. 
? Created pivot tables and ran VLOOKUP's in Excel as a part of data validation. 
? Used Informatica PowerCenter for extraction, loading and transformation (ETL) of data in the datawarehouse. 
? ed on data analysis, data discrepancy reduction in the source and target schemas. 
? Designed and developed complex mappings, from varied transformation logic like Unconnected and
Connected lookups, Router, Filter, Expression, Aggregator, Joiner, Update Strategy and more. ? Preparation of System requirements (SRS), Database specifications (DBS), Software design document (SDD). 
? Responsible for the maintenance of few applications in PowerBuilder 10.2 
? Involved in using SQLServer 2005 for fixed the production issues in the background. 
? Coordination and Quality activities on delivery 
? Involved in testing with validation of all fields, functions, programs, agents from front end and backend code reviews across the application. 
? Involved in preparation program specifications, unit tests, test cases and user manual documents. 
 
Environment: Informatica 8.x, PowerBuilder 10.2, SQL Server 2005.
Java Developer
Clover InfoTech - Mumbai, Maharashtra
October 2011 to November 2012
Description: BOLSTER Solutions was established in 2011 and is based in Hyderabad. Our project clients vary in size from start-ups and SMEs to larger international companies. Our main area of expertise is building complex, secure and database-driven web-based applications. Last year we ed for 26 clients, completing over 50 , 65% of our business came from repeat orders from existing clients. 
Responsibilities: 
? Involved in each phase of Software Development Life Cycle(SDLC) models like Requirementgathering and analysis, Design, Implementation, Testing, Deployment and Maintenance. 
? Developed Login, Policy and Claims Screens for customers using HTML 5, CSS3, JavaScript, AJAX, JSP,and jQuery. 
? Used Core Java to develop Business Logic. 
? ML models developed: Customer Survival Analysis for better targeting, Member Engagement callcenter optimization, Financial Forecasting for product realization. 
? Involved in the development of business module applications using J2EE  like Servlets,JSP. 
? Designed and developed the web-tier using JSP's, Servlets frame. 
? Used various Core Java concepts such as Multi-Threading, Exception Handling, Collection APIs toimplement various features and enhancements. 
? Strong experience in design & development of applications using Java/J2EE components such as JavaServer Pages (JSP). 
? Developed EJB MDB's and message Queue's using JMS technology. 
? EJB Session Beans were used to process requests from the user interface and CMP entity beans wereused to interact with the persistence layer. 
? Developed stored procedures, triggers, and queries using PLSQL in SQL Server. 
? Use Spring MVC as frame and JavaScript for client-side view, used frames for client-sidedata validation, creating dynamic web pages-Ajax, jQuery. Developed model classes based on the forms to be displayed on the UI. 
? ML Algorithms used: Logistic/Linear Regression, Random Forest, XG-Boost, K-Means Clustering e.t.c.
 
? Implemented various design patterns in the project such as Business Delegate, Data Transfer
Object, Data Access Object, Service Locator and Singleton. 
? Used SQL statements and procedures to fetch the data from the database. 
? Developed test cases and performed unit test using JUnit Frame. 
? Used CVS as version control and ANT scripts to fetch, build, and deploy application to developmentenvironment. 
 
Environment:Java, HTML, CSS, JavaScript, MySQL, Struts, EJB, Spring MVC.
Education

Bachelor's

Data modeling, Db2, Ms access, Sql server, Sql server 2012, Oracle, Pl/sql, Sql, Sybase, Sqoop, Hbase,
Flume, Hadoop, Tableau server, Teradata, C++, Hadoop, Hbase, Hive, Html
Additional Information

Technical : 
 
Data Modeling  Erwin r 9.6/9.5, ER/Studio 9.7, Star-Schema Modeling, Snowflake-Schema
Modeling, FACT and dimension tables, Pivot Tables. 
Databases Oracle 11g/12c, MS Access, SQL Server 2012/2014, Sybase and DB2, Teradata, Hive. 
Big Data  Hadoop, Hive, Spark, Pig, HBase, Sqoop, Flume. 
BI  Tableau 7.0/8.2, Tableau server 8.2, Tableau Reader 8.1,SAP Business Objects, Crystal Reports
 
Packages Microsoft Office 2010, Microsoft Project 2010, SAP and Microsoft Visio, Share point Portal Server 
Operating Systems Microsoft Windows 8/7/XP, Linux and UNIX. 
Languages SQL, PL/SQL, ASP, Visual Basic, XML, Python, SQL, T-SQL, SQL Server, C, C++, JAVA, HTML , UNIX shell scripting, PERL, R. 
Applications Toad for Oracle, Oracle SQL Developer, MS Word, MS Excel, MS Power Point, Teradata, Designer 6i. 
Methodologies RAD, JAD, RUP, UML, System Development Life Cycle (SDLC), Waterfall Model. 
Data Modeling  Erwin r 9.6/9.5, ER/Studio 9.7, Star-Schema Modeling, Snowflake-Schema
Modeling, FACT and dimension tables, Pivot Tables. 
Databases Oracle 11g/12c, MS Access, SQL Server 2012/2014, Sybase and DB2, Teradata14/15, Hive. 
Big Data  Hadoop, Hive, Spark, Pig, HBase, Sqoop, Flume. 
BI  Tableau 7.0/8.2, Tableau server 8.2, Tableau Reader 8.1,SAP Business Objects, Crystal Reports
 
Packages Microsoft Office 2010, Microsoft Project 2010, SAP and Microsoft Visio, Share point Portal
Server",Data Scientist,emailed,indeed job post,resume
"Applied mathematics graduate student currently pursuing a Master of Arts degree in an ABETaccredited program. Obtained a Bachelor of Science degree in Aerospace Engineering in 2015. Possesses , and analytical and  . Loyal and focused, with problem solving and critical thinking . Ready for a career with opportunities to contribute to company goals and s.
 Experience

Applications Engineer
Spectra Quest, Inc - Richmond, VA
July 2015 to November 2017
* Conducted experiments, analyzed machine vibration data, and wrote  reports. 
* Tested prototype software and electrical components and systems. 
* Developed test procedures for new and existing products. 
* Recommended design modifications that enhance performance and eliminate malfunctions. 
* Processed digital signals and sensor data. 
* Proofread and edited product manuals. 
* Calibrated proximity probes, force transducer signal conditioner, and torque sensors. 
* Balanced high speed motors and provided the balance reports. 
* Performed electrical fabrication  including soldering, wiring, and BNC cable installation. 
* Provided  support to worldwide customers and resolved hardware and software issues. * Provided on-site training. 
* Assisted hands-on machine vibration analysis course. 
* Created and provided sales quotations. 
* Created  orders and shipping documents. 
* Assembled simulators, data acquisition, sensors, and embedded computer. 
* Fabricated faulted components such as bearings for experimental fault analysis. 
* Attended conferences and symposiums. 
 Turbomachinery & Pump Symposia (Houston, TX - Sep 2015 & 2016) 
 IMAC - Society for Experimental Mechanics (Orlando, FL - Jan 2016) 
 ASEE (New Orleans, LA - June 2016, Columbus, OH - June 2017) 
 MFP (Virginia Beach, VA - May 2017) 
 IMVAC (Orlando, FL - Nov 2017)
Engineering Intern
Diablo Canyon Power Plant - Avila Beach, CA
June 2009 to November 2009
* Monitored equipment vibration using SKF Microlog Analyzer AX Series and GX Series on a daily basis.* Reported results from vibration monitoring to the Predictive Maintenance Engineering division for analysis. 
* Developed a spreadsheet for each piece of equipment to identify routes, motor data, pump data, andtypes of bearings.
Outage Electrical Utility er
Diablo Canyon Power Plant - Avila Beach, CA
January 2009 to March 2009
* Gained knowledge of power plant operations and protocols. 
* Assisted electricians modify and repair electrical breakers. 
* Delivered supplies to on-site facilities from the warehouse. 
* Operated forklift. 
 
Intern, Research Experiences for Undergraduates 
Program in Robotics and Autonomous System
Intern, Research Experiences for Undergraduates
California Polytechnic State University - San Luis Obispo, CA June 2008 to August 2008
* Collaborated within a group to modify software in a robot to remotely walk towards a target. * Implemented the Sick LIDAR module to communicate with software in a robot via Bluetooth. * Researched Denavit-Hartenberg convention to assign coordinate frames to the robot.


Bachelor of Science degree in Aerospace Engineering
California Polytechnic State University - San Luis Obispo, CA
May 2020
Associate of Science degree in Mathematics
Cuesta College - San Luis Obispo, CA
June 2015
Master of Arts in Applied Mathematics
California State University - Fullerton, CA


Matlab (7 years), Python (1 year), Excel (10+ years), Outlook (3 years), Powerpoint (5 years), Word
(10+ years), Publisher (Less than 1 year), R (Less than 1 year), Latex (Less than 1 year), testing, Microsoft Office
Certifications/Licenses

ISO Vibration Analyst Certification Level II
July 2017 to Present
Vibration Institute
Commercial Pilot Helicopter License
March 2004 to Present
Federal Aviation Administration
Private Pilot Helicopter License
October 2000 to March 2004
Federal Aviation Administration
Assessments

Data Analysis  Expert
August 2019
Interpreting and producing graphs, identifying trends, and drawing justifiable conclusions from data.
Full results: https://share.indeedassessments.com/share_assignment/la7pv5a-aez0jftt
Problem Solving  Highly Proficient
August 2019
Analyzing relevant information when solving problems.
Full results: https://share.indeedassessments.com/share_assignment/4p0o8kttrd14qbvp
Indeed Assessments provides  tests that are not indicative of a license or certification, or continued development in any  field.
Additional Information

Written and verbal bilingual Japanese communication , and experience translating  documents and digital presentations.",Data Scientist,emailed,indeed job post,resume
"  with around 5 years of experience in all phases of diverse  specializing in Data Science, Big Data, Azure Machine Learning, Google Cloud and Tableau, using Cloud based infrastructure. 
 ed on analyzing large datasets on distributed databases and developing Machine Learningalgorithms to gain operational insights and present them to the leadership. 
 Extensively ed on Data preparation, exploratory analysis, Feature engineering using supervisedand unsupervised modeling. 
 Experienced the full software life cycle in SDLC, Agile and Scrum methodologies. 
 Expert in using of statistical  and programming languages (R, Python, C, C++, Java, SQL, UNIX) 
 Adapted statistical programming languages like R and Python 
 Well-versed with Linear/non-linear, regression and classification modeling predictive algorithms. 
 Actively involved in model selection, statistical analysis using SAS and Gretl statistical tool. 
 Created dashboards as part of Data Visualization using Tableau. 
 Proficiency in using Spark for Bigdata processing in the Hadoop/DataProc/ EMRE ecosystem.  Performed preliminary data analysis using descriptive statistics and handled anomalies such as removing duplicates and imputing missing values using Talend tool. 
 Performed Dimensionality reduction using principal component analysis, auto encoders, and t-SNE.  Validate the consolidated data and develop the model that best fits the data. Interpret data from multiple sources, consolidate it, and perform data cleansing using R/Python/Spark. 
 Performed multiple Data Mining techniques and derive new insights from the data. 
 Skilled in Machine Learning, Statistical Modeling, and Big Data. 
 Creative problem-solver with strong analytical, leadership, and communication  
 Proficient in Python, R, Scala, Java, SQL, and C 
 Experienced in Machine Learning, Data mining with large datasets of Structured and Unstructured
Data, Data Acquisition, Data Validation, and Predictive Modeling 
 Data Science Specialties include: Machine Learning, Sequential Modeling, Natural LanguageProcessing (NLP) 
 Use of Analytical : Bayesian Analysis, Inference, Time-Series Analysis, Regression Analysis,
Linear models, Multivariate analysis, Sampling methods, Forecasting, Segmentation, Clustering,
Sentiment Analysis, Part of Speech Tagging, and Predictive Analytics 
 Experienced in stochastic optimization and regression with machine learning algorithms 
 Experienced in formulating and solving discrete and continuous optimization problems 
 Able to research statistical machine learning, supervised learning, and classification methods  Strong mathematical and statistical modeling and computer programming  in an innovative manner 
 Use of Various Analytics : Classification and Regression Trees (CART), Support Vector Machine
(SVM), Random Forest, Gradient Boosting Machine (GBM), Principal Component Analysis (PCA),
Regression, Nave Bayes, Support Vector Machines 
 Experienced in AWS cloud computing, Spark, and capable of ing with large datasets 
 Deep Learning: Machine perception, Data Mining, Machine Learning algorithms, Neural Nets,TensorFlow, Keras 
 Delivered presentations and highly  reports; collaboration with stakeholders and cross-functional teams, advisement on how to leverage analytical insights 
 Development of clear analytical reports which directly address strategic goals 
 Identified and learn applicable new techniques independently as needed 
 Able to  comfortably and effectively within an interdisciplinary research environment 
 Experienced with validation of machine learning ensemble classifiers 
 Utilized the online datasets to implement machine learning models using Spark ML for buildingprototypes
Willing to relocate: Anywhere
Authorized to  in the US for any employer
 Experience

Data Science Engineer
Jefferies LLC - Jersey City, NJ February 2019 to Present
Responsibilities: 
 Coded in Python with selenium and automated website data scraping 
 Scripted using R for cleaning, merging and extraction of relevant data 
 Created interactive visualization using Tableau and performed data analysis to report findings andtrends 
 Analyzed massive data models with tables having over 100s of millions of records to draw insightsand useful information. 
 Architect complex database systems on Hadoop to scale out data development processes. 
 Explore  and gather insights from Amfam's operational data stores and data warehouses
(stored in Oracle, Greenplum, HDFS and S3) by querying them and creating data visualizations (in Tableau) 
 Designed and developed scripts to test data and find data defects. 
 Determined the quality of data, verify accuracy of information and ensure that the data is fit formodeling purposes. 
 Transformed data elements and attributes into usable form based on business requirements.  Blend data sets at different granularity levels using analytical queries, window functions and SQL joins. 
 Identified data duplicates and develop means to remove them. 
 Analyzed tabular data to determine or alter their grain (drill down or roll up) using analytical queries,MapReduce or python. 
 Designed and develop data pipelines to preprocess modeling data such as handle null values andclean up defective data attributes. 
 Developed Spark code to parse out and transform semi structured data such as XMLs, JSONs and
CSVs into hive tables or data frames. 
 Explored and determined ways to organize data in Hive tables for fast read and writes through hivetable partitions and buckets for optimized performance. 
 Developed programs to store data in appropriate file formats and logical grouping in tables. 
 Optimized code and queries to run faster and efficiently. Optimize ETL processes for distributed datamarts on Hadoop 
 Maintained development activities in version control and create updated documentation. 
 
Environment: HADOOP (HDFS) Horton s, AWS, SPARK, Python, Java, Hive, Beeline, Apache pig, Tableau, SAS, Oracle, DB2, MySQL
Data Scientist
Anthem - Atlanta, GA
June 2018 to February 2019
Responsibilities: 
 Translated business questions into research s, design and conduct analyses, developfindings and synthesize recommendations to deliver valuable, relevant, and actionable insights  Strong track record of contributing to successful end-to-end analytic solutions (clarifying business s and hypotheses, communicating project deliverables and timelines, and informing action based on findings) 
 Used Pandas, NumPy, Scikit-Learn in Python for performing exploratory analysis and developingvarious machine learning models such Random forest 
 The missing data in the dataset is handled using Imputer method in SkLearn library 
 Performed categorical variable analysis using python Label Encoder, fit transform, One Hot Encodermethods in sklearn library 
 Responsible for design and development of advanced R/Python programs to prepare to transformand harmonize data sets in preparation for modeling 
 Defined a generic classification function, which takes a model as input and determines the Accuracyand Cross-Validation scores 
 Advanced SQL ability to efficiently  with very large datasets. Ability to deal with non-standardmachine learning datasets 
 Built forecasting models in Python using Gradient Boost Regression Trees. Forecasted the revenuefor future 
 ed with applied statistics and applied mathematics  for performance optimization 
 ed with K-Means clustering and Hierarchical clustering algorithm to do segmentation of stores  Collected various store attributes and added them into our segmentation model in order to better classify different segments using clustering algorithms 
 Used cross-validation to test the models with different batches of data to optimize the models andprevent over fitting 
 Analyzed the SQL scripts and designed the solution to implement using PySpark and developedscripts as per the requirement 
 ed with Tableau in order to represent the data in visual format and better describe the problemwith solutions. 
 
Environments: Python, PyCharm, Jupyter, Notebook, Spyder, R, Tableau, MySQL
Data Scientist
US Bank - Brookfield, WI
September 2017 to June 2018
Responsibilities: 
 Used SQL alongside a variety or reporting  - BusinessObjects, Power BI, Tableau - to developoperational and visual reports for KPI monitoring 
 Data analysis and visualization (Python, R) 
 Designed, implemented and automated modeling and analysis procedures on existing andexperimentally created data 
 Increased pace & confidence of learning algorithm by combining state of the art  andstatistical methods; provided expertise and assistance in integrating advanced analytics into ongoing business processes 
 Parsed data, producing concise conclusions from raw data in a clean, well-structured and easilymaintainable format 
 Implemented Topic Modelling, linear classifier models 
 Collaborate with UI engineers, project managers, and designers to develop web portal thataggregates reports from various sources and  
 Member of Data Science team tasked with helping clients turn data into a strategic asset 
 Focused on front end features, browser manipulation, and cross-browser compatibility. 
 Utilize  expertise including HTML5, CSS3, JavaScript, JQUERY, HTML, Node.js, Angular.js, and
DOM to develop reporting portal. 
 Used Agile Scrum for BI  across different clients, which allowed for production prototyping,rapid deployment and transparency. 
 
Environment: Tableau, SQL, Java, HTML, Oracle, Agile, Hadoop.
Junior Data Scientist
Ordnance Factory Board
January 2016 to July 2017
Description: Built a new team and managed in designing cost effective A/B tests to determine high performance marketing campaigns and contributed to increase in sales by 20% and reduced the promotional cost by 35%. 
 
Responsibilities: 
 ed on various phases of data mining- data collection, data cleaning, developing models,validation, visualization. 
 Captured Modelling requirements from Senior Stakeholders to fetch functional requirements for SAS/R, Python. 
 Performed Data Manipulation and Aggregation from various sources including HDFS and createdvarious Predictive and Descriptive analytics using R and Tableau. 
 Used various libraries and developed various matching learning algorithms using Pandas, NumPy,
Seaborn, Scipy, matplotlib, Scikit-learn in python. 
 Designed Predictive analysis algorithms using Historical Data. 
 Utilized machine learning algorithms as well as implemented algorithms such as Decision Tree, linearregression, multivariate regression, Naive Bayes, Random Forests, K-means, & KNN. 
 ed on Map Reduce/Spark Python modules for machine learning and predictive analytics in
Hadoop on AWS. 
 ed on Reporting tool (Tableau) Test, Validate Data Integrity of Reports. 
 
Environment: Python, I Python, Scikit-Learn, MySQL, SQL, NoSQL, Data Modelling, Data Warehouse, Hadoop (MapReduce, HBase, Hive), Gradient Boost, Random Forest, Neural Nets, Sklearn etc.
Data Science Intern
iPrism Technologies - Hyderabad, Telangana
April 2015 to December 2015
Description: The project was to build a classification model predicting the probability of a customer who will not subscribe to paid membership, to help the marketing team to focus on improving the subscription rate. 
Responsibilities: 
 Collected data from end client, performed ETL and defined the uniform standard format 
 Wrote queries to retrieve data from SQL Server database to get the sample dataset containing basicfields 
 Performed string formatting on the dataset converting hours from date format to a numerical integer
 
 Used Python libraries like Matplotlib and Seaborn to visualize the numerical columns of the datasetsuch as day of week, age, hour and number of screens. 
 Developed and implemented predictive models like Logistic Regression, Decision Tree, Support
Vector Machine (SVM) to predict the probability of enrollment 
 Used Ensemble learning methods like Random Forest, Bagging & Gradient Boosting& picked the finalmodel based on confusion matrix, ROC & AUC & predicted the probability of customer enrollment  Tuned the hyper parameters of the above models using Grid Search to find the optimum models 
 Designed and implemented K-Fold Cross-validation to test and verify the model's significance  Developed a dashboard and story in Tableau showing the benchmarks and summary of model's measure. 
 
Environment: SQL Server 2012/2014, Python 3.x (Scikit-Learn, NumPy, Pandas, Matplotlib, Dateutil, Seaborn), Tableau, Hadoop
Education

Master's in Computer Engineering
Arizona State University - Tempe, AZ


PYTHON (3 years), SQL (3 years), Hadoop (3 years), HADOOP (3 years), MYSQL (2 years)
Additional Information

 : 
Programming languages Python, Java, R. C 
Data  and Frames s Hadoop (Apache Spark, Pig, Hive, MapReduce), D3.js, Tableau, MATLAB, R-Studio 
Data Science Machine Learning, Deep Learning, Natural Language Processing, Computer Vision, Neural
Nets, Machine Perception 
Operating systems Windows, Linux. 
Databases MySQL, MS SQL Server, NoSQL 
Web and Cloud Technologies AWS, HTML5 
Languages Python 3.7, SQL, R 3.6, Java, JavaScript, PL/SQL",Data Scientist,emailed,indeed job post,resume
"Doctor of Philosophy. Research area Plant Ecology & Evolution. Chinese Academy of Sciences/World Agroforestry Centre (ICRAF). 
2011  2015  	Master of Science in Ecology. Research area Plant Ecology & Evolution 
Xishuangbanna Tropical Botanical Garden, Chinese Academy of Sciences. 
2006  2008  	Bachelor of Science in Forestry. Faculty of Forestry, National University of Laos. 
2002  2005 	Associate Degree in Forestry. Faculty of Forestry, National University of Laos. 
 
PROFRESSIONAL EXPERIENCE 

2016  2017 	Associate Statistic Instructor for World banks sub-Project in Laos. 
2014  2016  	Team leader and researcher. The Land Degradation Surveillance Frame 
(LDSF), Mekong Sentinel Landscape Project, ICRAF and Consultative Group for International Agricultural Research (CGIAR) collaborative project. Led the fields (vegetation surveys, collecting soil samples and etc), plant identification, data management, data analysis, writing publications. 
2012  2013 	Non-timber Forest Product Advisor and Assistant Project Manager to Triangle Generation Humanitaire. 
2009  2011  	Lao plant database developer, Curriculum developer, and Lecturer assistant. 
2005  2011  Researcher and Biodiversity Consultant to International non-Government Organizations and Government of Lao PDR. 
 
 AND STATISTICAL SOFTWARE 
 
 
 
 
 
Advance statistical analysis and machine learning in R (8 years). 
Linear modelling, Structural Equation Modelling, Spatial analysis in R. 
Net Analysis, Path analysis, Factor analysis, cluster analysis in R. 
 
 
  
 
Regressions, ANOVA and etc in R. Data management, manipulation, visualization in R. 
Phylogenetic analysis in R, MEGA5.  Experimental design, Land navigation Forest inventory, Plant identification. 
GIS (QGIS) and spatial analysis in R. 
 
LANGUAGES 

Lao (native), English (fluent), Thai (fluent), Chinese mandarin (basic). 
 
RECENT SCIENTIFIC PAPERS 

1. Satdichanh, M., Ma, H., Yan, K., Dossa, G. G. O., Winowiecki, L., Vgen, T.-G.,  Harrison, R. D. (2019). Phylogenetic diversity correlated with above?ground biomass production during forest succession: Evidence from tropical forests in Southeast Asia. Journal of Ecology, 107(3), 14191432. doi:10.1111/1365-2745.13112. 
2. Satdichanh, M., Millet, J., Heinimann, A., Nanthavong, K., & Harrison, R. D. (2015). Using Plant Functional Traits and Phylogenies to Understand Patterns of Plant Community Assembly in a Seasonal Tropical Forest in Lao PDR. PLOS ONE, 10(6), e0130151. doi:10.1371/journal.pone.0130151. ",Data Scientist,emailed,indeed job post,resume
" Data Scientist with around 5 years of experience in areas including Data Analysis, Statistical Analysis,Machine Learning, Deep Learning, Data mining with large data sets of structured and unstructured data 
 Experienced in using various Python libraries (Beautiful Soup, NumPy, Scipy, matplotlib, python-twistter, Pandas, MySQL DB for database connectivity). 
 Experience in building end to end data science solutions using R, Python, SQL and Tableau byleveraging machine learning based algorithms, Statistical Modeling, Data Mining, Natural Language
Processing (NLP) and Data Visualization. 
 Adept and deep understanding of Statistical Modeling, Multivariate Analysis, model testing, problemanalysis, model comparison and validation. 
 Adapted with Python and OOP concepts such as Inheritance, Polymorphism, Abstraction, Association,etc. 
 Sound understanding of Deep learning using CNN, RNN, ANN, reinforcement learning, transferlearning. 
 Experienced in developing machine learning models for real-world problems using R and python  Experienced in Agile Methodologies, Scrum stories and sprints experience in a Python based environment, along with data analytics, data wrangling. 
 ed on theoretical foundations and practical hands-on  related to supervised learning(linear and logistic regression, boosted decision trees, Support Vector Machines (SVM), neural nets(NN), NLP), unsupervised learning (clustering, dimensionality reduction, recommender systems), probability & statistics, experiment analysis, confidence intervals, A/B testing, algorithms and data structures. 
 Excellent understanding of machine learning techniques and algorithms, such as K-NN, Naive Bayes,
SVM, Decision Forests, Random forest etc. 
 Experience with command-line scripting, data structures and algorithms. 
 Experienced in processing large datasets with Spark using Python. 
 Solid understanding of big data  like Hadoop, Spark, HDFS, MapReduce, Pig and Hive. 
 Experience with machine learning  and libraries such as Scikit-learn, R, Spark and Weka 
 Experience ing with large, real world data (Unsupervised Data) - big, messy, incomplete, full oferrors 
 Hands-on experience with NLP, mining of structured, semi-structured, and unstructured data 
 Expertise in database Performance Tuning using Oracle Hints, explain plan, TKPROF, Partitioning andIndexes 
 Experience in manipulating the large data sets with R packages like tidyr, tidyverse, dplyr reshape,lubridate, Caret and visualizing the data using lattice and ggplot2 packages. 
 Intensive hands-on Boot camp on Data Analytics course spanning from Statistics to Programmingincluding data engineering, data visualization, machine learning and programming in R, SQL.  Strong background in Machine Learning, Predictive Analysis and Data Mining with a broad understanding of Supervised and Unsupervised learning techniques and algorithms (eg: Regression, K-
NN, SVM, Nave Bayes, Decision trees, Clustering, etc.) 
 Proficient in data visualization  such as Tableau 10.5, Power BI 2.30, Python Matplotlib/Seaborn,R ggplot2/Shiny to generate charts like Box Plot, Scatter Chart, Pie Chart and Histogram etc., and to create visually impactful and actionable interactive reports and dashboards. 
 Experience in using Teradata ETL  and utilizes such as BTEQ, MLOAD, FASTLOAD, TPT, FastExport. 
 Experience with  such as R Programming, visualizations, SAS, Open Source etc. 
 Strong experience writing stored procedures, functions, triggers and adhoc queries using PL/SQL 
 Experienced in integration of various relational and non-relational sources such as DB2, Oracle,
Netezza, SQL Server, NoSQL, COBOL, XML and Flat Files, to Netezza database. 
 Extensive experience in Normalization (1NF, 2NF, 3NF and BCNF) and De-normalization techniquesfor improved database performance Data Warehouse/Data Mart environments.
Willing to relocate: Anywhere
Authorized to  in the US for any employer
 Experience

Sr. Data Scientist/Machine Learning Engineer
Nike - Hillsboro, OR
April 2018 to Present
Responsibilities: 
 ings on different data formats such as JSON, XML and performed machine learning algorithms inPython. 
 Setup storage and data analysis  in Amazon Web Services (AWS) cloud computing infrastructure.
 
 Implemented end-to-end systems for Data Analytics, Data Automation and integrated with customvisualization  using R, Mahout, Hadoop and MongoDB. 
 ed as Data Architects and IT Architects to understand the movement of data and its storage andER Studio 9.7. 
 ed with several R packages including knitr, dplyr, SparkR, Causal Infer, Space-Time. 
 Coded R functions to interface with Caffe Deep Learning Frame. 
 Used Pandas, Numpy, Seaborn, Scipy, Matplotlib, Sci-kit-learn, and NLTK in Python for developingvarious machine learning algorithms. 
 Demonstrated experience in design and implementation of Statistical models, Predictive models,enterprise data model, metadata solution and data life cycle management in both RDBMS, Big Data environments. 
 Utilized domain knowledge and application portfolio knowledge to play a key role in defining thefuture state of large, business  programs. 
 Machine Learning algorithms such as decision trees and random forest were used in this process topredict the urgency of the problem statement received by the company, this was done by calculating the weighted totals of the polarity and subjectivity of the problem statements and classifying each statement accordingly. 
 Text data received in the problem statements was converted into numerical/ordinal data usingparameters like polarity and subjectivity by developing a mathematical model to integrate the two statistics. 
 Installed and used Caffe Deep Learning Frame 
 Utilized Spark, Scala, Hadoop, HBase, Cassandra, MongoDB, Kafka, Spark Streaming, MLLib, Python,a broad variety of machine learning methods including classifications, regressions, dimensionally reduction etc. 
 Used Spark Data frames, Spark-SQL, Spark MLLib extensively and developing and designing POC'susing Scala, Spark SQL and MLlib libraries. 
 Used Data Quality Validation techniques to validate Critical Data Elements (CDE) and identifiedvarious anomalies. 
 Developed various Qlik-View Data Models by extracting and using the data from various sources files,DB2, Excel, Flat Files and Big data. 
 Participated in all phases of Datamining, Data-collection, Data-Cleaning, Developing-Models,
Validation, Visualization and Performed Gap Analysis. 
 Good knowledge of Hadoop Architecture and various components such as HDFS, Job Tracker, Task
Tracker, Name Node, Data Node, Secondary Name Node, and MapReduce concepts. 
 As Architect delivered various complex OLAP Databases/Cubes, Scorecards, Dashboards and Reports.
 
 Programmed a utility in Python that used multiple packages (Scipy, Numpy, Pandas) 
 Implemented Classification using supervised algorithms like Logistic Regression, Decision trees, KNN,Naive Bayes. 
 Designed both 3NF data models for ODS, OLTP systems and Dimensional Data Models using Star andSnowflake Schemas. 
 Updated Python scripts to match training data with our database stored in AWS Cloud Search, so thatwe would be able to assign each document a response label for further classification. 
 Created SQL tables with referential integrity and developed queries using SQL, SQL PLUS and PL/SQL.
 
 Designed and developed Use Case, Activity Diagrams, Sequence Diagrams, OOD (Object orientedDesign) using UML and Visio. 
 
Environment: AWS, R, Informatica, Machine learning-Algorithms, Anaconda, Market Basket Analysis,
Sentiment Analysis, Polarity, Predictive Analytics, Deep Learning- Algorithms, CNN, HCNN, Python,
Data Mining, Data Collection, Data Cleaning, Validation, HDFS, ODS, OLTP, Oracle 10g, Hive, OLAP, DB2, Metadata, MS Excel, Mainframes MS Vision, Map-Reduce, Rational Rose, SQL, and MongoDB.
Data Scientist
Vanguard - Malvern, PA
August 2017 to March 2018
Responsibilities: 
 Implementation of machine learning methods, optimization, and visualization. Mathematical methodsof statistics such as Regression Models, Decision Tree, Nave Bayes, Ensemble Classifier, Hierarchical
Clustering and Semi-Supervised Learning on different datasets using Python. 
 Researched and implemented various Machine Learning Algorithms using the R language. 
 Devised a machine learning algorithm using Python for facial recognition. 
 Used R for a prototype on a sample data exploration to identify the best algorithmic approach andthen wrote Scala scripts using spark machine learning module. 
 Used Scala scripts for spark machine learning libraries API execution for decision trees, ALS, logisticand linear regressions algorithms. 
 ed on Migrating an On-premises virtual machine to Azure Resource Manager Subscription with
Azure Site Recovery. 
 Provide consulting and cloud architecture for premier customers and internal  running on MS
Azure platform for high availability of services, low operational costs. 
 Develop structured, efficient and error-free codes for Big Data requirements using my knowledge in
Hadoop and its Eco-system. 
 Development of web service using Windows Communication Foundation and.Net to receive andprocess XML files and deploy on Cloud Service on Microsoft Azure. 
 ed on various methods including data fusion and machine learning and improved the accuracyof distinguished right rules from potential rules. 
 Developed Merge jobs in Python to extract and load data into a MySQL database. 
 Used Test driven approach for developing the application and Implemented the unit tests using
Python Unit test frame. 
 Wrote unit test cases in Python and Objective-C for other API calls in the customer frames. 
 Tested with various Machine Learning algorithms like Support Vector Machine (SVM), Random Forest,
Trees with XGBoost concluded Decision Trees as a champion model. 
 Built models using Statistical techniques like Bayesian HMM and Machine Learning classificationmodels like XGBoost, SVM, and Random Forest. 
 ed on different data formats such as JSON, XML and performed machine learning algorithms inPython. 
 
Environment: Machine Learning, R Language, Hadoop, Big Data, Azure, Python, Java, J2EE, Spring,
Struts, JSF, Dojo, JavaScript, DB2, CRUD, PL/ SQL, JDBC, coherence, MongoDB, Apache CXF, soap, Web Services, Eclipse
Data Scientist
Line -Vision - Boston, MA
January 2017 to July 2017
Responsibilities: 
 Analyzed large data sets apply machine learning techniques and develop predictive models,statistical models and developing and enhancing statistical models by leveraging best-in-class modeling techniques. 
 Analyzed pre-existing predictive model developed by advanced analytics team and factorsconsidered during model development. 
 Experienced in all phases of data mining; data collection, data cleaning, developing models,validation and visualization. 
 Analyzed metadata and processed data to get better insights of the data. 
 Created initial data visualizations in tableau to provide basic insights of data to the takeholders. 
 Application of various machine learning algorithms and statistical modeling like decision trees,regression models, clustering, SVM to identify Volume using scikit-learn package in Python. 
 Conducted regular communications with leaders of other teams to get better understanding of thedata at a deeper level. 
 Analyzed dataset of 14M record count and reduced it to 1.3M by filtering out rows with duplicatecustomer IDs and removed outliers using boxplots and univariate algorithms. 
 Performed extensive exploratory data analysis using Teradata to improve the quality of the datasetand developed Machine Learning algorithms using Python for predicting the model quality and created
Data Visualizations using Tableau. 
 Developed visualizations using R packages like ggplot2, choroplethr to identify patterns and trends inthe preprocessed data. 
 Experienced in RStudio packages and Python libraries like SciKit-Learn to improve the modelaccuracy from 65% to 86%. 
 Experienced in various Python libraries like Pandas, One dimensional NumPy and Two dimensionalNumPy. 
 Experienced in using PyTorch library and implementing natural language processing. 
 Developed data visualizations in Tableau to display day to day accuracy of the model with newlyincoming 
 data. 
 Hold a point-of-view on the strengths and limitations of statistical models and analyses in variousbusiness contexts and can evaluate and effectively communicate the uncertainty in the results.  Used Keras library to build and train deep learning models and fetched good results. 
 Propensity model developed that was beneficial with a greater ROI compared to other models.
Achieved 0.95 million dollars ROI per cycle with cycle duration of one quarter year. 
 Implemented complete data science project involving data acquisition, data wrangling, exploratorydata analysis (EDA), model development and model evaluation. 
 
Environment: MS Access, SQL Server, Teradata, Advanced SQL, RStudio (ggplot2, caret), Python (Pandas, NumPy, Sci-kit learn), Machine Learning (Logistic Regression, Decision trees, SVM, Random forest), PyTorch, Keras, Tableau, Excel
Data Analyst
Qualex - IN
April 2014 to November 2016
Responsibilities: 
 Collected data from end client, performed ETL and defined the uniform standard format 
 Wrote queries to retrieve data from SQL Server database to get the sample dataset containing basicfields 
 Performed string formatting on the dataset converting hours from date format to a numerical integer
 
 Used Python libraries like Matplotlib and Seaborn to visualize the numerical columns of the datasetsuch as day of week, age, hour and number of screens. 
 Developed and implemented predictive models like Logistic Regression, Decision Tree, SupportVector 
 Machine (SVM) to predict the probability of enrollment 
 Used Ensemble learning methods like Random Forest, Bagging, Gradient Boosting, picked the finalmodel based on confusion matrix, ROC, AUC predicted the probability of customer enrollment  ed on missing value imputation, outlier identification with statistical methodologies using
Pandas 
 NumPy 
 Tuned the hyper parameters of the above models using Grid Search to find the optimum models 
 Designed and implemented K-Fold Cross-validation to test and verify the model's significance  Developed a dashboard and story in Tableau showing the benchmarks and summary of model's measure. 
 Use  extensively like R, Python, ODS, DB2, Metadata, MS Excel etc. to analyze data frommultiple perspectives and was able to provide a robust Machine Learning algorithm. 
 Created new  and business processes that simplify, standardize and enables operationalexcellence. 
 Used  like Tableau for drilling-down data, creating insightful reports and garnering actionablebusiness insights. 
 
Environment: Tableau report builder, MS Outlook, SQL Server 2012/2014, Python (Scikit-Learn, NumPy, Pandas, Matplotlib, Dateutil, Seaborn), Tableau, Hadoop.
Education

Master's in information systems in information systems
University of Texas at Arlington - Arlington, TX
Skills

Cassandra, Hdfs, Mapreduce, Kafka, Db2, Hadoop, Machine learning, Nlp, Deep learning, Logistic regression, Neural nets, Random forest, Api, Git, Hadoop, Hive, Mapreduce, Pig, Python, Flask
Additional Information

TECHNICAL SKILLS: 
Machine Learning Neural Nets, Deep Learning, NLP, Recommendation Systems, IoT 
Software Development Agile, Scrum, Jira, Wiki, Git, SVN, AWS, Predix, Microsoft Azure, Third Party API integration, Unit Testing, Code coverage 
Database MySQL, MSSQL, DB2, PostgreSQL, Cassandra, HDFS 
Python Scipy, NumPy, IPython, Scikit-learn, Pyspark, Pandas, Flask, Tensor flow, keras 
R Recommender lab, Random forest, glm, rpart, xgboost 
SAS Logistic Regression, Decision Tree, Proc 
Visualization Tableau, PowerBI, ggplot, matplotlib 
Operating System Windows, Linux, Unix, Macintosh HD, Red Hat 
Data Modeling  Erwin r 9.6, 9.5, 9.1, 8.x, Rational Rose, ER/Studio, MS Visio, SAP Power designer 
Hadoop Ecosystem Hadoop, Hive, HDFS, MapReduce, Pig, Kafka",Data Scientist,emailed,indeed job post,resume
" 
Over 6+ years of experience in areas including Data Analyst, Statistical Analysis, Machine Learning, Deep Learning with large data sets of structured and unstructured data in travel services, strong functional knowledge, business processes, and latest market trends and manufactory industries. 

* Developed predictive models using Decision Tree, Random Forest, Nave Bayes, Logistic Regression, Cluster Analysis, and Neural Nets. 
* Involved in the entire data science project life cycle and actively involved in all the phases including data extraction, data cleaning, statistical modeling, and data visualization with large data sets of structured and unstructured data, created ER diagrams and schema.
* Expert in the entire Data Science process life cycle including Data Acquisition, Data Preparation, Data Manipulation, Feature Engineering, Machine Learning Algorithms, Validation, and Visualization.
* Strong knowledge in Statistical methodologies such as Hypothesis Testing, Principal Component Analysis (PCA), Sampling Distributions and Time Series Analysis.
* Proficient in Python and its libraries such as NumPy, Pandas, Scikit-Learn, Matplotlib and Seaborn.
* Expert in prepossessing data in Pandas using visualization, data cleaning and engineering methods such as looking for Correlations, Imputations, Scaling and Handling Categories
* Experience in building various machine learning models using algorithms such as Linear Regression, Gradient Descent, Support Vector Machines (SVM), Logistic Regression, KNN, Decision Tree, Ensembles such as Random Forrest, AdaBoost, Gradient Boosting Trees.
* Experienced the full software lifecycle in SDLC, Agile, and Scrum methodologies. 
* Strong SQL programming , with experience in ing with functions, packages, and triggers. 
* Excellent understanding of machine learning techniques and algorithms, such as k-NN, Naive Bayes, SVM, Decision Forests, natural language processing (NLP), etc. 
* Experienced in Data Integration Validation and Data Quality controls for ETL process and Data Warehousing using MS Visual Studio SSIS, SSAS, SSRS. 
* Expert in developing Data Conversions/Migration from Legacy System of various sources (flat files, Oracle, Non-Oracle Database) to Oracle system Using SQL LOADER, External table and Calling Appropriate Interface tables and API's Informatica. 
* Strong ing experience on Teradata query performance tuning by analyzingCPU, AMP Distribution, Table Skewness, and IO metrics.
* Excellent Data Mining  who can sift the grain from large datasets of Structured and Unstructured data, identify the patterns within data, analyze data and interpret results into actionable insights and business values.
* Good understanding of ing on Artificial Neural Nets and Deep Learning models using Theano and Tensor Flow packages using in Python.
* Extensive ing experience with Python including Scikit-learn, SciPy, Pandas, and NumPy developing machine learning models, manipulating and handling data. 
* Transformed traditional environment to virtualized environments with AWS-EC2, S3, EBS, ELB, and EBS. 
*  to build a fully automated, highly elastic cloud orchestration frame on AWS.
* Extensively ed on Teradata Utility  like BTEQ, Fast Load, Fast Export, Multi-Load, TPUMP, and TPT.
* Experience in developing and analyzing data models, involved in writing simple and complex SQL queries to extract data from the database for data analysis and testing 
* Strong knowledge in all phases of the SDLC (Software Development Life Cycle) from analysis, design, development, testing, implementation, and maintenance with timely delivery against deadlines 
* Proficient knowledge of statistics, mathematics, machine learning, recommendation algorithms and analytics with an excellent understanding of business operations and analytics  for effective analysis of data.
* Excellent communication . Successfully ing in fast-paced multitasking environment both independently and in a collaborative team, a self-motivated enthusiastic learner. 



Bachelor of Engineering.

 


Programming & Scripting Languages 
Python, C++, C, Java.
Databases
MS-Access, Oracle 12c/11g/10g/9i,Mysql,DB2
BI 
Microsoft Power BI, Tableau, SSIS, SSRS, SSAS, Business Intelligence Development Studio (BIDS), Visual Studio, Crystal Reports, Informatica 6.1.
Machine Learning Libraries
TensorFlow, Keras, PyTorch, NumPy, OpenCV, Scikit-Learn, SciPy, Pandas
Data Modelling 
Erwin r 9.6, 9.5, 9.1, 8.x, Rational Rose, ER/Studio, MS Visio, SAP Power designer. 
Teradata Utilities
BTEQ, Fast Load, Fast Export, Multi-load, TPUMP, and TPT 
Database 
Toad, SQL Developer, PL/SQL Developer, SQL Developer, Informatica Power Center 9.5.1.
Operating Systems 
Windows (10, 7, Vista), XP, UNIX, Linux. 
Project 1                                                                                                                                                                      Data Scientist                                                                                                                                                                                                                                       
Automotive Lighting,Auburn hills, Michigan.    			Apr 2018 - Till Date

Description:Automotive Lighting (AL) is a German-based company founded in 1999 as a 50-50 joint venture between the Italian Magneti-Marelli and the German Robert Bosch GmbH. In 2001 MagnetiMarelli raised its share to 75% after the acquisition of Seima Group. 
Responsibilities:
* Writing complex SQL queries for validating the data against different kinds of reports generated by Cognos.
* Designed an object detection program by utilizing Python, YOLOmodel, and TensorFlow to identify objects by drawing the boundaries of each identified object in an image.  
* Filtered the discovered boundaries by implementing a non-max suppression algorithm to achieve an optimal bounding box per identified object.
* Evaluated models using Cross-Validation, Log loss function, ROC curves and used AUC for feature selection and elastic  like Elasticsearch, Kibana,etc
* Addressed overfitting by implementing the algorithm regularization methods like L2 and L1.
* Implemented statistical modeling with XGBoostmachinelearning software package using Python to determine the predicted probabilities of each model.
* Created master data for modeling by combining various tables and derived fields from client data and students LORs, essays and various performance metrics.
* Formulated a basis for variable selection and GridSearch, KFold for optimal hyperparameters
* Used NumPy, SciPy, pandas, NTLK(Natural Language Processing Toolkit),matplotlib to build the model.
* Formulated several graphs to show the performance of the students by demographics and their mean score in different USMLE exams.
* Application of various Artificial Intelligence(AI)/machine learning algorithms and statistical modeling like decision trees, text analytics, natural language processing(NLP), supervised and unsupervised, regression models.
* Performed Data Cleaning, features scaling, features engineering using pandas and NumPy packages in python and build models using deep learning frames.
* Created deep learning models using TensorFlow and Keras by combining all tests as a single normalized score and predict residency attainment of students.
* Used XGB classifier if the feature is a categorical variable and XGBregressor for continuous variables and combined it using FeatureUnion and FunctionTransfomer methods of Natural Language Processing.
* Created data layers as signals to Signal Hub to predict new unseen data with performance not less than the static model build using deep learning frame.
* ed with the Data Governance group in creating a custom data dictionary template to be used across the various business lines.
* Create statistical models using distributed and standalone models to build various diagnostics, predictive and prescriptive solution.
* Interface with other  teams to load (ETL), extract and transform data from a wide variety of data sources
* Provides input and recommendations on  issues to Business & Data Analysts, BI Engineers, and Data Scientists.
Environment: Python 2.x,3.x, Hive, AWS, Linux, Tableau Desktop, Microsoft Excel, NLP, Deep learning frames such as TensorFlow, Keras, Boosting algorithms, DB2, R, Python, Visio, HP ALM, Agile.

Project 2                                                                                                                                                                      Data Scientist                                                                                                                                                                                                                                       
Theron  Solutions, Chicago.ILJun 2017-March 2018

Description:Theron  Solutions. LLC provides customers with one strategic team that partners to audit, design, build and maintain purpose-built front and back-end solutions that meet high-growth companies' distinct needs.
Responsibilities:
* Analyzed and solved business problems and found patterns and insights within structured and unstructured data.
* Implemented advanced computer vision techniques like distortion correction, thresholding techniques, and the sliding window method to identify the lane markings to highlight the entire lane.
* Tested the algorithm in a video to ensure that the lane boundaries are accurately identified.
* Utilized a diverse array of  and  as needed, to deliver insights such as R, SAS, MATLAB, Tableau and more.
* Detected near-duplicated news by applying NLP methods and developing machine learning models like label spreading and clustering.
* Employed the output of the semantic segmentation to perform drivable space estimation in 3D, lane estimation and to filter errors in the output of the 2D object detectors.
* prototyping and experimenting with Algorithms and integrating into a production system for different business needs.
* Implemented Porter Stemmer (Natural Language Tool Kit) and NLP bag of words model (Count Vectorizer) to prepare the data.
* Implemented number of customer clustering models and these clusters are plotted visually using Tableau legends for the higher management.
* Used T-SQL queries to pull the data from disparate systems and Data warehouse in different environments.
* ed closely with the Data Governance Office team in assessing the source systems for project deliverables.
* Developed SQL procedures to synchronize the dynamic data generated from GTID systems with the Azure SQL Server.
* Process automation using Python/R scripts with Oracle database to generate and write the results in the production environment on a weekly basis.
* Used Data Quality validation techniques to validate Critical Data elements (CDE) and identified various anomalies.
* Performing Data Validation / Data Reconciliation between the disparate source and target systems for various projects.
* Writing complex SQL queries for validating the data against different kinds of reports generated by Cognos.
* Provides input and recommendations on  issues to Business & Data Analysts, BI Engineers, and Data Scientists.

Environment: SAS, R, MLIB, Python, Data Governance, MDM, MATLAB, Tableau,Azure SQL Server.

Project 3                                                                                                                                                                      Data Scientist                                                                                                                                                                                                                                       
Global Atlantic Financial Group, Brighton, MA.                                                                                Oct 2016-May 2017

Description:Global Atlantic Financial Group, through its subsidiaries, offers a broad range of retirement, life and reinsurance products designed to help our customers address financial challenges with confidence. A variety of options help Americans customize a strategy to fulfill their protection, accumulation, income, wealth transfer and end-of-life needs.
Responsibilities:
* Used various approaches to collect the business requirements and ed with the business users for ETL application enhancements by conducting various JRD sessions to meet the job requirements
* Designed data profiles for processing, including running PL/SQL queries and using R for Data Acquisition and Data Integrity which consists of Datasets Comparing and Dataset schema checks
* Performed exploratory data analysis like calculation of descriptive statistics, detection of outliers, assumptions testing, factor analysis, etc., in R
* Applied Clustering Algorithms such as K-Means to categorize customers into certain groups
* Implemented Key Performance Indicator (KPI) Objects, Actions, Hierarchies and Attribute Relationships for added functionality and better performance of SSAS Warehouse
* Used Tableau and designed various charts and tables for data analysis and creating various analytical Dashboards to showcase the data to managers
* Performed data management, including creating SQL Server Report Services to develop reusable code and an automatic reporting system and designed user acceptance test to provide end with an opportunity to give constructive feedback.

Environment:R/R Studio, SAS, Oracle Database 11g, Oracle BI , Tableau, MS-Excel.


Project 4                                                                                                                                                                        Data Analyst                                                                                                                                                                                                                                       
Sterling Insurance, Brentwood, TN.						Oct 15 - Sep 2016

Description: Since 1895, the mission of Sterling Insurance Company has been to provide unexcelled service and sound insurance protection to its policyholders. And for over a century, weve delivered on our promise. In our first one hundred years, weve succeeded by doing two things exceedingly well. First, weve built a solid financial base on which our policyholders can rely.

Responsibilities:
* Participated in all phases of data acquisition, data cleaning, developing models, validation, and visualization to deliver data science solutions.
* Retrieving data from SQLServer database by writing SQL queries like stored procedure, temp table, view.
* ed with the DBA group to create a Best-Fit Physical Data Model from the Logical Data Model using Forward engineering using Erwin.
* Connected Database with Jupyter notebook for Modeling and Tableau for visualization and reporting.
* ed on fraud detection analysis on loan applications using the history of loan taking with supervised learning methods.
* Used Pandas, NumPy, Seaborn, Matplotlib, Scikit-learn in Python for developing various machine learning models and utilized algorithms such as Logistic Regression, Random Forest, Gradient BoostDecisionTree, and NeuralNet.
* Experienced in performing feature engineering such as PCA for high dimensional datasets, important feature selection by Tree-based models.
* Perform model tuning and selection by using cross-validation, parameters tuning to prevent overfitting.
* Ensemble methods were used to increase the accuracy of the training model with different Bagging and Boosting methods.

Environment: SQL Server 2008, Python 2.x (NumPy/Pandas/Scikit-Learn), GitHub.

Project 5                                                                                                                                                                      Data Analyst
Amity IT Solutions, Hyderabad, India.                                                                                               May 2013 - Sep 2015

Description: A Gwalior based IT Solutions and Software Services Development Company. Providing cost-effective, innovative and robust business solutions. We focus on combining web-based application development, market-savvy design, and reliable business strategy to advance a clients business goals. 
Responsibilities:
* Involved in Data mapping specifications to create and execute detailed system test plans. The data mapping specifies what data will be extracted from an internal data warehouse, transformed and sent to an external entity.
* ed closely with stakeholders to understand, define, document business questions needed.
* Review system/application requirements (functional specifications), test results and metrics for quality and completeness.
* Designed and Developed Oracle PL/SQL Procedures and UNIXShellScripts for Data Import/Export and Data Conversions.
* Have Used InformaticaData Quality as an ETL tool to transform the data from various sources and bring them into one common format and load them into the target database for the analysis purpose from Data Warehouse.

Environment: Oracle PL/SQL, UNIX Shell Scripts, Data Import/Export, Informatica Data Quality.


",Data Scientist,emailed,indeed job post,resume
" 

University of California at Berkeley  	 	 	 	                Graduated: May 2017 
Master of Arts in Statistics 
 GPA: 3.72/4.00 
 Course: Advanced Probability, Advanced Statistics, Statistical Computing with R, Linear Models, Time Series 
 
University of California at Santa Barbara                                                     Graduated: June 2013 Bachelor of Science in Financial Mathematics and Statistics 
 Cumulative GPA: 3.96/4.00 
 Relevant Course: Stochastic Processes, Numerical Analysis, Microeconomic Theory, Partial 
Differential Equations and Fourier Series, SAS Base Programming 
 Experience 

Applied Underwriters, Foster City, CA  	 	 	 	    June 2018  September 2018 
Financial Analyst Intern 
 Maintained and Updated data files relating to rate filings. 
 Helped in evaluating terrorism risk for client locations. 
 
Project Experience 

Portfolio Optimization Project                                                                        Feb 2017  May 2017 
 Studied the effectiveness of Portfolio Optimization Techniques, starting with those created by Harry Markowitz. 
 ed with a small team to try and make a portfolio that would outdo a naive portfolio. 
 Used convex optimization with 5 years worth of monthly stock data to create an optimal portfolio, then measured its future performance. 
 Performed convex optimization using cvxpy package in Python. 
Honors 

Rama Thogarati Memorial Award                                                                                             2013 
 	Awarded each year to a Senior Undergraduate at UCSB for academic excellence in the Department of Statistics and Applied Probability. 
  

Programming :  R, Python, SAS, C, MATLAB, Latex, Excel, SQL 
 ",Data Scientist,emailed,indeed job post,resume
"Willing to relocate: Anywhere
 Experience

Student Specialist III
University of Alabama in Huntsville - Huntsville, AL May 2017 to December 2018
Office of Information : Re-imaging of instructor machines and installing a new Operating System on the PC. Triaged and resolved numerous tickets related to Software Issues on the Instructor machines. 
Online Learning: Edited tests and quizzes for online courses on Sociology, Mythology, History and Management. Captioned lecture videos for improved accessibility. Designed posters for events and created online forms for surveys.
Event Manager
May 2015 to May 2015
Managed multiple events like singing, cooking and electrical engineering competitions while coordinating with volunteers and competitors. All events were successful without delays.
Student Volunteer
HELPING HANDS OF HUNTSVILLE - Huntsville, AL
Cleaned up lake in Huntsville and university duck pond .Helping Hands of Huntsville is a fall service tradition. It allows new and current students to engage in direct service  with a variety of focuses to help students understand the importance of giving back.


M.S. in Electrical Engineering
University of Alabama in Huntsville - Huntsville, AL December 2018
Visvesvaraya Technological University May 2016


Dimensionality reduction, Machine learning, Pca, Supervised learning, Big data analytics, Clustering,
Data analytics, Mapreduce, Python, Matplotlib, Numpy, Pandas, Ec2, Sql
Links

Additional Information

. 
Machine Learning: Regression, Classification and Clustering. 
Big Data Analytics: Dimensionality reduction like PCA, LDA. Supervised and un-Supervised Learning for feature engineering. 
 Programming Language: Python (NumPy, Pandas, Matplotlib, SciKit-Learn). 
 SQL querying. 
 Linux 
 Tableau, AWS's S3 and EC2. 
 Hadoop HDFS and MapReduce.",Data Scientist,emailed,indeed job post,resume
"Im searching for a savvy company to leverage my proven data science and public relations  in multiple flows including Finance, Chain Supply, Sales, Charity, Social Media Marketing and many other departments.  
 
Todays business-to-business and business-to-consumer cultures survive and thrive on valuable insights drawn from data analytics and innovative thinking. Id like to be part of your team. 
 
I am confident that my talents will be a positive addition to your organization. I appreciate your time and attention for this new opportunity.
Authorized to  in the US for any employer
 Experience

Senior Data Scientist
Western States Pharmacy Coalition - Lake Forest, CA
February 2018 to May 2019
Highly experienced business insights and analytics  with expert presentations  who delivers clear and concise operational, tactical, and strategic initiatives to increase profits.  
 
Innovative data scientist (mining, tracking and visualization) with growth minded problem solving  identifies simple solutions to leverage remarkable results. 
 
 Achieved $14 million in savings via customized NDC comparison database. 
 Identified more than 80K prospects; filtered to 7K high value targets.       Increased revenue $3 million with $24.8 million on deck. 
 
Created and streamlined 71 reports for Sales, Marketing, and Finance to help independent pharmacies maintain and increase profits.
Senior Analyst, Business Insights & Analytics
AmerisourceBergen - Orange, CA
February 2001 to November 2017
 Generated more than $2.1 Billion in revenue. 
 Saved more than $420 Million in operating expenses. 
 : Automation 45; Procedures 51; Templates 59  all still used today 
 
Promoted multiple times and remained successful through mergers, acquisitions, multiple platform and software conversions. 
 
Provided senior leadership and key decision makers with information to support numerous strategic initiatives and strategies. 
 
Interpreted business reports, trend analyses and projections to create action plans for segmentation sales teams to offer mutually profitable solutions. 
 
Streamlined, designed, developed and implemented best practices to improve business operations efficiencies and profitability for Supply Chain and Finance. 
 
Oversaw Reporting, Data Governance and Analytics teams to ensure day-to-day activities and valuable insights were delivered. 
 
Conducted hundreds of competitive pricing comparisons and dispensing data analyses for new business (request for proposals) to strategic accounts. 
 
Provided formal and informal trainings and shops for internal and external customers.


High school or equivalent


Tableau Desktop (2 years), Business Objects Advanced Reporting (10+ years), Salesforce (6 years),
Sigma Six (3 years), Microsoft Office Certifications (Excel, Access, Word, Outlook, and PowerPoint) (10+ years)
Links

Certifications/Licenses

Tableau Desktop Training I
June 2018 to Present
Training from Tableau
Tableau Prep Training I
December 2018 to Present
Training from Tableau
Tableau Calculations, Maps, and Visualizations
February 2018 to Present
Training via oft
Business Objects Advanced Reporting
August 2016 to Present
Training from SAP
SalesForce.com Wave Analytics
August 2016 to Present
Training from SalesForce.com
Business Objects Reporting
April 2016 to Present
Training from SAP
Six Sigma (WB)
April 2016 to Present
Training from Aveta Business Institute
SalesForce.com Reporting
April 2013 to Present
Training from SalesForce.com
Distinguished Toastmaster
December 2010 to Present
Training from Toastmasters International
Microsoft 2007 Expert (Excel, Access, Word, Outlook and PowerPoint)
September 2009 to Present
Certification from Microsoft
MicroStrategy Narrowcast Developer and Web Interface Designer
August 2005 to Present
Training from MicroStrategy
MicroStrategy SQL Essentials and Engine Specialist Track
June 2005 to Present
Training from MicroStrategy
Master Microsoft Office User Specialist (MOUS) 2000
February 2002 to Present
Certification from Microsoft
Microsoft 2000 Expert (Excel, Access, Word, Outlook and PowerPoint)
February 2002 to Present
Certification from Microsoft
Master Microsoft Office User Specialist 1997
April 2001 to Present
Certification from Microsoft
Microsoft 1997 Expert (Excel, Access, Word, Outlook and PowerPoint)
April 2001 to Present
Certification from Microsoft
Assessments

Spreadsheets with Microsoft Excel  Highly Proficient
June 2019
Excel knowledge including common , PivotTables, conditional & nested formulas, and custom visuals.
Full results: https://share.indeedassessments.com/share_assignment/wmq6e7p3iosi8c-u
Indeed Assessments provides  tests that are not indicative of a license or certification, or continued development in any  field.
Additional Information

Additional Career, Contract and Project Experience  
 
Childrens Hospital of Orange County CHOC, U.S. Healths, Kaiser Permanente, Proctor & Gamble,
Book Publicist of Southern California, Toastmasters International, American Red Cross and Mensa. 
 
Core Competencies 
 Excellent communication, leadership, time management and organizational . 
 Flexible to  effectively solo, as part of a team, or manage large or small groups. 
 Committed to quality process improvement methods, such as SMART, Six Sigma, PDCA,
Demming, KPI Metrics, POC, modeling and others. 
 Certified in Microsoft Office Suite  Excel, Access, Outlook, Word and PowerPoint. 
 Savvy Social Media Campaigns, MailChimp, Google Business Analytics, Facebook Insights, GoTo
Meeting, Zoom Administrator, and Webinars. 
 Knowledgeable about predictive models, statistical analysis, data warehouse and visualization: Business Objects, PowerBI, Qlik, Tableau, SalesForce, MicroStrategy and more 
 
Instructor 
Chapman University 
University of Phoenix 
Whittier College 
 
Talents 
Magic Castle Magician Member 
Distinguished Toastmaster 
Successful Inventor 
Published Author 
Host of FDTV 
Webmaster 
 
MAJOR ACCOMPLISHMENTS LAST 10 YEARS 
 
One Fiscal Year  
 Generated more than $2.1M in revenue opportunity. 
 Saved more than $500,000 in operating expenses. 
 Saved more than 21,000 man-hours (137 associates in 18 departments). 
 
High Level  
 Created 3 revenue generating reports totaling $2M for strategic accounts. 
 $930,000 annual opportunity  Health Systems B2G ACAP pilot. 
 $864,000 annual opportunity  IPBG Blue Box Opportunity Report. 
 $288,000 annual opportunity  Ingles Alt Size Sub Option B. 
 Saved more than 900 hours and more than $20,000 in operating expenses. 
 Streamlined 27 internal processes to be more efficient. 
 Automated 64 reports in Business Objects Reporting Tool. 
 Trained 14 associates across 6 departments. 
 
Local and Enterprise-wide Awards and Achievements, 20072017 
Individual 206 + Team 129 = Total 335 
 
KEY  (Alpha Order) 
 
? AHP Track n Trend 
? AHP Unit Dose Initiative 
? Automate NTI-GPN Opt Outs 
? Automated Weekly Updates 
? B2G Safe ACAP 
? BOBJ and DAX Formulas 
? BOBJ Scheduler 
? Brand to Generic Projections 
? CCE Reports and Templates 
? Cost of Goods Savings 
? Cost of Shopping 
? Costco Inventory Management 
? CTCA Pricing Analysis 
? Customer Compliance Tool 
? Email Etiquette 
? Estimator Calculator 
? Excel Learning Lab 
? FDB-ABC-PRxO Equivalents 
? FTS and ACAP Statistics 
? GPIs for RFPs 
? Kaboom! Playground 
? Key Shortage Report 
? Large Bottle Savings 
? Margin Analysis Template 
? Profit Per Script 
? QRC and Training 
? Rebate Comparison Model 
? Rebate Value Proposition 
? Save Big Buy Big 
? Substitution Resolution 
? Total Value Scorecard 
 ",Data Scientist,emailed,indeed job post,resume
"Data and research scientist with Physics PhD and machine learning certificate. Self-motivated, detail oriented, quick learner, and a team player. Able to think out of the box when tackling new challenges.
Willing to relocate: Anywhere
Authorized to  in the US for any employer
 Experience

Postdoctoral scholar
University of California Irvine - Irvine, CA September 2016 to July 2018
Repair/rebuilt instruments, designed experiments, data acquisition, signal processing, and image analysis.
Lecturer
University of California Irvine - Irvine, CA 2016 to 2018
Postdoctoral researcher
Physics Department, Harvard
September 2014 to September 2015
Designed and implemented experiments, and data analysis using Igor in a fast-paced environment.
Research Assistant
Physics Department, UMASS Amherst
September 2009 to May 2014
Developed an automated experimental setup, built and tested models, data mining using Matlab. 
20+ peer-reviewed publications, presentations, and conference papers(google scholar). 
Successful collaborations with 30+ scientists with chemistry and material science backgrounds.


Doctor of Philosophy in Physics
University of Massachusetts Amherst - Amherst, MA
May 2014
M. Sc. in Physics
University of Tehran - Tehran, IR May 2009
B. Sc. in Physics
Amirkabir University - Tehran, IR May 2006


Deep learning, Linear regression, Logistic regression, Machine learning, Random forest, Svm, Hadoop,
Html, Javascript, Python, Keras, Matplotlib, Numpy, Vba, Data manipulation, Mysql, Clustering, Hadoop,
Mongodb, Matlab
Links


Certifications/Licenses

Data Bootcamp, UCI
April 2019 to Present
Machine Learning, Coursera
July 2019 to Present
Additional Information

  
- Programing and Data Manipulation: Python(pandas, numpy, matplotlib, SQLAlchemy, seaborn,sklearn, keras), VBA, MATLAB, IGOR, familiar with R, Hadoop, Spark 
- Machine Learning Techniques: Linear Regression, Classification (Logistic Regression, Decision Tree, 
KNN, SVM), Random Forest, Clustering, and Deep learning. - Visualization: Tableau, HTML, Javascript, Leaflet, Geomap. - Databases: MySQL, mongoDB.",Data Scientist,emailed,indeed job post,resume
"Data Analyst Intern at V-ETS with 4 years of experience in IT. Skilled in Machine learning, python, data
 
science and database functioning. Committed to helping organizations advance by helping them to develop strategic plans based on predictive modeling and findings.
Willing to relocate: Anywhere
Authorized to  in the US for any employer
 Experience

Data Analyst Intern
V-ets - Washington, DC
June 2019 to Present
Project:  
 Building an application which converts audio conversations between medical practitioner andpatients into text.  
 Improve the accuracy of the conversion using machine learning techniques. 
 
: AWS Transcribe, AWS S3, EC2, AWS RDS, Speech to Text, Python, Project Management, NLP 
Senior System Engineer(Machine Learning domain)
Infosys Limited - Mysore, Karnataka
January 2015 to May 2018
Job Function Machine Learning, Data Analytics, Python, PLM domain 
 
Project Fraud risk prediction of online money transfer. 
 
Impact Model with 94% accuracy to save $ 1 million per annum. 
 Led a team of 4 and executed a project to predict money transfer frauds for a South Asian bankingfirm with a customer base over 10 million customers. 
 Aggregated data from Hadoop cluster and dumped into MySql database. Interfaced dB with Jupyternotebook, performed data transformation using complex SQL queries. 
 Visualizations using MATPLOTLIB and SEABORN libraries for effective client communication. 
 Used Pivot tables and lookup functions in MS Excel for weekly report preparation. 
 Applied Logistic regression, Random forest and Neural nets to predict fraud. 
 LASSO variable selection, exploratory data analysis to shortlist from 370 variables to a smallerintuitive set of 39 features. 
 Implemented Feature Selection (Chi -Square, p-Value for finding association among dependent,independent variables) 
 Data Transformation (Data Preprocessing- Handling missing values), Interactive Decision Tree.  Assisted product team in hosting a web based app. which was interfaced with the Statistical model built by our team. 
 
Project Duplicate Ads detection for one of India's largest online B2B marketplace 
 
Impact Reduced data storage cost, duplication rate abridged to 2.4% from 11%. 
 Data cleaning, stemming using NLTK Snowball to remove stop words/punctuations. 
 Computed TF-IDF similarities for text, hash similarities for images, nGrams Features for title anddescription, Fuzzywuzzy distances 
 Applied XGBoost (94% accuracy) on 587 final features with 1000 estimators, replaced NAN. 
 Applied Nnet (91% accuracy) with 3 hidden layers, 800 hidden units with 60% drop out. 
 Used ensemble of models to take a weighted average for final predictions of duplicate Ad's 
 
Project Enovia(Matrixone) upgrade to 3DEX 2017 
 
 End to End implementation: Analysis, Design, Development and Maintenance.  Performed unit testing, code review, QA, UAT and production support. 
 
Project Application Rehosting (Advitium) 
 Installation of application from scratch in HP data center (Windows 2008 R2) 
 Implemented knowledge of SMTP, LDAP, Active Directory, Ports, Load Balancer during datamigration.
System Engineer Trainee
Infosys - Mysore
July 2014 to December 2014
Learning: Java, Python, Data Mining, Data base(SQL), HTML & CSS as a part of generic training 


Master's in Data Science
University at Buffalo - Buffalo, NY August 2018 to Present
Bachelor's in Mechanical Engineering
College of Engineering Roorkee - Roorkee, Uttarakhand
August 2010 to July 2014


Data analysis, Sql server, Sql, Decision trees, Lda, Logistic regression, Pca, Svm, Bayesian, Hadoop,
Predictive analytics, C++, Hadoop, Python, Ggplot2, Keras, Matplotlib, Numpy, Pandas, Tensorflow,
Excel, Python (3 years), R (2 years), AWS (1 year), Tableau (1 year)
Links

Certifications/Licenses

Neural Nets and Deep Learning by deeplearning.ai on Coursera
June 2019 to Present
- Understand the major  trends driving Deep Learning 
- Be able to build, train and apply fully connected deep neural nets  
- Know how to implement efficient (vectorized) neural nets  
- Understand the key parameters in a neural net's architecture
Marketing Analytics in r statistical modeling
March 2019 to Present
Supervised Learning with scikit-learn
January 2019 to Present
Unsupervised Learning in Python
January 2019 to Present
Intermediate Python for Data Science
June 2018 to Present
Intro to Python for Data Science
June 2018 to Present
Google Analytics for Beginners March 2019 to March 2022
Improving Deep Neural Nets Hyperparameter tuning, Regularization and Optimization
July 2019 to Present
This course will teach you the ""magic"" of getting deep learning to  well. Rather than the deep learning process being a black box, you will understand what drives performance, and be able to more systematically get good results. You will also learn TensorFlow.  
 
After 3 weeks, you will:  
- Understand industry best-practices for building deep learning applications.  
- Be able to effectively use the common neural net ""tricks"", including initialization, L2 and dropoutregularization, Batch normalization, gradient checking,  
- Be able to implement and apply a variety of optimization algorithms, such as mini-batch gradientdescent, Momentum, RMSprop and Adam, and check for their convergence.  
- Understand new best-practices for the deep learning era of how to set up train/dev/test sets andanalyze bias/variance 
- Be able to implement a neural net in TensorFlow.  
 
This is the second course of the Deep Learning Specialization.
Structuring Machine Learning 
July 2019 to Present
You will learn how to build a successful machine learning project. If you aspire to be a  leader in AI, and know how to set direction for your team's , this course will show you how. 
 
Much of this content has never been taught elsewhere, and is drawn from my experience building and shipping many deep learning products. This course also has two ""flight simulators"" that let you practice decision-making as a machine learning project leader. This provides ""industry experience"" that you might otherwise get only after years of ML  experience. 
 
After 2 weeks, you will:  
- Understand how to diagnose errors in a machine learning system, and  
- Be able to prioritize the most promising directions for reducing error 
- Understand complex ML settings, such as mismatched training/test sets, and comparing to and/orsurpassing human-level performance 
- Know how to apply end-to-end learning, transfer learning, and multi-task learning 
 
I've seen teams waste months or years through not understanding the principles taught in this course. I hope this two week course will save you months of time. 
 
This is a standalone course, and you can take this so long as you have basic machine learning knowledge. This is the third course in the Deep Learning Specialization.
Assessments

Data Analysis  Proficient
April 2019
Measures a candidate's skill in interpreting and producing graphs, identifying trends, and drawing justifiable conclusions from data.
Full results: https://share.indeedassessments.com/share_assignment/4n1d8najsxiyrffw
Problem Solving  Expert
April 2019
Analyzing relevant information when solving problems.
Full results: https://share.indeedassessments.com/share_assignment/aaf2omtxquk4l9jp
Critical Thinking  Expert
June 2019
Using logic to solve problems.
Full results: https://share.indeedassessments.com/share_assignment/y7nb5ttzrdb9nkxn
Indeed Assessments provides  tests that are not indicative of a license or certification, or continued development in any  field.
Additional Information

ACADEMIC  : 
 
Project: Cervical cancer risk factors for classification (Kaggle). 
 
Description: Predict cancer risk based on features related to medical domain. 
  
Stages Data: Pre-processing (replacing NA/? values, normalizing, oversampling data), Data
Visualization using R, Classification Models (XGBoost (cross validation), Random forest, Confusion Matrix, Type II errors  
 
Results: Random forest (92.4% accuracy after oversampling) XGBoost (94.33% accuracy after oversampling, 100% specificity). 
 
Project: Kick-Starters (crowdfunding)  analysis (Kaggle). 
 
Description: Determine the most supported  and predict the success/ failure of future . 
 
Visualization Heat Map, World map, Word cloud, Box plots 
  
Models Logistic Regression, Gradient boosting, Random forest. 
 
  
Project: Petfinder.my Adoption Prediction(Kaggle). 
 
Description: Developing algorithms to predict adoptability of pets in Malaysia.  
 
Visualization: Bar plots, Distribution charts, Pie charts, Histogram, Word Cloud  
 
Models: CatBoost Classifier (70% accuracy without parameters tuning). ",Data Scientist,emailed,indeed job post,resume
" of : 
Positive and self-motivated Trilingual Russian, Japanese and English Experienced Licensed Massage Therapist, specializing in Chines Medical massage Tui Na, Swedish, Deep tissue, Hot Stones, Sport massage, Face lifting/acupressure, Chair, Prenatal, Cupping, Aromatherapy, Chakra Balance, Reiki looking to  at medical office 
as a massage therapist/chiropractor or acupuncture assistant or at spa. I have an experience at Coconuts, Sugar, Chocolate scrubs. I have management , teaching and translating/interpretations experience as well.
Authorized to  in the US for any employer
 Experience

Massage Therapist
Ichiban Massage Therapy - San Jacinto, CA
January 2018 to Present
Massages, scrubs.
Manager
Gprservice incorporated - Yokohama, Japan
October 2000 to Present
Developed export automotive products strategy and plan. Developed import products such Chaga
(health row material), strategy and plan. Developed import export and import wine strategy and plan. Interacted with Japanese, Russian and American customers and dealers through fax, skype,verbal and written communications to provide the information about the export and import products. Developed system implementations of planning, location sensitivity and vendor managed inventory, transportation planning and warehousing.Finding clients in Russia, Japan, USA, New Zealand and other countries. Goods descriptions, preparing contracts. Price negotiations. Select and interview job applicants. Maintaining good ing relationships with clients, vendors and government agency representatives. Attending trade shows.
Massage Therapist
Dr Wu Chiropractor - Chino, CA
January 2019 to September 2019
High quality deep tissue massages, pregnancy massages.
Massage Therapist
Dr Bark Acupuncture office - Fontana, CA
June 2019 to August 2019
30min,1hr massages. Deep tissue, Chinese medical massage.
Massage Therapist
Green spa - Chino Hills, CA
January 2019 to July 2019
Massage Therapy, pregnancy, scrubs, couple massages.
Massage Therapist
Southern California health clinic - Tarzana, CA June 2017 to December 2018
Massage Therapy.
Massage Therapist
Amore Day Spa - Temecula, CA
January 2018 to March 2018
Massages, scrubs.
Nursing Assistant
Sakura intermediated care - Los Angeles, CA
January 2017 to March 2017
Provides personal care, rehabilitative and supportive services. 
Assists in activities of daily living (personal hygiene, eating, dressing, ambulation). Supports the patient and provides care that encourages independence. Assists with medications. Vital signs.


Bachelor's in Doctor of acupuncture and oriental medicine
Pacific College of Oriental Medicine-San Diego - San Diego, CA
January 2018 to Present
CNA/EKG monitor technician in Nurse assistant/EKG monitor technician
St Jude Nursing school - Van Nuys, CA May 2015 to September 2015
Bachelor's in Management
Khabarovsk Economy university - Khabarovsk
September 1988 to May 1992


Deep Tissue, Marketing, Receptionist, Training, Swedish Massage, Microsoft (10+ years), Russian language (10+ years), Japanese language (10+ years), Management (10+ years), Inventory, Time Management, Fast learner, CPR, Outlook, Microsoft Word, Microsoft Excel, TuiNa Therapeutic massage
(5 years), Microsoft Office, Chinese medical massage Tui Na, Swedish massage, Pregnancy massages
Certifications/Licenses

Licensed Massage Therapist
CPR
Certified Nursing Assistant (CNA)
Massage Therapist",Data Scientist,emailed,indeed job post,resume
"* A data scientist  with 7 years of progressive experience in Data Analytics, StatisticalModeling, Visualization and Machine Learning. Excellent capability in collaboration, quick learning and adaptation. 
* Experience in Data mining with large datasets of Structured and Unstructured data, Data Acquisition,
Data Validation, Predictive modeling, Data Visualization. 
* Experience in integrating data, profiling, validating and data cleansing transformation and datavisualization using R and Python. 
* Theoretical foundations and practical hands-on  related to (i) supervised learning (linearand logistic regression, boosted decision trees, Support Vector Machines, neural nets, NLP), (ii) unsupervised learning (clustering, dimensionality reduction, recommender systems), (iii) probability & statistics, experiment analysis, confidence intervals, A/B testing, (iv) algorithms and data structures. 
* Extensive knowledge on Azure Data Lake and Azure Storage. 
* Experience in migration from heterogeneous sources including Oracle to MS SQL Server. 
* Hands on experience in design, management and visualization of databases using Oracle, MySQL andSQL Server. 
* In depth knowledge and hands on experience of Big Data / Hadoop ecosystem (MapReduce, HDFS,Hive, Pig and Sqoop). 
* Experience in Apache Spark, Kafka for Big Data Processing & Scala Functional programming. * Experience in manipulating the large data sets with R packages like tidyr, tidyverse, dplyr reshape, lubridate, Caret and visualizing the data using lattice and ggplot2 packages. * Experience in dimensionality reduction using techniques like PCA and LDA. 
* Intensive hands-on Boot camp on Data Analytics course spanning from Statistics to Programmingincluding data engineering, data visualization, machine learning and programming in R, SQL. 
* Experience in data analytics, predictive analysis like Classification, Regression, RecommenderSystems. 
* Good Exposure with Factor Analysis, Bagging and Boosting algorithms. 
* Experience in Descriptive Analysis Problems like Frequent Pattern Mining, Clustering, OutlierDetection. 
* ed on Machine Learning algorithms like Classification and Regression with KNN Model, Decision
Tree Model, Nave Bayes Model, Logistic Regression, SVM Model and Latent Factor Model. 
* Hands-on experience on Python and libraries like Numpy, Pandas, Matplotlib, Seaborn, NLTK, Sci-Kitlearn, SciPy. 
* Expertise and knowledge in TensorFlow to do machine learning/deep learning package in python. 
* Good knowledge on Microsoft Azure SQL, Machine Learning and HDInsight. 
* Good Exposure on SAS analytics. 
* Good Exposure in deep learning with Tensor flow in python. 
* Good Knowledge on Natural Language Processing (NLP) and Time Series Analysis and Forecastingusing ARIMA model in Python and R. 
* Good knowledge in Tableau, Power BI for interactive data visualizations. 
* In-depth Understanding in NoSQL databases like MongoDB, HBase. 
* Very good experience and knowledge in provisioning virtual clusters under AWS cloud which includesservices like EC2, S3, and EMR. 
* Experience and Knowledge in developing software using Java, C++ (Data Structures and Algorithms). 
* Good exposure in creating pivot tables and charts in Excel. 
* Experience in developing Custom Report and different types of Tabular Reports, Matrix Reports, Adhoc reports and distributed reports in multiple formats using SQL Server Reporting Services (SSRS). 
* Excellent Database administration (DBA)  including user authorizations, Database creation,Tables, indexes and backup creation. 
 
 SECTION 
 
Languages Java 8, Python, R Packages 
ggplot2, caret, dplyr, Rweka, gmodels, RCurl, C50, twitter, NLP, Reshape2, rjson, plyr, pandas, numPy, Seaborn, sciPy, matplot lib, sci-kit-learn, Beautiful Soup, Rpy2. 
 
Web  JDBC, HTML5, DHTML and XML, CSS3, Web Services, WSDL 
Data Modelling  Erwin r 9.6, 9.5, 9.1, 8.x, Rational Rose, ER/Studio, MS Visio, SAP Power designer 
Big Data  Hadoop, Hive, HDFS, MapReduce, Pig, Kafka 
Databases SQL, Hive, Impala, Pig, Spark SQL, Databases SQL-Server, My SQL, MS Access, HDFS,
HBase, Teradata, Netezza, MongoDB, Cassandra. 
Reporting  
MS Office (Word/Excel/Power Point/ Visio), Tableau, Crystal reports XI, Business Intelligence, SSRS, Business Objects 5.x/ 6.x, Cognos7.0/6.0. 
 
ETL  Informatica Power Centre, SSIS. 
Version Control  SVM, GitHub 
Project Execution Methodologies 
Ralph Kimball and Bill Inmon data warehousing methodology, Rational Unified Process (RUP), Rapid Application Development (RAD), Joint Application Development (JAD). 
 
BI  
Tableau, Tableau Server, Tableau Reader, SAP Business Objects, OBIEE, QlikView, SAP Business Intelligence, Amazon Redshift, or Azure Data Warehouse 
 
Operating System Windows, Linux, Unix, Macintosh HD, Red Hat
 Experience

Data Scientist
AT & T INC - Plano, TX
January 2019 to Present
This Project is developed for Roaming Command Center to minimize cost what we pay to different service providers when AT & T customers roaming to their Carriers by using Optimization techniques and Machine Learning models. 
Responsibilities: - 
  on all aspects of creating predictive models including collecting requirements, establishanalytics  plan, data exploration, cleansing, and preparation, identifying features, selecting machine learning algorithms, building and testing models, iteratively improving solutions. 
 Build multiple time series forecasting models using tsfresh, Prophet, ARIMA time series models topredict future usage of voice call, mobile data and SMS for different countries using forecasting and regression methods in machine learning. 
 Use Feature engineering to and create new time series features like rolling means, Date Timefeatures, Lag features to improve forecasting Model. 
 Evaluate Forecasting Model accuracy by Using RMSE and MAPE to check error rate between actualand predicted values. 
 Build Forecasting model to predict future usage of units by daily, weekly and monthly and used
Jenkins jobs for periodically run python script and store output in database. 
 Integrated Forecasting model with MongoDB using PyMongo to get data for model and storepredictions result from model. 
 Build Forecasting model pipeline to automate process. 
 Used Matplotlib library to plot graph for all countries and understand trend of usage of data andenhance model accuracy. 
 Build big event feature and holidays feature for model to improve model accuracy. 
 Used Rolling Window method to build forecasting model. 
 Perform Linear programing methods and models, Genetic Algorithm for optimization models to findoptimal solution by creating different constraints and  functions for cost minimization.  Extracted the data from hive tables by writing efficient Hive queries using PySpark. 
 Developed Production Document which includes data pipeline, database schema, Jenkins Jobspipeline and API application properties. 
 Follow Agile Craft for project development and to track project performance 
Environment: HDFS, Hive, Pyspark, Python, tsfresh, Prophet, ARIMA, Gradient Boosting Model, Random forest regressor, Linear programing, Optimization, Genetic algorithm, Agile.
Data Scientist
Capital Group - Los Angeles, CA March 2018 to December 2018
This project was to support auditing team and claim department to improve accounting accuracy and reduce risk of fraudulent activities via providing machine learning and modeling solutions to identify suspicious insurance claims. 
 
Claims severity prediction in real-time 
Built classification models to predict the fraudulent claims by severity in real-time reducing the time for execution from 6 hours to 4 seconds. Implemented the models as a predictive solution for finding the fraudulent claims for the credit disability and debt protection products. Forwarded the high-risk claims for further investigation. 
 
Text analytics for fraud prediction 
Executed topic modeling for finding different topics based on the notes made corresponding to the claimant's claim. Attributed the resulting topics to classify into fraud and not fraud categories. 
 
Risk assessment prediction 
Incorporated models built in Python and R into the business processes using clustering techniques to assess the risk involved with a customer. The models built are used in assessing the premiums amount required to be paid by the customer. 
 
Customer churn/attrition prediction 
Developed models that predict whether a customer's propensity to churn leveraging the information related to insurance policies, demographics, claims, related to the customer, payment frequency, home ownership status, household tenure etc. 
 
Responsibilities: 
* Implemented Data Exploration to analyze patterns and to select features using Python SciPy. * Built Factor Analysis and Cluster Analysis models using Python SciPy to classify customers into different target groups. 
* Built predictive models including Support Vector Machine, Random Forests and Nave Bayes Classifierusing Python Scikit-Learn to predict the personalized product choice for each client. 
* Using R's dplyr and ggplot2 packages performed an extensive graphical visualization of overall data,including customized graphical representation of revenue reports, specific item sales statistics and visualization. 
* Designed and implemented cross-validation and statistical tests including Hypothetical Testing,
ANOVA, Auto-correlation to verify the models' significance. 
* Designed an A/B experiment for testing the business performance of the new recommendationsystem. 
* Supported MapReduce Programs running on the cluster. 
* Evaluated business requirements and prepared detailed specifications that follow project guidelinesrequired to develop written programs. 
* Configured Hadoop cluster with Namenode and slaves and formatted HDFS. 
* Used Oozie flow engine to run multiple Hive and Pig jobs. 
* Participated in Data Acquisition with Data Engineer team to extract historical and real-time data byusing Hadoop MapReduce and HDFS. 
* Performed Data Enrichment jobs to deal missing value, to normalize data, and to select features byusing HiveQL. 
* Developed multiple MapReduce jobs in java for data cleaning and pre-processing. 
* Analyzed the partitioned and bucketed data and compute various metrics for reporting. 
* Involved in loading data from RDBMS and web logs into HDFS using Sqoop and Flume. 
* ed on loading the data from MySQL to HBase where necessary using Sqoop. 
* Developed Hive queries for Analysis across different banners. 
* Extracted data from Twitter using Java and Twitter API. Parsed JSON formatted twitter data anduploaded to database. 
* Launching Amazon EC2 Cloud Instances using Amazon Images (Linux/ Ubuntu) and Configuringlaunched instances with respect to specific applications. 
* Developed Hive queries for analysis, and exported the result set from Hive to MySQL using Sqoopafter processing the data. 
* Analyzed the data by performing Hive queries and running Pig scripts to study customer behavior. 
* Created HBase tables to store various data formats of data coming from different portfolios. 
* ed on improving performance of existing Pig and Hive Queries. 
* Created reports and dashboards, by using D3.js and Tableau 9.x, to explain and communicate datainsights, significant features, models scores and performance of new recommendation system to both technical and business teams. 
* Utilize SQL, Excel and several Marketing/Web Analytics  (Google Analytics, AdWords) in order tocomplete business & marketing analysis and assessment. 
* Used Git 2.x for version control with Data Engineer team and Data Scientists colleagues. * Used Agile methodology and SCRUM process for project developing. 
 
Environment: HDFS, Hive, Scoop, Pig, Oozie, Amazon Web Services (AWS), Python 3.x (SciPy, ScikitLearn), Tableau 9.x, D3.js, SVM, Random Forests, Nave Bayes Classifier, A/B experiment, Git 2.x, Agile/ SCRUM.
Sr. Data Scientist
PCCI Group - Dallas, TX
November 2016 to February 2018
PCCI provides services to healthcare industry which is characterized by its dynamism and the diversity of players by providing customized BPO solutions. Our services are designed to help healthcare insurers; medical device manufacturers, pharmaceutical companies and healthcare providers acquire new market shares and effectively support their current members. 
With a comprehensive understanding of your business, PCCI acts as a key point between your products/services and your members, delivering them a superior customer experience at every opportunity. 
We provide sales and customer service solutions. More specifically, our solutions can be customized to your organization's specific requirements, including: 
* Sales through programs enrollments 
* Customer service (Claim inquiries about billing & account) 
* Helpline services 
* Awareness campaigns 
* Appointment scheduling for labs, doctor offices, clinics visits 
* Data management 
* Technical support 
* Market research 
 
Responsibilities 
* Perform Data Profiling to learn about behavior with various features such as traffic pattern, location,Date and Time etc. 
* Extracted the data from hive tables by writing efficient Hive queries. 
* Performed preliminary data analysis using descriptive statistics and handled anomalies such asremoving duplicates and imputing missing values. 
* Analyze Data and Performed Data Preparation by applying historical model on the data set in
AZUREML. 
* Application of various machine learning algorithms and statistical modeling like decision trees, textanalytics, natural language processing (NLP), supervised and unsupervised, regression models, social net analysis, neural nets, deep learning, SVM, clustering to identify Volume using scikit-learn package in python, Matlab. 
* Exploring DAG's, their dependencies and logs using Airflow pipelines for automation. 
* Performed data cleaning and feature selection using MLlib package in PySpark and ing with deeplearning frames such as TensorFlow, Keras etc. 
* Conducted a hybrid of Hierarchical and K-means Cluster Analysis using IBM SPSS and identifiedmeaningful segments of customers through a discovery approach. 
* Develop Spark/Scala, Python, R for regular expression (regex) project in the Hadoop/Hiveenvironment with Linux/Windows for big data resources. Used clustering technique K-Means to identify outliers and to classify unlabeled data. 
* Evaluate models using Cross Validation, Log loss function, ROC curves and used AUC for featureselection and elastic  like Elastic Search, Kibana etc. 
*  with NLTK library to NLP data processing and finding the patterns. 
* Categorize comments into positive and negative clusters from different social neting sites using
Sentiment Analysis and Text Analytics. 
* Analyze traffic patterns by calculating autocorrelation with different time lags. 
* Ensure that the model has low False Positive Rate and Text classification and sentiment analysis forunstructured and semi-structured data. 
* Addressed over fitting by implementing of the algorithm regularization methods like L2 and L1. 
* Use Principal Component Analysis in feature engineering to analyze high dimensional data. * Create and design reports that will use gathered metrics to infer and draw logical conclusions of past and future behavior. 
* Perform Multinomial Logistic Regression, Random forest, Decision Tree, SVM to classify package isgoing to deliver on time for the new route. 
* Implemented different models like Logistic Regression, Random Forest and Gradient-Boost Trees topredict whether a given die will pass or fail the test. 
* Perform data analysis by using Hive to retrieve the data from Hadoop cluster, Sql to retrieve datafrom Oracle database and used ETL for data transformation. 
* Use MLlib, Spark's Machine learning library to build and evaluate different models. 
* Perform Data Cleaning, features scaling, features engineering using pandas and numpy packages inpython. 
* Develop MapReduce pipeline for feature extraction using Hive and Pig. 
* Create Data Quality Scripts using SQL and Hive to validate successful data load and quality of thedata. Create various types of data visualizations using Python and Tableau. 
* Communicate the results with operations team for taking best decisions. 
* Collect data needs and requirements by Interacting with the other departments. 
 
Environment: Python 2.x, R, HDFS, Hadoop 2.3, Hive, Linux, Spark, IBM SPSS, Tableau Desktop, SQL Server 2012, Microsoft Excel, Matlab, Spark SQL, Pyspark.
Data Scientist
Cummins - Denver, CO
April 2015 to August 2016
SLM Corporation is a publicly traded U.S. corporation that provides consumer banking & is a student loan company with over 40 years of providing student loans for college, supporting graduate and undergraduate study, and more. The primary goal of the Customer Administration effort is to provide a single point of management for all customer data. 
 
Responsibilities: 
* Developing Data Mapping, Data Governance, Transformation and Cleansing rules for the Master Data
Management (MDM) Architecture involving OLTP, ODS and OLAP. 
* Providing source to target mappings to the ETL team to perform initial, full, and incremental loadsinto the target data mart. 
* Conducting JAD sessions, writing meeting minutes, collecting requirements from business users andanalyze based on the requirements. 
* Involved in defining the source to target data mappings, business rules, and data definitions. 
* Transformation on the files received from clients and consumed by Sql Server. 
* ing closely with the ETL, SSIS, SSRS Developers to explain the complex Data Transformationusing Logic. 
* ed on DTS Packages, DTS Import/Export for transferring data between SQL Server 2000 to 2005.
 
* Performing Data Profiling, Cleansing, Integration and extraction  
* Defining the list codes and code conversions between the source systems and the data mart using
Reference Data Management (RDM). 
* Applying data cleansing/data scrubbing techniques to ensure consistency amongst data sets. * Extensively using ETL methodology for supporting data extraction, transformations and loading processing, in a complex EDW. 
 
Environment: MS Excel, Agile, Oracle 11g, Sql Server, SOA, SSIS, SSRS, ETL, UNIX, T-SQL, HP Quality Center 11, RDM (Reference Data Management).
Data Scientist
John Wiley & Sons - Hoboken, NJ
January 2014 to March 2015
Wiley's Scientific, Technical, Medical, and Scholarly business, also known as Wiley-Blackwell, serves the world's research and scholarly communities, and is the largest publisher for  and scholarly societies. Wiley has been actively increasing its presence in the Higher Education Department by partnering with 3rd party Learning Management Systems (LMS). The project involved integration of Wiley PLUS E4 LMS with Blackboard LMS. 
Responsibilities: 
* Involved in complete Software Development Life Cycle (SDLC) process by analyzing businessrequirements and understanding the functional  flow of information from source systems to destination systems. 
* A highly immersive Data Science program involving Data Manipulation & Visualization, Web Scraping,
Machine Learning, Python programming, SQL, Unix Commands, NoSQL, Hadoop. 
* Used pandas, numpy, seaborn, scipy, matplotlib, scikit-learn, NLTK in Python for developing variousmachine learning algorithms. 
* ed on different data formats such as JSON, XML and performed machine learning algorithms inPython. 
* Analyzed sentimental data and detecting trend in customer usage and other services. 
* Analyzed and Prepared data, identify the patterns on dataset by applying historical models. 
* Collaborated with Senior Data Scientists for understanding of data. 
* Used Python and R scripting by implementing machine algorithms to predict the data and forecastthe data for better results. 
* Used Python and R scripting to visualize the data and implemented machine learning algorithms. 
* Experience in developing packages in R with a shiny interface. 
* Used predictive analysis to create models of customer behavior that are correlated positively withhistorical data and use these models to forecast future results. 
* Predicted user preference based on segmentation using General Additive Models, combined withfeature clustering, to understand non-linear patterns between user segmentation and related monthly platform usage features (time series data). 
* Perform data manipulation, data preparation, normalization, and predictive modeling. 
* Improve efficiency and accuracy by evaluating model in Python and R. 
* Used Python and R script for improvement of model. 
* Application of various machine learning algorithms and statistical modeling like Decision Trees,Random Forest, Regression Models, neural nets, SVM, clustering to identify Volume using scikitlearn package 
* Performed Data cleaning process applied Backward - Forward filling methods on dataset for handlingmissing values. 
* Developed a predictive model and validate Neural Net Classification model for predict thefeature label. 
* Performed Boosting method on predicted model for the improve efficiency of the model. 
* Presented Dashboards to Higher Management for more Insights using Power BI and Tableau. 
* Hands on experience in using HIVE, Hadoop, HDFS and Bigdata related topics. 
 
Environment: R/R studio, Python, Tableau, Hadoop, Hive, MS SQL Server, MS Access, MS Excel, Outlook, Power BI.
Data Reporting Analyst
C3-Solutions - Bengaluru, Karnataka
October 2012 to April 2013
* Designed and implemented an internal reporting tool named I-CUBE using Python to automate salesand financial operational data accessible through a built-in SharePoint for leaders globally. Used API for
I-Cube to extract sales data on an hourly-basis. 
* Built and customized interactive reports on forecasts, targets and actuals data using BI/ETL such as SAS, SSAS, SSIS in the CRM which slashed manual efforts by 8%. 
* Conducted operational analyses for business worth $3M ing through all phases such asrequirements gathering, developing use cases, data mapping and creating flow diagrams. * Accomplished data cleansing and analysis results using Excel pivot tables, VLOOKUPs, data validation, graphs and chart manipulation in Excel. 
* Designed complex SQL queries, Views, Stored Procedures, Functions and Triggers to handle databasemanipulation and performance. 
* Used SQL, PLSQL scripts for automating repeatable tasks of customer feedback survey datacollection and distribution which increased the departmental efficiency by 8%.
Education

M.S in Data Mining and Predictive Analytics. in Data Mining and Predictive Analytics
St. John's University - Queens, NY
B.E in Computer Science & Engineering. in Computer Science & Engineering
Gujarat Technological University - Gujarat, IN",Data Scientist,emailed,indeed job post,resume
"

:
* 10+ years of  IT experience and ed on various  implementations.
* Experience in various industries including Healthcare, Financial, Banking and Energy.
* experience in implementing data mining and statistical machine learning solutions to various business problems, performing Principal Component Analysis, SVM, implementing supervised and unsupervised machine learning models.
* of experience in implementing batch and real time descriptive analytical solutions using big data, visualizations and reporting .
* Built predictive analytics models to generate actionable insights.
* experience in Python, R and Tableau.
* Expertise with the  in Hadoop ecosystem including HDFS, Spark, Spark SQL, HBase, Hive, and Sqoop.
* Used Agile Methodology ofDataWarehouse development using Kanbanize.
* Experience with leveraging Hadoop ecosystem components including Hive for Data Analysis, Sqoop for Data Migration between RDBMS systems and Hadoop.
* Design and implement Hadoop based data ingestion solutions and implement the capabilities and offerings of the enterprise Hadoop platform.
* Experience in Business Intelligence with SQL Server SSIS, SSAS, SSRS packages
* Experience in MySQL and MS SQL server administering and creating stored procedures.
* Proficient in analyzing and translating business requirements to  requirements and architecture.
* Developed and implementeddatacleansing,datasecurity,dataprofiling anddatamonitoring processes.
* DevelopedDataMapping,DataGovernance, and Transformation and cleansing rules for the MasterDataManagement (MDM) Architecture involving OLTP, ODS.
* Extensive exposure to Functional as well as  aspects.

Authorized to  in United States for any employer



Doozer.com, GA (Remote)
Mar' 17 to Present
DataScientist


Responsibilities:
* Reports creation and analysis at various levels for example Currency level report(CLR), Account Level Report(ALR) and other reports
* Developing fraud detection models using different machine learning techniques
* Defining the data streams and analyzing the data
* Developed the ETL process in Hadoop Platform
* Prepared High Level LogicalDataModels using Erwin, and later translated the model into the physical model using the Forward Engineering technique.
* Loaded semi structured data into Hadoop File System (HDFS).
* Involved in writing T-SQL ing on SSIS, SSRS, SSAS,DataCleansing,DataScrubbing, andDataMigration. involved in writing stored procedures and involved in writing ad-jacqueries for the datamining.
* Developed the Sqoop and Hive scripts for data transfer and data analysis.

Environment: Isolation Forest, Logistic Regression machine learning algorithms, Python, Cloudera, HDFS, Hive, Sqoop



VBGN  Inc., Seattle, WA
Oct' 15 to Feb' 17
DataScientist

Responsibilities:
* Perform extensive data studies to understand the data behaviour
* Perform Principal Component Analysis using different PCA techniques
* Developing predictive models
* Applied machine learning algorithms like clustering and segmentation methods for product offerings
* Created PhysicalDataAnalyst from the LogicalDataAnalyst using Compare and Merge Utility in ERStudio and ed with the naming standards utility.
* Developed classification models to predict the products and offers subscriptions
* Designed Python frames for machine learning models
* Involved from design to implementation of machine learning models
* Performed Import and export of data into HDFS and Hive using Sqoop and managed data coming from different sources
* Defining the requirements for data lakes/pipe lines
* ed onDataAnalysis,Dataprofiling, andDataModeling,datagovernance identifyingDataSets, SourceData, Source MetaData,DataDefinitions andDataFormats.
* Analyzed and fixed the data related issues with help of Hive and Sqoop.
* Creating Hive tables, loading data and writing Hive queries.
* Involved withDataAnalysis primarily IdentifyingDataSets, SourceData, Source MetaData,DataDefinitions andDataFormats
* Modeled various hive tables and optimized the access by designing partitions and bucketing.
* Involved in code review and bug fixing for improving the performance
* ed on the core and Spark SQL modules of Spark using programming languages like Python

Environment: K-means clustering, Hierarchical clustering, Logistic Regression, Nave Bayes, Decision Tree, KNN machine learning algorithms, PCA, Python, Cloudera, HDFS, Hive, Sqoop, Spark



HCL      
Dallas, TX
Jan' 14 to Sep' 15
DataScientist

Responsibilities:
* Played key role in extracting the features based on users behavioural, demographic and social data.
* Identified the factors that increase churn risk using tree based machine learning model and visualize the plot to see the importance to feature.
* Train the model using boosting algorithm, not only just one algorithm trained the model using various machine learning algorithms like Random Forest, KNN, SVM, Regression and compare the accuracy of the models.
* Cross validated the model using sampling techniques , plot roc curve and created confusion matrix to evaluate the accuracy.
* Calculated customer value metric to priorities customer based on probability and value.
* Created Dashboards using Tableau to exposed the analysis in Dashboards.

Environment: Python, Machine Learning, MySql, Tableau



Home Depot    
Atlanta GA
Jul' 11 to Dec' 13
DataScientist

Responsibilities:
* Performed Explanatory Data Analysis that included Data Profiling on descriptive statistics (unknown response values, imbalanced data), Feature Engineering and data pre-processing functions like transformations, imputation of missing data, capping skewed values, binning, duplicates using R
* Performed advanced SQL operations that included advanced filtering and data aggregation, window functions and preparing data for use with analytic 
* Conducted Explanatory Data Analysis and carried out visualizations with ggplot2 () function
* Performed chi-square test of independence to study the association between the categorical independent variable
* Addressed overfitting by implementing the algorithm regularization methods like L2 and L1.
* Used Principal Component Analysis in feature engineering to analyze high dimensional data.
* Involved with statistical domain experts to understand the data and ed with data management team on data quality assurance
* Provided statistical analyses in comprehensive written reports or sampling plans
* ed with division personnel to ensure the deliveryof high quality and timely statistical services necessary to achieve business goals
* Carried out predictive analysis using logistic regression, decision trees and random forests
* Carried out logistic regression with forward and backward and stepwise selection procedures
* Analyzed output results through Confusion Matrix, Sensitivity, Specificity, Accuracy and Kappa
* Validated the machine learning classifiers using Accuracy, AUC, ROC Curves and Lift Charts
* Performed random forests and analyzed graphs on training and testing errors
* Continuously interacted with Marketing Strategists and Business leaders to identify their analytic needs
* Created Tableau scorecards, dashboards using Stack bars, bar graphs, scattered plots, geographical maps, Gantt charts using show me functionality. Created dashboards to have clear view of descriptive statistics of all variables, region wise trend analysis and predicted vs actual response rate for each region. ed extensively with Advance analysis Actions, Calculations, Parameters, Background images and Maps. Effectively used data blending feature in Tableau.
     
     Environment: Oracle, SQL, R, Tableau, MS Excel and PowerPoint
     
     
Wells Fargo
San Francisco, CA
Feb' 10 to Jun' 11
Senior Analytics Consultant

Responsibilities:
* Implemented Order Management systems which included Data analysis, Data modelling and reporting for various modules
* Designed and delivered a customer churn attribution project using classification algorithms in R and built a strategy to reduce the customer churn rate that can generate an additional $2 million in annual revenue
* Developed scripts to collect transaction data from 4500 stores using Teradata to load data into data warehouse
* Decided the sourcing and scheduling rules for various clients implementing neural net algorithm in Python.
* Developed operational performance analysis adhoc, dynamic reports on IBM Cognos, Tableau, excel and SQL
* Increased conversation rates of customers by 5% using predictive analytics and market basket analysis targeting promotions
* Developed a tool from scratch for automation of Integration systems using JAVA & JMeter reducing the lead time by 80%



Microsoft
Seattle WA
Oct' 08 to Jan' 10
DataScientist

Responsibilities:
* Responsible for enabling analysis through producing information products and is involved in the research and development efforts. Traditional programming (SAS, SQL) and business intelligence (i.e. Spotfire, Tableau, or Qlik) experience
* Ability to support the creation of sophisticated, value-added analytic systems that support revenue generation, risk management, operational efficiency, regulatory compliance, portfolio management, and research.
* Deployment of advanced techniques (e.g., text mining, statistical analysis, etc.) to deliver insights
* Possesses a degree in hard science or another heavy quantitative business or social discipline
*  closely with traders, quantitative & analytics group, risk, research group to integrate pricing and risk models for new and structured trades
* Design and develop fixed income trade blotter using C#, Windows forms, Infragistics UI components, SQL Server, TIBCO messaging and Click Once deployment strategy
* Develop and Migrate CDS trades and CDS spreads with EM pricing sheets to strategic systems using Excel VBA & C#


Education:
Bachelor of commerce, Gujrat University, Jun 1994  May 1997",Data Scientist,emailed,indeed job post,resume
" Over 8+ years of Experience on Machine Learning, Statistical Modelling, Predictive Modelling, Data
Analytics, Data Modelling, Data Architecture, Data Analysis, Data Mining, Text Mining & Natural
Language Processing (NLP) 
 Proficient in gathering and analyzing the Business Requirements with experience in documenting
System Requirement Specifications (SRS) and Functional Requirement Specifications (FRS) 
 Extensive experience in Text Analytics, developing different Statistical Machine Learning, Data
Mining solutions to various business problems and generating data visualizations using R, Python, and Tableau. 
 Experience in Extracting data for creating Value Added Datasets using Python, R, SAS, Azure and SQLto analyze the behavior to target a specific set of customers to obtain hidden insights within the data to effectively implement the project s. 
 Good experience of software development in Python (libraries used: Beautiful Soup, NumPy, SciPy,
Matplotlib, Python-Twitter, Pandas data frame, net, Urllib2, MySQL) 
 Experience on Artificial Intelligence algorithms, Business Intelligence, Analytics Models (like Decision
Trees, Linear & Logistic Regression, Hadoop (Hive, PIG), R, Python, Spark, Scala, MS Excel, SQL and Postgre SQL, Erwin. 
 Extensive experienced on business intelligence (and BI )  such as OLAP, Datawarehousing, reporting and querying , Data mining and Spreadsheets. 
 Efficient in developing Logical and Physical Data model and organizing data as per the businessrequirements using Sybase Power Designer, Erwin, ER Studio in both OLTP and OLAP applications 
 Strong understanding of when to use an ODS or data mart or data warehousing. 
 Experienced in employing RProgramming, MATLAB, SAS, Tableau and SQL for datacleaning, datavisualization, risk analysis and predictive analytics 
 Collaborated with the lead Data Architect to model the Data warehouse in accordance to FSLDMsubject areas, 3NF format, Snow flake schema. 
 Skilled in E-R/Dimensional Data Modeling, Database Design and implementing RDBMS specificfeatures. 
 Experience on advanced SAS programming techniques, such as PROC SQL (JOIN/ UNION), PROC
APPEND, PROC DATASETS, and PROC TRANSPOSE. 
 Hands on experienced with Machine Learning, Regression Analysis, Clustering, Boosting,
Classification, Principal Component Analysis and Data Visualization  
 Highly skilled in using Hadoop (pig and Hive) for basic analysis and extraction of data in theinfrastructure to provide data summarization and data manipulation using Linux Commands. 
  Knowledge on designing and implementing a fully operational production grade largescale data solution on Snowflake Data Warehouse 
 Familiarity with Crystal Reports, and SSRS - Query, Reporting, Analysis and Enterprise Information
Management 
 Experienced in Database using Oracle, XML, DB2, Teradata15/14, Netezza, server, Big Data andNoSQL. 
 ed closely with other data scientists to create data driven products. 
 Strong experienced in Statistical Modeling/Machine Learning and Visualization  
 Proficient in Hadoop, HDFS, Hive, MapReduce, Pig and NOSQL databases like MongoDB, HBase,
Cassandra and expertise in applying data mining techniques and optimization techniques in B2B and
B2C industries and proficient Machine Learning, Data/Text Mining, Statistical Analysis & Predictive Modeling. 
 Experienced in Data Modeling & Data Analysis experience using Dimensional DataModeling andRelational Data Modeling, Star Schema/Snowflake Modeling, FACT& Dimensions tables, Physical & Logical Data Modeling.
Willing to relocate: Anywhere
 Experience

Data Scientist
Century link - Littleton, CO August 2018 to Present
Description: CenturyLink is the second largest U.S. communications provider to global enterprise customers. With customers in more than 60 countries and an intense focus on the customer experience, CenturyLink strives to be the world's best neting company by solving customers' increased demand for reliable and secure connections.It also serves as its customers' trusted partner, helping them manage increased net and IT complexity and providing managed net and cyber security solutions that help protect their business. 
 
Responsibilities: 
 Built data pipelines for reporting, alerting, and data mining. Experienced with table design and datamanagement using HDFS, Hive, Sqoop, MySQL. 
 ed with statistical models for data analysis, predictive modeling, machine learning approachesand recommendation and optimization algorithms. 
 ing in Business and Data Analysis, Data Profiling, Data Migration, Data Integration and MetadataManagement Services. 
 ed extensively on Databases preferably Oracle 11g/12c and writing PL/SQL scripts for multiplepurposes. 
 Analyzing Business Intelligence Reporting requirements and translating them into data sourcingand modeling requirements including Dimensional & Normalized data models, Facts, Dimensions,
Snowflake Schemas. 
 Design, coding, unit testing of ETL package source marts and subject marts using Informatica ETLprocesses for Oracle database. 
 ed with Big Data  such Hadoop, Hive, Map Reduce 
 Developed MapReduce/Spark Python modules for machine learning & predictive analytics in Hadoopon AWS. Implemented a Python-based distributed random forest via Python streaming. 
 Setup storage and data analysis  in Amazon Web Services cloud computing infrastructure. 
 A highly immersive Data Science program involving Data Manipulation & Visualization, Web Scraping,
Machine Learning, Python programming, SQL, GIT, Linux Commands, NoSQL, MongoDB, Hadoop. 
 Performed scoring and financial forecasting for collection priorities using Python, R and
SASmachinelearning algorithms. 
 Handled importing data from various data sources, performed transformations using Hive,
MapReduce, and loaded data into HDFS and batch processing with Linux 
 Managed existing team members lead the recruiting and on boarding of a larger DataScience teamthat addresses analytical knowledge requirements. 
 Created SQL scripts and analyzed the data in MS Access/Excel and ed on SQL and SASscriptmapping. 
 Rapid model creation in Python using pandas, numpy, sklearn, and plot.ly for data visualization.These models are then implemented in SAS where they are interfaced with MSSQL databases and scheduled to update on a timely basis. 
 Attained good knowledge in Hadoop Data Lake Implementation and HADOOP Architecture for clientbusiness data management. 
 Above scoring models resulted in millions of dollars of added revenue to the company and a changein priorities of the entire company. 
 
Environment: R, SQL, Python 2.7.x, SQL Server 2014, regression, logistic regression, random forest, neural nets, Topic Modeling, NLTK, SVM (Support Vector Machine), JSON, XML, HIVE, HADOOP, PIG, Sklearn, SciPy, Graph Lab, No SQL, SAS, SPSS, Spark, Hadoop, Kafka, HBase, MLib.
Data Scientist
Verizon - Richardson, TX
May 2017 to July 2018
Description: Verizon Communications Inc. is an American multinational telecommunications conglomerate and a corporate component of the Dow Jones Industrial Average. The Company, through its subsidiaries, provides communications, information and entertainment products and services to consumers, businesses and governmental agencies. Its segments include Wireless and Wireline. The Wireless segment offers communications products and services, including wireless voice and data services and equipment sales, to consumer, business and government customers across the United States. 
 
Responsibilities: 
 Responsible for performing Machine-learning techniques regression/classification to predict theoutcomes. 
 Responsible for design and development of advanced R/Python programs to prepare to transformand harmonize data sets in preparation for Modeling. 
 Identifying and executing process improvements, hands-on in various  such as Oracle,
Informatica, and Business Objects. 
 Designing and implementing data warehouses and data marts using components of KimballMethodology, like Data Warehouse Bus, Conformed Facts & Dimensions, Slowly Changing Dimensions in Snowflake Schema 
 Develop and implement innovative AI and machine learning  that will be used in the Risk. 
 Performed the feature engineering of supervised and unsupervised machine learning models. 
 Implemented SparkMLLib utilities such as including classification, regression, clustering, collaborativefiltering, and dimensionality reductions 
 Utilized Convolution Neural Nets to implement a machine learning image recognitioncomponentusing TensorFlow. 
 Strong in ETL and data integration experience in developing ETL mappings and scripts usingInformatica 
 Interaction with Business Analyst, SME and other Data Architects to understand Business needs andfunctionality for various project solutions. 
 Designed the prototype of the Data mart and documented possible outcome from it for end-user. 
 Involved in business process Modeling using UML. 
 Handled importing data from various data sources, performed transformations using Hive, Map
Reduce, and loaded data into HDFS. 
 ed on Spark tool collaborating with ML libraries in eliminating a shotgun approach to understandcustomer buying patterns. 
 Responsible for handling Hive queries using Spark SQL that integrates with Spark environment. 
 Created SQL tables with referential integrity and developed queries using SQL, SQL*PLUS, andPL/SQL. 
 Involved with Data Analysis primarily Identifying Data Sets, Source Data, Source Meta Data, Data
Definitions, and Data Formats 
 Performance tuning of the database, which includes indexes, and optimizing SQL statements,monitoring the server. 
 Updated Pythonscripts to match training data with our database stored in AWS Cloud Search, sothatwe would be able to assign each document a response label for further classification. 
 Wrote simple and advanced SQL queries and scripts to create standard and Adhoc reports for seniormanagers. 
 Created PL/SQL packages and Database Triggers and developed user procedures and prepared usermanuals for the new programs. 
 
Environment: Python, MDM, MLLib, PL/SQL, Tableau, Git, NLP, SQL Server, MLLib, Scala NLP, SSMS, ERP, CRM, Netezza, Cassandra, SQL, PL/SQL, SSRS, Informatica, Spark, Azure, R Studio, MongoDB, JAVA, HIVE.
Data Scientist
Direct Energy - Houston, TX
January 2016 to April 2017
Description: Direct Energy is a North American retailer of energy and energy services. The company was founded in 1986. The company has more than six million customers in Canada and the United States, making it the largest energy retailer in North America. 
 
Responsibilities: 
 Involved in extensive hoc reporting, routine operational reporting, and data manipulation to produceroutine metrics and dashboards for management 
 Created parameters, action filters and calculated sets for preparing dashboards and sheets inTableau. 
 Interacting with other datascientists and architects, custom solutions for data visualization using like a tableau and Packages in Python. 
 Involved in running Map Reduce jobs for processing millions of records. 
 Written complex SQL queries using joins and OLAP functions like Count, CSUM, and Rank etc. 
 The building, publishing customized interactive reports, report scheduling and dashboards usingTableauserver. 
 Developed in Python programs for manipulating the data reading from various Teradata and convertthem as one CSV Files. 
 Performing statistical data analysis and data visualization using Python. 
 ed on creating filters and calculated sets for preparing dashboards and sheets in Tableau.  Created data models in Splunk using pivot tables by analyzing the vast amount of data and extracting key information to suit various business requirements. 
 Created new scripts for Splunk scripted input for the system, collecting CPU and OS data. 
 Implemented data refreshes on Tableau Server for biweekly and monthly increments based ona business change to ensure that the views and dashboards were displaying the changed data accurately. 
 Developed normalized Logical and Physical database models for designing an OLTP application. 
 Knowledgeable in AWS Environment for loading data files from on prim to Redshift cluster. 
 Performed SQL Testing on AWSRedshift databases. 
 Developed TeradataSQLscripts using OLAP functions like rank and rank over to improve the queryperformance while pulling the data from large tables. 
 Developed and implemented SSIS, SSRS and SSAS application solutions for various business unitsacross the organization. 
 Designed the Data Marts in dimensional data modelling using star and snowflake schemas. 
 Analyzed DataSet with SASprogramming, R and Excel. 
 Publish Interactive dashboards and schedule auto-data refreshes 
 Maintenance of large data sets, combining data from various sources by Excel, Enterprise, and SAS
Grid, Access and SQL queries. 
 Created Hive queries that helped market analysts spot emerging trends by comparing incrementaldata with Teradata reference tables and historical metrics. 
 Design and development of ETL processes using InformaticaETL  for dimension and fact filecreation. 
 Develop and automate solutions for a new billing and membership Enterprise data Warehouseincluding ETL routines, tables, maps, materialized views, and stored procedures incorporating
Informatica and Oracle PL/SQL ets. 
 Performed analysis of implementing Spark uses Scala and wrote spark sample programs usingPySpark. 
 
Environment:SQL/Server, Oracle 10g/11g, MS-Office, Teradata, Informatica, ER Studio, XML, R connector, Python, R, Tableau 9.2
Data Scientist/R Developer
Becton Dickinson - Franklin Lakes, NJ March 2014 to December 2015
Description: BD is a global medical technology company that is advancing the world of health by improving medical discovery, diagnostics and the delivery of care. BD leads inpatient and healthcare er safety and the  that enable medical research and clinical laboratories. 
 
Responsibilities: 
 The conducted analysis in assessing customer consuming behaviors and discover the value ofcustomers with RMF analysis, applied customer segmentation with clustering algorithms such as K-
Means Clustering and HierarchicalClustering. 
 Collaborated with data engineers to implement the ETL process, wrote and optimized SQL queries toperform data extraction and merging from Oracle. 
 Involved in managing backup and restoring data in the live Cassandra Cluster. 
 Used R, Python, and Spark to develop a variety of models and algorithms for analytic purposes.  Performed data integrity checks, data cleaning, exploratory analysis and feature engineer using R and Python. 
 Developed personalized product recommendation with Machine learning algorithms, includingGradient Boosting Tree and Collaborative filtering to better meet the needs of existing customers and acquire new customers. 
 Used Python and Spark to implement different machine learning algorithms, including Generalized
Linear Model, RandomForest, SVM, Boosting and Neural Net. 
 Evaluated parameters with K-Fold Cross Validation and optimized performance of models. 
 ed on benchmarking Cassandra Cluster using the Cassandra stress tool. 
 A highly immersive Data Science program involving Data Manipulation and Visualization, Web
Scraping, Machine Learning, GIT, SQL, LINUX Commands, Python programming, No SQL. 
 ed on data cleaning, data preparation, and feature engineering with Python, including Numpy,
Scipy, Matplotlib, Seaborn, Pandas, and Scikit-learn. 
 Identified risk level and eligibility of new insurance applicants with MachineLearning algorithms. 
 Determined customer satisfaction and helped enhance customer using NLP. 
 Utilized SQL and HiveQL to query, manipulate data from variety data sources including Oracle and
HDFS, while maintaining data integrity. 
 Performed datavisualization and Designeddashboards with Tableau and D3.js and providedcomplexreports, includingcharts, summaries, and graphs to interpret the findings to the team and stakeholders. 
 
Environment: R, MATLAB, MongoDB, exploratory analysis, feature engineering, K-Means Clustering,
Hierarchical Clustering, Machine Learning), Python, Spark (MLlib, PY Spark), Tableau, Micro Strategy, SAS, Tensor Flow, regression, logistic regression, Hadoop 2.7, OLTP, random forest, OLAP, HDFS, ODS, NLTK, SVM, JSON, XML and MapReduce.
Data Analyst
ZETA Interactive
December 2012 to February 2014
Description: We're Zeta Global, a data-driven marketing technology company that helps brands acquire more customers, keep the ones they have for longer and grow their value. We create personalized customer experiences by leveraging strategy, analytics, machine learning, and awardwinning creative that ignite a perpetual dialogue between brands and their customers. 
 
Responsibilities: 
 Used SAS Proc SQLpass-throughfacility to connect to Oracle tables and created SAS datasets usingvarious SQL joins such as left join, right join, inner join and full join. 
 Performing data validation, transforming data from RDBMS oracle to SAS datasets. 
 Produce quality customized reports by using PROC TABULATE, PROC REPORT Styles, and ODS RTFand provide descriptive statistics using PROC MEANS, PROC FREQ, and PROC UNIVARIATE. 
 Developed SAS macros for data cleaning, reporting and to support routing processing. 
 Performed advanced querying using SAS Enterprise Guide, calculating computed columns, usingafilter, manipulate and prepare data for Reporting, Graphing, and Summarization, statistical analysis, finally generating SAS datasets. 
 Involved in Developing, Debugging, and validating the project-specific SAS programs to generatederived SAS datasets, summary tables, and data listings according to study documents. 
 Created datasets as per the approved specification collaborated with project teams to completescientific reports and review reports to ensure accuracy and clarity. 
 Experienced in ing with data modelers to translate business rules/requirements into conceptual/logical dimensional models and ed with complex de-normalized and normalized data models 
 Performed different calculations like Quick table calculations, Date Calculations, Aggregate
Calculations, String and Number Calculations. 
 Created action filters, user filters, parameters and calculated sets for preparing dashboards andsheets in Tableau. 
 Used the dynamic SQL to perform some pre-and post-session task required while performing
Extraction, transformation, and loading. 
 Designing the ETL process using Informatica to populate the Data Mart using the flat files to Oracledatabase 
 Expertise in Agile Scrum Methodology to implement project life cycles of reports design anddevelopment 
 Combined Tableau visualizations into Interactive Dashboards using filter actions, highlight actionsetc. and published them on the web. 
 Gathering business requirements, creating business requirement documents (BRD /FRD) 
  closely with business leaders and users to define and design the data sources requirementsand data access Code, test, identify, implement and document technical solutions utilizing JavaScript, PHP&MySQL. 
 ing with themanager to prioritize requirements and preparing reports on theweekly and monthlybasis. 
 
Environment: SQL Server, Oracle 11g/10g, MS Office Suite, PowerPivot, Power Point, SAS Base, SAS Enterprise Guide, SAS/MACRO, SAS/SQL, SAS/ODS, SQL, PL/SQL, Visio.
Data Analyst
Karvy Global Services
January 2011 to November 2012
Description:: Karvy Global Services is a specialist knowledge process outsourcing services firm and an arm of the Karvy Group, one of India's largest integrated financial services companies Responsibilities: 
 Participated in requirement gathering sessions with business stakeholders to understand the projectgoals and documented the business requirement documents (BRD) 
 Studied the Requirements Specifications, Use Cases and analyzed the data needs of the Businessusers. 
 Implemented conceptual and logical data models using Erwin 7.2 by adopting agile methodologies asper organization standards. 
 Redesigned some of the previous models by adding some new entities and attributes as per thebusiness requirements. 
 Converted the Logical data models to Physical data models to generate DDL scripts. 
 Reverse Engineered existed data models for analyzing and comparing the business process. 
 Expertise in the Forward Engineering of logical models to generate the physical model using Erwin.  Created the Logical data models using Erwin 7.2 and ensured that it follows the normalization rules and have the best possible traceability pattern. 
 Migrated several models from Erwin 4.1/7.1 to ERWIN 7.2 and updated the previous namingstandards. 
 Extensively ed with enterprise data warehouse development by building data marts, staging,and restaging. 
 Scheduled reports for daily, weekly, monthly reports for executives, Business analyst and customerrepresentatives for various categories and regions based on business needs using SQL Server
Reporting Services (SSRS) 
 ed with business users to understand metric definitions, presentation, and user needs. Environment: Erwin, Informatica, Cognos, Oracle 9i, SQL Server 2003, SQL, MS Office, Windows 2003.
Education

Bachelors of Science in Computer science Engineering
GITAM University 2011
Skills / IT Skills

SQL (8 years), DATABASE (6 years), INFORMATICA (6 years), SAS (5 years), Serial Attached SCSI (5 years)
Additional Information

TECHNICAL SKILLS: 
 
Languages HTML5, DHTML, WSDL, CSS3, C, C++, XML, R/R Studio, SAS Enterprise Guide, SAS, R
(Caret, Weka, ggplot), python 
Software/Libraries Keras, Caffe, TensorFlow, OpenCV, Scikit-learn, Pandas, NumPy, Microsoft Visual Studio, Microsoft Office. 
Development  Microsoft SQL Studio, IntelliJ, Eclipse, NetBeans. Packages ggplot2, caret, dplyr, RWeka, gmodels, RCurl, tm, C50, twitter, NLP, Reshape2, rjson, plyr, pandas, numpy, seaborn, scipy, Matplot lib, scikit-learn, Beautiful Soup, Rpy2, sqlalchemy. 
 
Machine Learning Algorithms 
Neural Nets, Decision trees, Support Vector Machines, Random forest, Convolutional Neural Nets, Logistic Regression, PCA, K- means, KNN. 
 
Development Methodologies Agile/Scrum, UML, Design Patterns, Waterfall 
Database SQL, Hive, Impala, Pig, Spark SQL, Databases SQL-Server, My SQL, MS Access, HDFS, HBase,
Teradata, Netezza, Mongo DB, Cassandra. 
Reporting  MS Office (Word/Excel/PowerPoint/ Visio/Outlook), Crystal Reports XI, SSRS, Cognos 7.0/6.0. 
Big Data  Hadoop, Hive, HDFS, Map Reduce, Pig. 
BI  
Microsoft Power BI, Tableau, SSIS, SSRS, SSAS, Business Intelligence Development Studio (BIDS), Visual Studio, Crystal Reports, Informatica 6.1. 
 
Database Design  and Data Modeling 
MS Visio, ERWIN 4.5/4.0, Star Schema/Snowflake Schema modeling, Fact & Dimensions tables, physical & logical data modeling, Normalization and De-normalization techniques, Kimball &Inmon
Methodologies",Data Scientist,emailed,indeed job post,resume
"
 :
* Around 4+ years of experience as Data scientist with strong  expertise in implementing advanced Machine Learning and Natural Language Processing algorithms upon data from diverse domains and building highly efficient models to derive actionable insights for business environments leveraging exploratory data analysis, feature engineering, statistical modelling and predictive analytics
* Experience with Statistical Analysis, Data Mining and Machine Learning  using Python and SQL
* Deep sighted knowledge in Data Structures & Algorithms, Statistics, Pattern recognition and Predictive modeling
* Strong mathematical knowledge and hands on experience in implementing Machine Learning algorithms like K-Nearest Neighbors, Logistic Regression, Linear regression, Nave Bayes, Support Vector Machines, Decision Trees, Random Forests, Gradient Boosted Decision Trees, Stacking Models.
* Expert knowledge in breadth of Machine Learning algorithms with an ability to evaluate and choose best suited algorithm, perform feature selection and optimize machine learning model
* Experience in using various packages in R and python like ggplot2, caret, dplyr, Rweka, gmodels, RCurl, tm, C50, twitteR, NLP, Reshape2, rjson, plyr, pandas, numpy, seaborn, scipy, matplotlib, scikit-learn, BeautifulSoup, Rpy2. 
* ed on various applications using Python integrated IDEs such as Anaconda and Py Charm.
* Valuable experience ing with large datasets and Deep Learning algorithms with Tensor Flow.
* Proficient in Statistical Modeling and Machine Learning techniques (Linear, Logistics, Decision Trees, Random Forest, SVM, K-Nearest Neighbors, Bayesian, XGBoost) in Forecasting/ Predictive Analytics, Segmentation methodologies, Regression-based models, Hypothesis testing, Factor analysis/ PCA, Ensembles. 
* Excellent knowledge of Machine Learning, Mathematical Modeling and Operations Research. Comfortable with R, Python, SAS and Weka, MATLAB, Relational databases. Deep understanding & exposure of BigDataEco-system. 
* Strong experience and knowledge in Data Visualization with Tableau creating Line and scatterplots, BarCharts, Histograms, Piechart, Dot charts, Boxplots, Time series, Error Bars, Multiple Charts types, Multiple Axes, subplots etc. 
* Created data visualizations with Tableau for publishing and presenting dashboards, Storyline on web and desktop platforms.
* Skilled in performing data parsing, data manipulation and data preparation with methods including describe data contents, compute descriptive statistics of data, regex, split and combine, Remap, merge, subset, reindex, melt and reshape.
* Experienced with tuning parameters for different machine learning models to improve performance.
* Excellent communication . Successfully ing in fast-paced multitasking environment both independently and in collaborative team, a self-motivated enthusiastic learner

 and :

Languages
C, java Script, R, Python, Matlab

Databases
MS SQL Server,  Oracle, HBase, Amazon Redshift, MS SQL
Statistical Methods:
Hypothetical Testing, Exploratory Data Analysis (EDA), Confidence Intervals, Bayes Law, Principal Component Analysis (PCA), Dimensionality Reduction, Cross-Validation, Auto-correlation
Machine Learning
Regression analysis, Nave Bayes, Decision Tree, Random Forests, Support Vector Machine, Neural Net, Sentiment Analysis, Collaborative Filtering, K-Means Clustering, KNN, CNN, RNN and Ada Boosting.
Data Visualization
Tableau, MatPlotLib, Seaborn, ggplot2,d3.js
Packages
ggplot2, caret, dplyr, Rweka, gmodels, RCurl, tm, C50, twitter, NLP, Reshape2, rjson, plyr, pandas, numPy, seaborn, sciPy, matplot lib, scikit-learn, Beautiful Soup, Rpy2, sqlalchemy.
Hadoop Ecosystem
Hadoop 2.x, Spark 2.x, MapReduce, Hive, HDFS, Pig
Cloud Services
Amazon Web Services (AWS) EC2/S3/Redshift
Reporting 
Tableau Suite of  10.x, Server and Online, Server Reporting Services(SSRS),  MS Office (Word/Excel/Power Point/ Visio)
Version Control 
SVM, GitHub.
Operating Systems
Microsoft Windows, Linux (Ubuntu), Microsoft Office Suite (Word, PowerPoint, Excel)

:


Client: ZenQ LLC (Dallas, TX)
March2018-Current
Role: Data Scientist
Description: ZenQ is one of the fastest growing digital, business consulting &  service firms. They serve clients across the US, UK, Canada, Australia and Newzealand. This project is for a client related to financial services which includes predicting customer lifetime value modelling (CLV) and sorting the customers of different levels having different credit ranges and increasing their credit limit based on their credit history and usage of credit card. This also includes fraud detection, customer analytics, NLP tasks, Ticket routing techniques, etc.
Responsibilities:
* Performed Data Profiling to learn about behavior with various features such as traffic pattern, location, Date and Time etc. Integrating with external data sources and APIs to discover interesting trends.
* Personalization, Target Marketing, Customer Segmentation and profiling.
* Performed Data Cleaning, features scaling, featurization, features engineering.
* Used Pandas, NumPy, SciPy, Matplotlib, Seaborn, Scikit-learn in Python at various stages for developing machine learning model and utilized machine learning algorithms such as linear regression, Naive Bayes, Random Forests, Decision Trees, K-means, & KNN. 
* Customer segmentation based on their behaviour or specific characteristics like age, region, income, geographical location and applying Clustering algorithms to group the customers based on their similar behaviour patterns. 
* The results from the segmentation helps to learn the Customer Lifetime Value of every segment and discover high value and low value segments and to improve the customer service to retain the customers.
* Analyzed and implemented few research proofs of concept models for Real time fraud detection over credit card and online banking purchases.
* ed with Credit Analysis, Risk modeling algorithms to implement in customer acquisition strategies into the real time business.
* Studied and implemented Fraud detection models to monitor the unconventional purchases from customer bases and alert them with updates.
* Performed Clustering with historical, demographic and behavioral data as features to implement the Personalized marketing that offers right product to right person at the right time on the right device.
* Evaluated models using Cross validation, Log loss function used to measure the performance and used ROC curves and AUC for feature selection.
* Used Principal Component Analysis and t-SNE in feature engineering to analyze high dimensional data.
* Addressed overfitting and underfitting by tuning the hyper parameter of the algorithm and by using L1 and L2 Regularization.
* Used Spark's Machine learning library to build and evaluate different models.

Environment: Python 2.x,Linux, Spark, AWS, Tableau Desktop, SQL Server 2012,NLP, Cluster Analysis, Random Forest, Microsoft Excel, Spark SQL, PySpark, Teradata.



Client: METMOX (Schaumburg, IL)
Sep2017-Feb2018

Role: Data Scientist

Description: METMOX builds custom tailored solutions. Whether you need security, net operations, server management, SAP, or software development. The company has several clients with businesses around in 59 countries. The company operates with respect to the business requirements and timeline of clients. The project aimed at assisting the Fraud Investigations team to improve the accuracy of identifying fraudulent transactions and the Loan Compliance team to identify fraudulent loan applications using advanced data analytic approaches and machine learning models.

Responsibilities:
* Extensively involved in all phases of data acquisition, data collection, data cleaning, model development, model validation, and visualization to deliver data science solutions.
* Built machine learning models to identify fraudulent applications for loan pre-approvals and to identify fraudulent credit card transactions using the history of customer transactions with supervised learning methods.
* Extracted data from database, copied into HDFS File system and used Hadoop  such as Hive and Pig Latin to retrieve the data required for building models.
* ed on data cleaning and ensured data quality, consistency, integrity using Pandas, NumPy.
* Tackled highly imbalanced Fraud dataset using sampling techniques like down-sampling, up-sampling and SMOTE (Synthetic Minority Over-Sampling Technique) using Python Scikit-learn.
* Used PCA and other feature engineering techniques to reduce the high dimensional data, feature normalization techniques and label encoding with Scikit-learn library in Python.
* Used Pandas, NumPy, Seaborn, Matplotlib, Scikit-learn in Python for developing various machine learning models such as Logistic regression, Gradient Boost Decision Tree and Neural Net.
* Used cross-validation to test the models with different batches of data to optimize the models and prevent overfitting.
* Experimented with Ensemble methods to increase the accuracy of the training model with different Bagging and Boosting methods.
* Implemented a Python-based distributed random forest via PySpark and MLlib.
* Used AWS S3, DynamoDB, AWS lambda, AWS EC2 for data storage and models' deployment.
* Created and maintained reports to display the status and performance of deployed model and algorithm with Tableau.

Environment: Python 2.x, CDH5, ML, HDFS, Hadoop 2.3, Hive, Impala, Linux, Spark, Tableau Desktop, SQL Server 2012, Microsoft Excel, MATLAB, Spark SQL, PySpark.



Client: TA Digital (Hyderabad, Telangana)
May2015-july2017

Role: Data Scientist
Description: TA Digital is a provider of full-service Digital Transformation solutions and services for enterprise companies with businesses across Asia, United States, Canada and the United Kingdom. It provides a variety of services in Information , Financial Services, Transportation, Retail, Automotive, Manufacturing, Utilities, Chemical, Healthcare, Pharmaceuticals, & Biotech. I ed for various in-house projects which handles customer analytics, NLP tasks, OCR models, Visualizations etc.

Responsibilities:
 Initial  started as a Mainframe Developer which changed designing the t Data Architecture.
 Involved in designing the Physical Data Architecture model of Machine to Machine Model based on Consumer Model.
 Improved the efficiency of processing to achieve a defect-free bulk order for New Multiple Connections, under VISION System.
 ed with consumers and different teams to gain insights about the data concepts behind their business.
 Analyzed business requirements, system requirements, data mapping requirement specifications, and responsible for documenting functional requirements and supplementary requirements.
 Involved in initial data pattern recognition and data cleaning using dplyr package in R.
 Developed Tabulation datasets and Analysis datasets as per the specifications.
 Coordinate with the team members to reach the target successfully.
 Identified, reviewed and documented the business requirements for pricing calculation and billing processing.
 Responsible for periodic Reporting using Reports and Graphs in TABLEAU and EXCEL.
 Compared actual results to expected results and recorded test results.
 Involved in coordinating testing activities with different testing and development teams.
 Implemented Checkpoints for Back-end Testing
 Created several efficacy tables like Summaries of Best Response etc. and determined survival analysis by using proc Life Test.
 Determined the missing data, outlier and invalid data and applied appropriate data management techniques.
 Analyzed different trends and market segmentation based on historical data using K means clustering, Classification techniques.
 Created dashboard and stories for senior managers.
 Involved in creating dashboards and reports in Tableau 8.1.1 and Maintaining server activities, user activity, and customized views on Server Analysis.
 Created Rich Graphic visualization/dashboards in Tableau to enable a fast read on claims and key business drivers and to direct attention to key area
Environment: Python, R, R Studio, DB2, OLAP, OLTP, Multi-dimensional modelling, Data Warehousing, SQL, Microsoft Office, Tableau 8.1",Data Scientist,emailed,indeed job post,resume
"Willing to relocate: Anywhere
Authorized to  in the US for any employer
 Experience

Vice President
UConn Consulting Group
September 2018 to Present
 Establishes and maintains relationships with internal and external clients and service provider  Partners with the executive team in formulating and implementing action plans for the clubs enhancement and sustainability  
 Drives UGC's strategic planning initiatives 
 Manages relationships with the UGC and UConn alumni  
 Facilitates the smooth running of logistics  
 Help in laying out the rules of the club in consultation with the entire leadership team 
 
Data Analytics Consultant
Feel Good Lab - Hartford, CT
September 2018 to Present
 Tracking and reporting key performance metrics, traffic behaviors and campaign performance usingGoogle Analytics 
 Strategizing the product line by market analysis and Maintaining client databases and dashboards tomonitor sales pipeline
Sr. Data Analyst
Khel Academy
September 2017 to July 2018
 Optimized the marketing campaigns by building/evaluating/eliminating segments of frequentlytargeted and suppressed populations(participants) for tournaments 
 Improved e-commerce website traffic through data analysis using Google Analytics, A/B testing by60% 
 Performed customer segmentation using Machine learning algorithms on data coming from differentmarketing campaigns, increasing participants engagement in various sports tournaments by 32%
Analyst
TEKSystems Global Services
July 2015 to June 2016
 Automated analytical reports using SQL and business intelligence  (OBIEE, ODI), performedregression analysis for end to end process for prediction & forecasting of sales, resulting in revenue boost by 5% 
 Generated and analyzed metrics to track daily status of various  integrating data dumpsthrough Tableau-based dashboard, achieving a man-hour saving of 10 hours/week, garnered appreciation from the higher management


Master of Science in Business Analytics in Analytics & Project Management
University of Connecticut School of Business December 2019
Bachelor's in Electronics and Communication Engineering
National Institute of  Surathkal
July 2011 to May 2015


Python (2 years), SQL (2 years), Javascript (3 years), Java, R (2 years)
Awards

 Certificate, Awards & Achievements
Received Best Employee of Quarter (highest employee recognition award) for outstanding performance in Khel Academy
Certifications/Licenses

 Certificate, Awards & Achievements
Additional Information

 ing as Vice President - Strategy and Operation at UCONN Graduate Consulting Group at UCONN- School of Business 
 Received Best Employee of Quarter (highest employee recognition award) for outstandingperformance in Khel Academy 
 Initiated and organized various CSR activities at TEKSystems Global Services 
 Secured 99.2 percentile (out of 1.1 Million participants) in All India Engineering Entrance Exam 2011",Data Scientist,emailed,indeed job post,resume
":
 11 years of hands on experience and comprehensive industry knowledge in Data science/Analyst in the areas of -- Data Mining, Machine Learning, Statistical Modeling, Data Modeling, Business Intelligence, Data Visualization, solving real-world practical business problems. 
 ed with various stakeholders throughout the organization to identify opportunities for leveraging company data to drive business solutions. 
 Mined and analyzed data from company databases to drive optimization to day to day business processes, product development and business strategies.
  ing experience in Machine Learning algorithms such as linear regression, segmentation and modelling, logistic regression, Tree Algorithms - Decision Trees, Random Forest, Gradient Boosting, XG Boost, Clustering, Neural Nets - RNN, LSTM, CNN, Text Analytics - TF-IDF, LDA, Time Series Analysis - ARIMA, ARCH, GARCH, Survival Analysis, Sentimental Analysis, ANOVA, Bayesian Statistics, Reinforcement Learning, Deep Learning, Dimension Reduction - PCA (Principal Compoonent Analysis)
 Experience in using Python & R libraries like Numpy, SciPy, Pandas, Matplotlib, Scikit-learn, Beautiful Soup, Caret, forecast, xgboost, tidy text, dplyr, ggplot2, Shiny, bokeh, Seaborn, SparkContext, Tensor- Flow, genism Microsoft Azure
 Deep understanding of Software Development Life Cycle (SDLC) as well as Agile/Scrum methodology to accelerate Software Development iteration.
 Proficient in SAS, MATLAB and Hands on experience in writing queries in SQL (Teradata, MySQL) and Informatica to extract, transform and load (ETL) and Visualization using Tableau
 Used predictive modeling to optimize current processes and improve corrective action timeliness. Co-ordinated with different functional teams to implement models and monitor outcomes.
 Experience in designing visualizations using Tableau and Power BI software and Storyline on web and desktop platforms, publishing and presenting dashboards. 
  expertise and business acumen necessary to translate business requirements and s into scalable, highly resilient and successful system solutions. 
 Proficient in managing end to end data science life cycle and building end to end data pipelines of Machine Learning models including Data Acquisition, Data Preparation, Data Manipulation, Feature Engineering, Statistical Modeling, Testing and Validation, Visualization and Reporting the insights. 
 Strong knowledge of Statistical methodologies such as Hypothesis Testing, T-tests, ANOVA, Monto Carlo Sampling, Time Series Analysis. Developed Machine Learning/Statistical models in R and Python using various Supervised and Unsupervised Machine Learning algorithms  Regression, Classification, Clustering, Dimension Reduction, Association Rule Learning, and Natural Language Processing.
 Proficient in Machine Learning techniques -- Decision Trees, Linear/Logistic Regression, Random Forest, SVM, Bayesian, XG Boost, K-Nearest Neighbors) and Statistical Modeling in Forecasting/ Predictive Analytics, Segmentation methodologies, Regression based models, Hypothesis testing, Factor analysis/ PCA, Ensembles.
 Expertise in Data Integration, Data Cleaning, Data Analysis and Profiling, Data Import and Export using multiple ETL  such as SQL Server, SSIS and SSAS. 
 Expertise in using Microsoft Office including using Microsoft Excel to build Pivot Tables and Visualizations. 
 Strong knowledge in all phases of SDLC (Software Development life cycle) from Analysis, Design, Development, Testing, Implementation and Maintenance. 
 Great exposure in interacting with end-users to gather and document the requirements, project planning and scheduling.  Actively participated in creation and implementation of Test plans. 
 Strong business sense and abilities to communicate data insights to both  and non clients. ed in both Agile and Waterfall implementations.


 

Machine Learning
Linear/Logistic Regression, Lasso, Ridge, Elastic Nets, Decision Trees, Random Forest, K-Nearest Neighbor, Nave Bayes, Support Vector Machines, Gradient Boosting, Bayesian models, Ensemble Methods, Principal Component Analysis, Association Rules, Factor Analysis, Cluster analysis - K-means / Hierarchical, Market Basket Analysis.
Statistics
Statistical tests: T-tests, Chi-square analysis, Correlation tests, A/B testing, Normality tests, Residual diagnostics, ANOVA
Time Series: ARIMA, Holt winters, Exponential smoothing, Bayesian structural 
Programming
R(caret, glmnet, xgboost, dplyr, survival, rpart, ggplot, dply)
Python (numpy, pandas, scikit-learn, scipy, matplotlib, seaborn, tensor-flow, Keras)
SQL, Spark, Scala, Java
Data Visualization
Tableau, PowerBI, R (ggplot2), Python (Matplotlib, Seaborn)
Database / ETL Querying
SQL Server, Postgres, MySQL, SSIS, SSAS
DDL/DML statements, Subqueries, Joins, Normalization, Entity Relationship Diagrams, Star schema / Snow flake schema (Fact and Dimension tables) 
Cloud 
Azure
Others
Git, Software Development Life Cycle, Agile, IT Project Management, Prototyping, Gathering Functional and Non-Functional Requirements identification and analysis, SWOT Analysis, Root cause analysis, Dataflow diagrams, Use case diagrams,  Documentation,


:







Client: Discover                                                                                                                   Riverwoods, IL 
Data Scientist                                                                                                                                                Apr 2018-Present                                                                                                                                                    

Credit Card Fraud detection:		
 Designed and developed features related to fraudulent transactions. Built models to estimate the likelihood of credit card transactions being fraudulent by training artificial neural nets and other tree-based methods
 Deployed the models in production systems and a developed visualization to monitor several fraud KPIs, model accuracy charts like - Precision-Recall curve, confusion matrix, etc.

No Pay on Loan Prediction:
 Built Predictive models that analyzed large volumes of data regarding historical payments and then accurately determine which customers are unlikely to make their next payment.
 This valuable information can be used to prioritize bank staff to focus on high-risk customers, 
prepare the client for upcoming defaults and as an early-indicator for identifying potential issues with the underwriting process.

Credit default rates Prediction:
 Operationalized real time loan assessments by building models that predicted the likelihood of future defaults.
 Utilized past information of borrowers default rates to predict the likelihood of default for future borrowers. This made real time loan approval process easy allowing business to scale up and expand their loan portfolios.    
                                                                                                 
Market Mix Modeling:
? Developed and refined complex marketing mix statistical models in a team environment and ed with diverse functional groups with over $100M in annual marketing spend
? Responsible for all stages in the modeling process, from collecting, verifying, & cleaning data to visualizing model results, presenting results, and making client recommendations

Credit Risk Scoring :
? Built credit risk scorecards and marketing response models using SQL and SAS. Evangelized the complex  analysis into easily digestible reports for top executives in the company.
? Developed several interactive dashboards in Tableau to visualize nearly 2 Terabytes of credit data by designing a scalable data cube structure.    



Client: United Health Care Ltd                                                                                                     Mountain View, CA               
Data Scientist                                                                                                                                            May 2016- Apr 2018                                                                                                                                                                                       

Predicting Fraudulent claims:
 Developed a classification model that predicted the likelihood that a claim is fraudulent. Created a rank ordered queue of claims for fraud units to investigate with results from the model. 
 Fraud units are used to create a databased queue, investigating only those incidents that likely to require it.
 Used historical data with previous fraudulent for training the machine learning models using Algorithm like Random forest and Xgboost algorithms and used various statistical techniques like SMOTE Analysis to overcome class imbalance problem.
Claim Amount Prediction:
 Forecasted the ultimate claim amount based on the claim characteristics at the time the claim was filed. Built extremely accurate predictive models that lead to a better understanding of how much a claim will ultimately cost.
 This in turn helped the business to have an over view of how much to reserve for incurred but not reported loss amounts.
 The predicted developed loss for each claim was further used as a dependent variable for developing a pricing model.
Modelling claim Autopayment:
 Determined which claims need to be manually inspected and which claims can be auto payed using machine learning to build extremely accurate models.
 By auto paying claims where manual inspection provides little or no value helped business to close the claims quicker, driving their costs down and enhanced customer satisfaction.      
Risk Assessment:
 Used of predictive modeling to proactively identify potential customers who are at highest risk of poor health outcomes and will benefit most from intervention is one solution believed to improve risk management for providers transitioning to value-based payment.
 Created risk scoresbased on lab testing, biometric data, claims data, patient-generated health data, and the social determinants of health can give healthcare providers insight into which individuals might benefit from enhanced services or wellness activities.



Client: Kroger                                                                                                                                                Cincinnati, OH                                                                                                   
Data Scientist/Analyst                                                                                                                            Oct 2014- April 2016

Product Recommendation:
? Recommending Visually Similar Products Using Content Based Features using several factors like title similarity, image similarity, product description.  
? Implemented image similarity using VGG-16 architecture and sci-kit image packages in python to improveand personalize the customer experience.
 Carton Prediction: 
? Designed Logistics ML Carton Prediction algorithm to predict cartons arriving in a warehouse 10 days in advance. These predictions can be used in warehouses to allocate and plan staffing days in advance. 
? This methodology also standardized the way inbound estimates are calculated versus different methods at different warehouses right now
Price Optimization:
? Measured the price elasticity for products that experienced price cuts and promotions using regression methods; based on the elasticity, Kroger made selective and cautious price cuts for certain licensing categories.
Market Basket Analysis:
? Implemented market basket algorithms from transactional data, which helped identify products ordered together frequently. 
? Discovering frequent product sets helped unearth Cross sell and Upselling opportunities and led to better pricing, bundling and promotion strategies for sales and marketing teams.
Product Review Analysis:
 Extracted the customer reviews on Kroger products from Yelp and build a sentiment analysis model that extracts the negative reviews from the customer.
 Used NLTK and Sci-kit Learn Libraries to develop the text classifier


Alexion Pharmaceuticals                                                                                                                      NewHaven, CT
Data Analyst                                                                                                                                    May 2012 to Sep 2014
Responsibilities:
 Develop and implement databases, data collection systems, data analytics and other strategies that optimize statistical efficiency and quality.
 Utilized SAS and SQL to extract data from statewide databases for analysis.
 Acquired data from primary or secondary data sources and maintain databases/ data systems.
 Performed data Analysis using visualization  such as Tableau, Spotfire, and SharePoint to provide insights into the data.
 Configured Azure platform offerings for web applications, business intelligence using Power BI, Azure Data Factory etc.
 Data flow check with source to target mapping of the data.
 Data matrix creation for mapping the with the business requirements.
 Data profiling to cleanse the datain the data base and raise the dataissues found.
 Performed dataanalysis and dataprofiling using complex SQL on various sources systems including Oracle and Teradata.
 Involved with dataprofiling for multiple sources and answered complex business questions by providing data to business users.
 Assisted in mining data from the SQL database that was used in several significant presentations.
 Involved in SSIS packages to extract datafrom different sources like SQL server, MS Excel, MS Access, transform and then load into Dimension and Fact tables in DataWarehouse using SSIS.


ARIZONA DEPARTMENT OF EDUCATION,						Phoenix, AZ                                                                                                                                            
Role: QA Engineer                                                                  					Sep2007-May2009
Responsibilities: 
 Identified data streams and reviewed data models for testing strategies 
 Written test scripts for back-end validations and Validated mappings of correct and con- ducted data 
 Validating the load process of ETL, the target tables are populated according the data mapping provided that satisfies the transformation rules 
 Writing complex SQL queries using Case Logic, Intersect, Minus, Sub Queries, Inline Views, and Union in Oracle 
 Validating the Archive process to purge the data that meet the defined business rules. 
 Create stored procedures for Incremental extract based on time stamp, version date and record flag 
 Wrote UNIX Shell Scripts and command line utility to interact with Informatica Server 
 Implemented SCD I, II extracts based on the business requirement and client expectations 


TCS, India                                                                                       		Client: XEROX CORPORATION                                                                                                                                                                      Role: QA TESTER     								Jul 2005  Aug 2007                                                       
Responsibilities:  
 Planning and executing test cases, Reported defects in A R Tool 
 Composing and reporting daily and weekly reports 
 Giving creative suggestions to the customer and receives customer input and instructions. 
 Developed Automated Test Scripts Using DAML


Education:
Masters of  (2002-2004), VIT, India.
Bachelors of  (1998- 2002), SKIT, India.
 






",Data Scientist,emailed,indeed job post,resume
"
Clearance: T3 Secret DOD Clearance (Pending Eligible)

:

* Over 10+ years of experience as Big Data Engineer /Data Engineer and Data Analyst including designing, developing and implementation of data models for enterprise-level applications and systems.
*  IT experience this includes recent experience in Big Data/Hadoop Ecosystem Competence in using various Hadoop components such as MapReduce (MR1),YARN(MR2), HDFS, Pig, Hive, HBase, ZooKeeper, Oozie, Hue Experience in building highly reliable, scalable Big data solutions on Hadoop distributions Cloudera, Horton s, AWS EMR.
* Good experienced in Data Modeling and Data Analysis as a Proficient in gathering business requirements and handling requirements management.
* Pleasant experience ing in Agile/Scrum development environment participated in  discussions with client and contributed to project analysis and development specs.
* Experience in transferring the data using Informatica tool from AWS S3 to AWS Redshift Hands on experience in Normalization (1NF, 2NF, 3NF and BCNF) Denormalization techniques for effective and optimum performance in OLTP and OLAP environments.
* Expertise in moving structured schema data between Pig and Hive using HCatalog.
* Excellent ing experience in Scrum / Agile frame and Waterfall project execution methodologies.
* Experience in migrating the data using Sqoop from HDFS and Hive to Relational Database System and vice-versa according to client's requirement.
* Experience in SQL and good knowledge in PL/SQL programming and developed Stored Procedures and Triggers and Data Stage, DB2, Unix, Cognos, MDM, Hadoop, Pig.
* Experience with RDBMS like SQL Server, MySQL, Oracle and data warehouses like Teradata and Netezza.
* Proficient knowledge and hands on experience in writing shell scripts in Linux.
* Good Experience on importing and exporting the data from HDFS and Hive into Relational Database Systems like MySQL and vice versa using Sqoop.
* Good knowledge on NoSQL Databases including HBase, MongoDB, MapR-DB.
* Installation, configuration and administration experience in Big Data platforms Cloudera Manager of Cloudera, MCS of MapR.
* Strong experience and knowledge of NoSQL databases such as MongoDB and Cassandra.
* Strong Knowledge of Data Warehouse Architecture and Star Schema, Snow flake Schema, FACT and Dimensional Tables.
* Experience in using PL/SQL to write Stored Procedures, Functions and Triggers.
* Excellent  and analytical  with clear understanding of design goals of ER modeling for OLTP and dimension modeling for OLAP.
* Experience ing with Relational Database Management Systems (RDMS) Capable of processing large sets of structured, semi-structured and unstructured data and supporting systems application architecture Good understanding of service-oriented architecture (SOA) and web services like XML and SOAP.
* Experience in object-oriented analysis and design (OOAD), used modelling language (UML) and design patterns.
* Expertise in SQL Server Analysis Services (SSAS) and SQL Server Reporting Services (SSRS) .
* Involve in writing SQL queries, PL/SQL programming and created new packages and procedures and modified and tuned existing procedure and queries using TOAD.
* Good Understanding and experience in Data Mining Techniques like Classification, Clustering, Regression and Optimization.
* Experience in complete project life cycle (design, development, testing and implementation) of Client Server and Web applications.

ADEXEC Austin, TX (Remote)	 Mar' 17 to Present
Lead Data Scientist

Responsibilities: 

* Designed and developed software applications, testing, and building automation .
* Involved in start to end process of Hadoop jobs that used various  such as Sqoop, PIG, Hive, MapReduce, Spark and Shells scripts (for scheduling of few jobs).
* Involved in importing and exporting data between RDBMS and HDFS using Sqoop.
* Performed querying of both managed and external tables created by Hive using Impala.
* Designed/developed tables, views, various SQL queries, stored procedures, functions.
* Involved in PL/SQL code review and modification for the development of new requirements.
* Extracted data from existing data source and performed ad-hoc queries.
* Utilized SAS and SQL extensively for collecting, validating and analyzing the raw data received from the client.
* Executed data extraction programs/data profiling and analyzing data for accuracy and quality.
* Analyzed the data using advanced excel functions like Pivot tables, VLOOK up, visualizations to get the descriptive analysis of the data.
* Created Schema objects like Indexes, Views, and Sequences, triggers, grants, roles, Snapshots.
* Used advanced Microsoft Excel to create pivot tables, and other excel functions to prepare reports and dashboard with user data.
* Maintained numerous monthly scripts, executed on monthly basis, produces reports and submitted on time for business review.
* Developed ad-hoc reports using Crystal reports for performance analysis by business users.
* Implemented the Big Data solution using Hadoop, and hive to pull/load the data into the HDFS system.
* Installed and configured Hadoop and responsible for maintaining cluster and managing and reviewing Hadoop log files.
* Implemented and configured flows using Oozie to automate jobs.
* Migrated the needed data from MySQL in to HDFS using Sqoop and importing various formats of flat files into HDFS.
* As a Sr. Big Data Engineer, provided  expertise and aptitude to Hadoop  as they related to the development of analytics.
* Expertise in writing Hadoop Jobs to analyze data using MapReduce, Hive, and Pig.
* Experience in designing, building and implementing complete Hadoop ecosystem comprising of MapReduce, HDFS, Hive, Pig, HBase, MongoDB, and Spark.
* ed experience in Scrum / Agile frame and Waterfall project execution methodologies.
* Managed data from various file system to HDFS using UNIX command line utilities.
* ed in Azure environment for development and deployment of Custom Hadoop Applications.
* Managed and support of enterprise Data Warehouse operation, big data advanced predictive application development using Cloudera &.
* Extensively ed on Shell scripts for running SAS programs in batch mode on UNIX.
                                                                             
Oracle4u LLC, Dallas, TX.           	Sep' 15 to Jan' 17
Lead Data Scientist

Responsibilities: 

* Participated in requirements sessions to gather requirements along with business analysts and product owners.
* Involved in Agile development methodology active member in scrum meetings.
* Involvement in design, development and testing phases of Software Development Life Cycle (SDLC).
* Installed and configured Hive and also written Hive UDFs and Cluster coordination services through Zookeeper.
* Architected, Designed and Developed Business applications and Data marts for reporting.
* Involved in different phases of Development life including Analysis, Design, Coding, Unit Testing, Integration Testing, Review and Release as per the business requirements.
* Developed Big Data solutions focused on pattern matching and predictive modeling
*  of this project is to build a data lake as a cloud based solution in AWS using Apache Spark.
* Installed and configured Hadoop Ecosystem components.
* ed on implementation and maintenance of Cloudera Hadoop cluster.
* Created Hive External tables to stage data and then move the data from Staging to main tables
* Implemented the Big Data solution using Hadoop, hive and Informatica to pull/load the data into the HDFS system.
* Pulling the data from data lake (HDFS) and massaging the data with various RDD transformations.
* Experience in Server infrastructure development on Gateway, ELB, Auto Scaling, Dynamo DB, Elastic search, Virtual Private Cloud (VPC)
* Involved in Kafka and building use case relevant to our environment.
* Developed Scala scripts, UDF's using both Data frames/SQL and RDD/MapReduce in Spark for Data Aggregation, queries and writing data back into RDBMS through Sqoop.
* Developed Spark code using Scala and Spark-SQL/Streaming for faster processing of data.
* Developed Oozie flow jobs to execute hive, Sqoop and MapReduce actions.
* Provided thought leadership for architecture and the design of Big Data Analytics solutions for customers, actively drive Proof of Concept (POC) and Proof of Technology (POT) evaluations and to implement a Big Data solution.
* Responsible for developing data pipeline using flume, Sqoop and pig to extract the data from weblogs and store in HDFS.
* Imported the data from different sources like HDFS/HBase into Spark RDD and developed a data pipeline using Kafka and Storm to store data into HDFS.
* Used Spark streaming to receive real time data from the Kafka and store the stream data to HDFS using Scala and NoSQL databases such as HBase and Cassandra.
* Documented the requirements including the available code which should be implemented using Spark, Hive, HDFS, HBase and Elastic Search.
* Developed Spark code using Scala for faster testing and processing of data.
* Explored MLlib algorithms in Spark to understand the possible Machine Learning functionalities that can be used for our use case.
* Apache Hadoop installation & configuration of multiple nodes on AWS EC2 system
* Developed Pig Latin scripts for replacing the existing legacy process to the Hadoop and the data is fed to AWS S3.
* Collaborated with Business users for requirement gathering for building Tableau reports per business needs.
* Developed continuous flow of data into HDFS from social feeds using Apache Storm Spouts and Bolts.
* Involved in loading data from Unix file system to HDFS.

Environment: Hadoop 3.0, 3NF, flume 1.8, Sqoop 1.4, pig 0.17, YARN, HDFS, HBase 1.2, Kafka, Scala 2.12, NoSQL, Cassandra 3.11, Elastic Search, MLlib, Teradata 15, Sqoop, MapReduce, UNIX, Zookeeper 3.4

PWC, Irvine, CA	 Jan' 15 to Aug' 15
Sr. Data Scientist

Responsibilities: 

* Installed and configured Apache Hadoop, Hive and Pig environment on the prototype server.
* Configured SQL database to store Hive metadata.
* Loaded unstructured data into Hadoop File System (HDFS).
* Created ETL jobs to load Twitter JSON data and server data into MongoDB and transported MongoDB into the Data Warehouse.
* Created reports and dashboards using structured and unstructured data.
* Performed SQL tuning Advisor for tuning of SQL queries and database systems.
* Created User accounts, Roles and granting required access permissions and privileges to the users.
* Partitioning and re-organization of table and indexes.
* Responsible in configuring and backing up database using RMAN, Hot backups, Cold backups and Logical backups.
* Written documentation for backups and cloning of database for future reference.
* Migrated databases from HP-UX to IBM AIX platforms.
* Continuously monitored database performance during batch jobs running on the database using AWR report and OEM.
* Used Data pump to take the logical backups of databases.
* ed on ASM instances for 11g databases.
* Installed and configured 12c software and database.
* ed on pluggable and container 12c database model.
* Performed other DBA activities including space management and performance monitoring.

Environment: HP-UX, Big Data Database Version 10g/11g, RMAN, Data guard, RAC, ASM, Oracle Enterprise Manager Grid Control 12c, Opatch, Shell Scripting.

Ford Inc, Detroit MI ( Remote)	Jul' 14 to Dec' 14
Sr. Hadoop Developer

Responsibilities:

* Cluster capacity planning along with operations team and management team and Cluster maintenance as well as creation and removal of nodes, HDFS support and maintenance.
* Manage and review Hadoop log files, File system management and monitoring.
* Involved in Cluster upgrade and required jobs are modified.
* Involved in implementing security on Hadoop Cluster with Kerberos by ing along with operations team to move non secured cluster to secured cluster.
* Data migration from RDMS to hadoop using sqoop for analysis and implemented Oozie jobs for automatic data imports from source.
* Used Hive data warehouse tool to analyze the data in HDFS and developed Hive queries.
* To analyze data migrated to HDFS, used Hive data warehouse tool and developed Hive queries.
* Created external tables with proper partitions for efficiency and loaded the structured data in HDFS resulted from MR jobs.
* Implemented Hive UDF for comprehensive data analysis.
* Responsible for troubleshooting MapReduce jobs by reviewing the log files.
* Involved in importing the real time data to hadoop using Kafka and implemented the Oozie job for daily imports.
* Involved with various teams on and offshore for understanding of the data that is imported from their source.
* Developed generic Shell scripts to automate Sqoop job by passing parameters for data imports.
* Involved in data visualization and provided the files required for the team by analysing the data in hive and developed Pig scripts for advanced analytics on the data.
* As a part of POC used the Amazon AWS S3 as an underlying file system for the Hadoop and implemented the elastic Map-Reduce jobs on the data in S3 buckets.
* Participated with operations team for Spark Installation on Secured cluster.
* Provided updates in daily SCRUM and Self planning on start of sprint and provided the planned task using JIRA. In sync up with team in order to pick priority task

Environment: Hadoop, HDFS, Pig, Sqoop, Spark, MapReduce, Cloudera, Snappy, Zookeeper, NoSQL, HBase, Shell Scripting, Ubuntu, Linux Red Hat.

Wal-Mart, San Jose, CA 	March' 13 to Jun' 14
Sr. Hadoop Developer

Responsibilities: 

* Developed Big Data Solutions that enabled the business and technology teams to make data-driven decisions on the best ways to acquire customers and provide them business solutions.
* Involved in installing, configuring and managing Hadoop Ecosystem components like Hive, Pig, Sqoop and Flume.
* Migrated the existing data to Hadoop from RDBMS (SQL Server and Oracle) using Sqoop for processing the data.
* ed on Importing and exporting data from different databases like MySQL, Oracle into HDFS and Hive using Sqoop.
* ed on Writing Hive queries for data analysis to meet the business requirements.
* Responsible for loading unstructured and semi-structured data into Hadoop cluster coming from different sources using Flume and managing.
* Developed MapReduce programs to cleanse and parse data in HDFS obtained from various data sources and to perform joins on the Map side using distributed cache.
* Used Hive data warehouse tool to analyze the data in HDFS and developed Hive queries.
* Created internal and external tables with properly defined static and dynamic partitions for efficiency.
* Implemented Hive custom UDF's to achieve comprehensive data analysis.
* Used the RegEx, JSON and Avro SerDe's for serialization and de-serialization packaged with Hive to parse the contents of streamed log data.
* Developed Pig scripts for advanced analytics on the data for recommendations.
* Experience in writing Pig UDF's and macros.
* Exported the business required information to RDBMS using Sqoop to make the data available for BI team to generate reports based on data.
* Developed generic Shell scripts to automate Sqoop job by passing parameters for data imports.
* Migrated the existing data to Hadoop from RDBMS (SQL Server and Oracle) using Sqoop for processing the data.
* Implemented daily flow for extraction, processing and analysis of data with Oozie.
* Responsible for troubleshooting MapReduce jobs by reviewing the log files.

Environment: Hadoop, MapReduce, Hive, Oozie, Sqoop, Flume, JAVA, LINUX, CentOS



Mindseeker Inc, Laurel, MD	 Nov' 11 to feb 13                                                                                                                
Hadoop Developer

Responsibilities: 

* Involved in analyzing the requirements and establish development capabilities to support future opportunities.
* Involved in Design and Development of  specifications using Hadoop technology.
* Handled importing of data from various data sources, performed transformations using PIG, MapReduce, loaded data into HDFS and extracted the data from MySQL into HDFS using SQOOP.
* ed on streaming the analyzed data to the existing relational databases using SQOOP by making it available for visualization and report generation to the BI team.
* Created Pig Latin scripts to sort, group, join and filter the enterprise wise data.
* Analyzed Web server log data using Apache Flume.
* Implemented schedulers on the Job tracker to share resources of the cluster for the MapReduce jobs given by cluster.
* Used Sqoop to import and export the data from HDFS.
* Moved data from HDFS to Cassandra using MapReduce and Bulk Output Format class.
* Participated with the admin team in designing and migrating the cluster from Cloudera to HDP.
* Developed some helper class for abstracting Cassandra cluster connection act as core toolkit.
* Involved in Agile methodologies, daily Scrum meetings, Sprint planning.
* Wrote Query Mappers and MQ Experience in Junit test Cases.
* Involved in designing the next generation data architecture for the unstructured and semi structured data.

Environment: HDFS, MapReduce, Cassandra, Pig, Hive, Sqoop, Maven, Log4j, Junit, Tableau.


Kforce Inc, Reston, VA 	Feb' 10 to Oct' 11
Hadoop Developer

Responsibilities:

* Knowledge on the real-time message processing systems (Storm, S4)
* Collected the business requirements from the Business Partners and Experts.
* Involved in installing Hadoop Ecosystem components.
* Responsible to manage data coming from different sources.
* Used Apache flume to ingest log data from multiple sources directly into HDFS.
* Customized flume to enrich data with LDAP lookups and GOIP lookups.
* Involved in writing Map Reduce Programs which are running on the cluster.
* Involved in HDFS maintenance and loading of structured and unstructured data.
* Installed and configured Pig and also written PigLatin scripts.
* Wrote MapReduce job using Java API.
* Wrote MapReduce job using Pig Latin.
* Imported data from MySQL to HDFS by using Sqoop to load data.
* Developed Scripts and Batch Job to schedule various Hadoop Program.
* Wrote Hive queries for data analysis to meet the business requirements and generated reports.
* Created Hive tables by using Hive QL and ed on them.
* Wrote Hive UDF for frequently used HiveQL queries.
* Utilized Agile Scrum Methodology to help manage and organize a team of 4 developers with regular code review sessions.
* Regular meetings with  teams and active participation in code review sessions with other developers.
* Used Continuum for integration testing and JUnit for unit testing.

Environment: Hadoop, HDFS, MapReduce, Unix, Flume, Python, Pig, MySQL , MySQL bench Hive
Java , Hbase, Storm, Flume, Zookeeper, Putty, Eclipse , Cloudera, Eclipse , Linux.

Bank of America, Charlotte,NC 	Apr' 08 to Jan' 2010                                                                                           
Java Developer

Responsibilities:

* Involved in the design, development and analysis of the application. Participated in users' meetings along with the Business Analyst for requirements gathering and analysis
* Designed the UML class diagrams and sequence diagrams using Rational Rose
* Used HTML, XML, CSS, AJAX and JavaScript for developing front end pages and client side validations
* Developed the application using Spring MVC Frame integrated with Hibernate
* Used Spring IOC, Spring ORM, Spring MVC modules for developing the application
* ed on Struts Tiles Frame and written modules for internationalization using i18N concept in the front end
* Involved in designing of LDAP-backed authentication system for employee secure login
* Widely used Design Patterns like DAO, Singleton, Factory Pattern, DTO in the process of system designing and development
* Communicating with the BA's for any requirements changes, attending meetings with them regarding design reviews and code reviews
* Agile delivery of software using SCRUM methodology
* Implemented persistence layer using Hibernate that use the POJO classes to represent the persistence database tables
* Used Oracle as a backend database and was responsible to configure and write stored procedures to create, insert, delete and modify data in the database
* Coding JUnit test scripts of the SG, SSG, Role Management, Event Notification and Authorization modules
* Used MAVEN for creating and deploying the .war files
* Developed the J2EE components in the IDE tool, RAD
* Code Review for Roster, System, Node and Product modules
* Development, enhancements and bug fixes in the application

Environment: JDK 1.4/1.5, J2EE, JSP, Servlets, Spring MVC, Hibernate 3.x, Struts Tiles and Validator Frames, i18n, JUnit, RAD 6, WebSphere Application Server, HTML/DHTML, AJAX, CSS, XML, XSLT, JavaScript, Rational Rose, Agile(Extreme Programming), Oracle, IPlanet Directory Server, Web Services, Apache Axis, Maven, CVS


Capgemini, El Paso TX 	June 05 to Mar 08
Java Developer

Responsibilities: 

* Interacted with business users to gather check-in online module requirement.
* Develop the sequence and class diagrams and get it approved from the client
* Designing classes using design pattern methodologies such as Singleton, Service Locator, and DAO factory and session faade patterns.
* Developed Hibernate DAO classes and with the Spring frame  manager classes retrieve and save the data
* Involved in Performance tuning.
* Created the front end using JSP, HTML, XML, and advanced JavaScript.
* Used Oracle as backend
* Performed Unit Cases for components using JUnit
* Configuration management - Clear case used.
* Provided support for user acceptance testing & performance testing.

Environment: Java1.6, JSP, Servlets, JSTL, Struts, Hibernate, Multi-threading, JAXB2.0, SOAP, HTML, CSS, XML, PL/SQL, TOAD, Rational Rose, JavaScript, Weblogic 8.1, eclipse 3.0, Maven Java Messaging services(JMS), MQ and Apollo mainframe server, Content management application (CMA).

Education

* M.S., Johns Hopkins University, Baltimore, MD -2004
* Bs Cs ., Johns Hopkins University, Baltimore, MD -2001
",Data Scientist,emailed,indeed job post,resume
"
 

	? 	A Mathematical Science major senior currently looking for a full-time data analyst job after graduation. 
 

University of California, Santa Barbara, CA                                                                                           Expected June 2020  
Major: Mathematical Science B.S.       Minor: Statistical Science 
Course: multivariate calculus, linear algebra, ODE, PDE, linear regression, complex analysis 
 

Program Language  
    * python, R, SQL, SAS Data Analysis 
    * pandas, scipy, numpy, dplyr, ggplot2, matplotlib, excel 
 EXPERIENCE 

Food Bank of Santa Barbara, CA                                                                                                    March 2019-August 2019 
Program Evaluation Intern 
* Tracked data for specific modules and based on the project requirements 
* Entered survey data into spreadsheets and produce  statistic using R 
* Create charts as visual representation of results to provide data support for operational strategy 
Disabled Student Program, UCSB, CA                                                                                                  January 2018-Present Notetaker  
* Produced accurate, legible and clearly stated notes and uploaded in a timely manner 
* Maintained report form to protect student confidentiality and  relationships 
PROJECT EXPERIENCE 

Multiple Linear Regression Analysis                                                                                                                     Spring 2019 
* Built a regression model in R to predict the determining factors of the housing price and produced statistical tests and data visualization to optimize the model 
* Participated in conducting multiple linear regression analysis and outlier test in R to examine the determining factors of the cement density 
* Delivered a formal report based on the analysis by R markdown 
Research on the effect of GDP recession on house prices                                                                                 Summer 2019 
* Independently analyzed data from Zillow, Wikipedia and US Department of Commerce and completed data cleaning, data wrangling, querying, hypothesis testing by using the pandas, scipy and numpy library in Python 
* Verified the housing prices in university towns and that were less affected by the recession 
Research on the weather pattern near Ann Arbor, Michigan                                                                          Summer 2019 
* Conducted data visualization by pandas, matplotlib in Python 
* Collected climate data near Ann Arbor, Michigan through NCEI 
* Completed data cleaning and wrangling by pandas, plotted a line graph by matplotlib 
* Recorded high and low temperatures by day of the year (2005-2014), shaded region between the recorded high and low temperature, overlaid a scatter of the broken high and low temperatures in 2015.      ",Data Scientist,emailed,indeed job post,resume
" :									

* 7 years of experience as Data Scientist/Engineer with strong  expertise, business experience, and communication  to drive high-impact business outcomes through data-driven innovations and decisions.
* Skilled in data cleansing, analyzing, interpreting results to business users with statistical modelling and visualization with Python, R, Alteryx
*  in business, analytics, database modeling and product design and development of applications
* Proficient in Feature Engineering of variables and Machine Learning algorithms (Linear, Logistics, Decision Trees, Random Forest, SVM, K-Nearest Neighbors, Bayesian) in Predictive Analytics
* Designed SQL queries for marketing and sales teams and implemented dashboards and reports to prepare weekly and monthly metrics to Directors and VPs of departments
* Expertise in Big data  such as Spark, Hadoop, HiveQL, Pig on AWS cloud environment to create HDFS files from Unstructured data
* Experience in designing stunning visualizations using Tableau software and publishing and presenting dashboards, Stories on web and desktop platforms. 
* Extensive experience in machine learning and statistics to draw meaningful insights from data. I am good at communication and storytelling with data. 
* Expertise in all aspects of Agile SDLC from requirement analysis, Design, Development Coding, Testing, Implementation, and maintenance
* Delivered enterprise level  under budget and on time to Fortune 500 clients with global cross functional teams
* Extensive hands-on experience and high proficiency with structures, semi-structured and unstructured data, using a broad range of data science programming languages and big data  including R, Python, Spark, SQL, ScikitLearn, Hadoop MapReduce 
* Regularly accessing JIRA tool and other internal issue trackers for the Project development. 
* Skilled in System Analysis, E-R/Dimensional Data Modeling, Database Design and implementing RDBMS specific features. 
* Utilize analytical applications/libraries like Plotly D3JS to identify trends and relationships between different pieces of data, draw appropriate conclusions and translate analytical findings into marketing strategies that drive value. 
* Strong knowledge of statistical methods (regression, time series, hypothesis testing, randomized experiment), machine learning, algorithms, data structures and data infrastructure. 
* Expertise in  proficiency in Designing, Data Modeling Lead for Architecting Data Warehouse/Business Intelligence Applications. 
* Defining job flows in Hadoop environment-using  like Oozie for data scrubbing and processing. 
* Experience in Data migration from existing data stores to Hadoop. 
* Developed MapReduce programs to perform Data Transformation and analysis.


:

Masters of Science, Business Analytics, Silicon Valley University, San Jose, CA 
Bachelors of , Jawaharlal Nehru Technological University, Hyderabad, India





 AND :
Databases/ETL/Query
Teradata, SQL Server, Postgres and Hadoop (Map Reduce); SQL, Hive, Pig and Alteryx
Visualization
Tableau, ggplot2 and R-Shiny
Statistics
Exploratory Data Analysis: Univariate/Multivariate Outlier detection, Missing value imputation, Histograms/Density estimation, EDA in Tableau 
Supervised Learning: Linear/Logistic Regression, Lasso, Ridge, Elastic Nets, Decision Trees, Ensemble Methods, Random Forests, Support Vector Machines, Gradient Boosting, Neural Nets
Unsupervised Learning: Principal Component Analysis, Association Rules, Factor Analysis, K-Means, Hierarchical Clustering, Market Basket Analysis
Machine Learning
R: caret, glmnet, forecast, xgboost, rpart, survival, arules, sqldf, dplyr, nloptr, lpSolve, ggplot
Python: pandas, numpy, scikit-learn, scipy, stats models, matplotlib,tensorflow
SAS: Forecast server, SAS Procedures and Data Steps
Spark: MLlib, GraphX
SQL: Subqueries, joins, DDL/DML statements
:	

  Client: Pacific Life, Newport Beach, CA
                                                             May 2018  Present
 Role: Senior Data Scientist  Marketing/Insurance
Responsibilities:
* Analyzed product plans in competitive markets using A/B testing and recommended metrics to increase operational efficiency with model development using DataRobot
* Implemented Data pipelines for big data processing using Spark Clusters in Amazon EMR, Python and performed customer segmentation
* Implemented business processing models using predictive & prescriptive analytics on transactional data with regression, classification
* Designed flows using SQL queries with Alteryx for data preprocessing such as cleansing and transformation of data to implement & automate KPI dashboards
* Implemented Logistic, Random forests with Python packages to decide insurance purchase by a USAA member
* Implemented HDFS clusters from unstructured marketing data platforms with HiveQL, Spark , Hadoop 
* Collaborated with business owners of products for understanding business needs and automated business processes and data storytelling in Tableau
* Used pandas, numpy, Seaborn, scipy, matplotlib, sci-kit-learn in Python for developing various machine learning algorithms  
* Executed process improvements in data flows using Alteryx processing engine
* ed as Data Architects and IT Architects to understand the movement of data and its storage
* Data Manipulation and Aggregation from different source using Nexus, Toad, Business Objects and SmartView. 
* Implemented Agile Methodology for building the applications and data frame development 
* Focus on integration overlap and Informatica newer commitment to MDM with the acquisition of Identity Systems.  
* Delivered various complex OLAP databases/cubes, scorecards, dashboards and reports. 
* Handled importing data from various data sources, performed transformations using Hive, MapReduce, and loaded data into HDFS. 
* Researched, evaluated, architected, and deployed new , frames, and patterns to build sustainable Big Data platforms for the clients 

Environment: R, Python, TensorFlow, Machine Learning, Tableau, Bigdata, Alteryx, Hive, OLAP, DB2, Metadata, MS Excel, DataRobot

  Client: Info-Matrix, Camp Hill, PA
                                                          Dec 2016  April 2018
 Role: Data Scientist - /Marketing
Responsibilities:
* Used pandas, numpy, Seaborn, scipy, matplotlib, sci-kit-learn in Python for developing various machine learning algorithms 
* Participated in all phases of data mining; data collection, data cleaning, developing models, validation, visualization and performed Gap analysis. 
* Implemented Logistic regression, TensorFlow with R packages  dplyr, mice, rpart 
* ed as Data Architects and IT Architects to understand the movement of data and its storage
* Data Manipulation and Aggregation froma different source using Nexus, Toad, Business Objects, PowerBI and SmartView. 
* Focus on integration overlap and Informatica newer commitment to MDM with the acquisition of Identity Systems.  
* Updated Python scripts to match training data with our database stored in AWS Cloud Search, so that we would be able to assign each document a response label for further classification. 
* Data transformation from various resources, data organization, features extraction from raw and stored. 
* Handled importing data from various data sources, performed transformations using Hive, MapReduce, and loaded data into HDFS. 
* Interaction with Business Analyst, SMEs, and other Data Architects to understand Business needs and functionality for various project solutions 
* Researched, evaluated, architected, and deployed new , frames, and patterns to built sustainable Big Data platforms for the clients 
* Identifying and executing process improvements, hands-on in various  such as Oracle, Informatica, Business Objects. 


Environment: Python, TensorFlow, Informatica 9.0, ODS, OLTP, Bigdata, Oracle 10g, Hive, OLAP, DB2, Metadata, MS Excel, H20.ai

Client: Accion International, Boston, MA			                                                           Dec 2014  Nov 2016 
Role: Financial Data Analyst

      Responsibilities:
* ed with several R packages including knitr, dplyr, SparkR, Causal Infer, space-time. 
* Implemented end-to-end systems for Data Analytics, Data Automation and integrated with custom visualization  using R, Mahout, Hadoop, andMongoDB. 
* Gathering all the data that is required from multiple data sources and creating datasets that will be used in theanalysis. 
* Extracted data using SQL from data sources and performed Exploratory Data Analysis (EDA) and Data Visualizations using R, and Tableau. 
* Implemented Uni-variate and Bi-variate analysis to understand the intrinsic effect/combined effects. 
* ed with Data Governance, Data quality, data lineage, Data architect to design various models and processes. 
* Independently coded new programs and designed Tables to load and test the program effectively for the given POC's using with Big Data/Hadoop. 
* Designed data models and data flow diagrams using MS Visio. 
* As an Architect implemented MDM hub to provide clean, consistent data for anSOA implementation. 
* Developed, Implemented &Maintained the Conceptual, Logical&PhysicalDataModels using Erwin for forwarding/Reverse Engineered Databases.  
* Lead the development and presentation of a dataanalytics data-hub prototype with the help of the other members of the emerging solutions team 
* Performed data cleaning and imputation of missing values using R. 
* Take up ad-hoc requests based on different departments and locations 
* Used Hive to store the data and perform datacleaning steps for huge datasets. 
* Created dash boards and visualization on regular basis using ggplot2 and Tableau.
* Creating customized business reports and sharing insights to the management. 
* ed with BTEQ to submit SQL statements, import and export data, and generate reports in Teradata. 
* Interacted with the other departments to understand and identify data needs and requirements and  with other members of the IT organization to deliver data visualization and reporting solutions to address those needs.

Environment: R, SQL, Informatica, ODS, OLTP, Oracle 10g, Hive, OLAP, DB2, Metadata, MS Excel, Mainframes MS Visio, Rational Rose, and Requisite Pro, Hadoop, PL/SQL, etc.


Client: Axis Bank, India      					                                                 Sep 2012  Nov 2014
Role: Marketing Data Analyst

Responsibilities:
* Analyzed survey response data to determine consumer preferences on client products and proposed recommendations
* Improved efficiency of business processes by 10% through implementation of data management procedures
* Automated the computations to determine market metric information on consumer demographic information
* Implemented predictive modeling techniques to increase long-term growth by 12% for products in US regions
* Developed a scoring mechanism using SAS based on customer segmentation to increase sales by 20%
* Performed Map Reduce Programs those are running on the cluster.
* Developed multiple MapReduce jobs in java for data cleaning and preprocessing. 
* Analyzed the partitioned and bucketed data and compute various metrics for reporting. 
* Involved in loading data from RDBMS and weblogs into HDFS using Sqoop and Flume. 
* ed on loading the data from MySQL to HBase where necessary using Sqoop. 
* Developed Hive queries for Analysis across different banners.
* Extracted data from Twitter using Java and Twitter API. Parsed JSON formatted twitter data and uploaded tothe database. 
* Launching Amazon EC2 Cloud Instances using Amazon Images (Linux/ Ubuntu) and Configuring launched instances with respect to specific applications.
* Exported the result set from Hive to MySQL using Sqoop after processing the data. 
* Analyzed the data by performing Hive queries and running Pig scripts to study customer behavior. 
* Have hands-on experience ing withSequence files, AVRO, HAR file formats and compression. 
* Used Hive to partition and bucket data. 
* Experience in writing MapReduce programs with Java API to cleanse Structured and unstructured data. 
* Created HBase tables to store various data formats of data coming from different portfolios. 
* ed on improvingthe performance of existing Pig and Hive Queries. 

Environment: SQL/Server, Oracle, MS-Office, Teradata, Informatica, ER Studio, XML, Business Objects, HDFS, Teradata 14.1, JSON, HADOOP (HDFS), Python, MapReduce, PIG, Spark, R Studio, MAHOUT, JAVA, HIVE, AWS.
Client: Persistent Systems Limited, Hyderabad, India		                                               Oct 2011  Aug 2012
Role: Senior Data Analyst	

  Responsibilities:
* Extracted and validated financial data from external data source like Quandl to generate reports to C-level executives
* Designed a data story frame and new financial benchmark metrics on Costs and departmental expenditures
* Implemented charts, graphs and distribution of revenues through visualization  in Tableau for CFOs
* Reduced 500 man-hours by auto cleaning of data with validations using Python and R to improve efficiency
* Predicted revenue based on R&D and Sales expenses using financial econometric models
* ed with large amounts of structured and unstructured data.
* Knowledge of Machine Learning concepts (Generalized Linear models, Regularization, Random Forest, Time Series models, etc.)
* ed in Business Intelligence  and visualization  such as Business Objects, ChartIO, etc.
* Configured the project on WebSphere 6.1 application servers
* Communicated with other Health Care info by using Web Services with the help of SOAP, WSDL JAX-RPC
* Conducted Design reviews and  reviews with other project stakeholders.
* Was a part of the complete life cycle of the project from the requirements to the production support
* Created test plan documents for all back-end database modules

Environment: MDM, Tableau, Statistical modeling, PL/SQL, HDFS, Teradata, Python, JSON, HADOOP (HDFS), MapReduce, PIG, R Studio, MAHOUT, JAVA, HIVE, AWS.",Data Scientist,emailed,indeed job post,resume
"Information Systems graduate and Data Analytics aficionado with 2 years of experience and certification in Data Science, Visualization and Machine Learning. Passionate about solving real-world problems with data using available resources at hand. Critical thinker and innovative solution explorer.   
Databases: MS SQL Server, MS Access, Teradata 
BI / Visualization: Microsoft Power BI, Tableau, Visio, ggplot2 
Methodologies: Data Mining, Data Discovery & Cleaning, Regular Expressions, Regression and Classification Model Building & Tuning, Clustering, Feature Engineering, Hyperparameter Tuning, Hypothesis Testing, NLP, Results Analysis & Presentation 
Algorithms: GLM, KNN, K-Means, SVM, Neural Net, Boosted Decision Trees, Bagging and Cross Validation, Nave 
Bayes, Random Forest, Linear Programming 
IDEs:  RStudio, Jupyter Notebook, H2O Driverless AI 
Business System Analysis: Data Flow Diagrams, Use Cases, Context Diagram, Data Modeling, Software 
Design/Requirements Specification gathering, Software Development Plan Languages:  R, T-SQL, Python, C, HTML 
 
 
	DecisionLogic, San Diego, CA 	 	 	 	        	 	             	       Sept 2018  Jul 2019 
Data and Business Analyst, IT Dept 
 Model Based Bank Transaction Categorization:  
- Set up a predictive web service using Azure ML Studio to categorize incoming bank transactions, achieving a Test AUC of over 95% and less than 1.5 sec web service response time 
- Built the categorization model in R utilizing fastText library for superfast training 
 Loan Default Score: 
- Built the Proof of Concept for detecting first time loan defaults using clients loan performance data, resulting in an AUC of 85% in the small dollar personal loan market 
- Completed milestones like Data Gathering, Wrangling, Visualizing, Model Building/Tuning and Reporting 
- Constructed the binomial classification model on an ensemble of LightGBM, XGBoost and GLM using H2Os Driverless AI  
 Income Stability Score: 
- Effectively drove the conceptualization, development and productionizing of Income Stability Score, ranging from 1-100, was built on 3 major attributes namely borrowers payroll frequency, payroll amount and payroll source to give an estimate of financial stability 
- Successfully deployed the score as a web service using Plumber API in R and scalable docker containers in Ubuntu 
 KPI and Performance Reporting: 
- Built and published numerous dashboards in Microsoft Power BI to monitor KPIs related to Client Success rates, Data Aggregators performance, Financial and Sales stats by implementing complex cross DB joins in 
SQL Server 
 
	San Diego Housing Commission, San Diego, CA 	 	 	 	 	      	     July 2017  May 2018 
Information Systems Intern, IT Dept 
 Demonstrated problem solving abilities by manipulating data in Salesforce, SQL Server (Joins, SSAS), and Excel (Pivot Tables, V-Lookup) to create a centralized data source to build 3 Tableau dashboards leveraging business intelligence 
 Undertook requirements gathering and analysis for 2  and developed Business flows 
 Successfully performed bulk geo-coding of tenant data stored in SQL Server using QGIS for business departments 
 
	San Diego State University, Fowler College of Business, San Diego, CA 	 	    	   June 2017  April 2018 
	Statistical Research Assistant, Management Information Systems Dept 	 
 Research assistant to the Management Information Systems department chair in the field of statistical analysis using R 
 Successfully reduced data processing time of analytical algorithm by more than 60% by building efficient R code 
 ed on building optimization functions, regression models and linear programming functions in R 
	Accenture Services PVT LTD, Pune, India 	 	 	      	 	  	        	        Nov 2013  Jul 2016 
Database Analyst, Data Quality Department 
 Demonstrated SQL  by performing data mining and analysis by using complex joins for and slicing and dicing the data by employing Stored Procedures, SSRS and SSAS services 
 Performed database, ETL, Functional and A/B testing on healthcare data in accordance with SDLC, STLC and agile methodology to verify quality and validate the completeness of data during data warehouse migration 
 ed on data requirements analysis for 2  with data mapping and lookup transformations  Certifications 
         Microsoft  Program for Data Science:  	 	 	 	 	 	 	     2019 
 Covers 11 Microsoft certified courses/ including Data Analysis, Visualization, Insights and Storytelling, Machine Learning, Statistics, etc. 
 
Relevant Academic  
	Toxic Online Comments Identification  	 	 	 	 	 	 	        	       Spring 2018 
 Text analysis project to classify online comments based on the level of toxicity using ML techniques in R and RShiny 
	Lending Club Loan Data Analysis Project 	 	 	 	 	 	 	                          Fall 2017 
 Classification of a given customer into predefined credit grades using classification, clustering and ML techniques in R 
          US Births Data Analysis 	 	 	 	 	 	 	 	 	                          Fall 2017 
 Census data analysis to compare, analyze and graph the Male and Female birth statistics using Python  like NLP, ""NumPy"", Matplotlib, ""Pandas"" and Jupyter notebooks 
          Gardening Catalog Response Mailing Dataset 	 	 	 	 	 	        	       Spring 2017 
 Marketing data analysis and statistical model building as a part of FICOs Academic Engagement Program using R and SQL 
          Alumni Outreach Database System 	 	 	 	 	 	 	                           Fall 2016 
 Database design project specifying required database model, normalization, and creation with user documentation by utilizing SSRS, SSAS and SQL Stored procedures in SQL Server Management Studio 
 
 
          San Diego State University | MS in Information Systems  GPA 3.6                         San Diego, CA | May 2018             Course: Statistical Analysis, Business Analytics, Enterprise Data Management, Big Data Analytics 
	- 	Volunteer Master Builder at Build IT Lab at SDSU Library 	 
 	 
          University of Pune | Bachelor of Engineering in Computer Science  	 	 	 Pune, India | June 2013 - 	In charge of IT dept. under engineering student body 
2 
 ",Data Scientist,emailed,indeed job post,resume
" 

* Over 8+ years of Experience on Machine Learning, Statistical Modelling, Predictive Modelling, Data Analytics, Data Modelling, Data Architecture, Data Analysis, Data Mining, Text Mining & Natural Language Processing (NLP)
* Proficient in gathering and analyzing the Business Requirements with experience in documenting System Requirement Specifications (SRS) and Functional Requirement Specifications (FRS).
* Extensive experience in Text Analytics, developing different Statistical Machine Learning, Data Mining solutions to various business problems and generating data visualizations using R, Python, and Tableau.
* Experience in Extracting data for creating Value Added Datasets using Python, R, SAS, Azure and SQL to analyze the behavior to target a specific set of customers to obtain hidden insights within the data to effectively implement the project s.
* Good experience of software development in Python (libraries used: Beautiful Soup, NumPy, SciPy, Matplotlib, Python-Twitter, Pandas data frame, net, Urllib2, MySQL)
* Experience on Artificial Intelligence algorithms, Business Intelligence, Analytics Models (like Decision Trees, Linear & Logistic Regression, Hadoop (Hive, PIG), R, Python, Spark, Scala, MS Excel, SQL and Postgre SQL, Erwin.
* Extensive experienced on business intelligence (and BI )  such as OLAP, Data warehousing, reporting and querying , Data mining and Spreadsheets.
* Efficient in developing Logical and Physical Data model and organizing data as per the business requirements using Sybase Power Designer, Erwin, ER Studio in both OLTP and OLAP applications
* Strong understanding of when to use an ODS or data mart or data warehousing.
* Experienced in employing RProgramming, MATLAB, SAS, Tableau and SQL for datacleaning, data visualization, risk analysis and predictive analytics
* Collaborated with the lead Data Architect to model the Data warehouse in accordance to FSLDM subject areas, 3NF format, Snow flake schema.
* Skilled in E-R/DimensionalDataModeling, Database Design and implementing RDBMS specific features.
* Experience on advanced SAS programming techniques, such as PROC SQL (JOIN/ UNION),PROC APPEND, PROC DATASETS, and PROC TRANSPOSE.
* Hands on experienced with Machine Learning, Regression Analysis, Clustering, Boosting, Classification, Principal Component Analysis and Data Visualization 
* Highly skilled in using Hadoop (pig and Hive) for basic analysis and extraction of data in the infrastructure to provide data summarization and data manipulation using Linux Commands.
*  Knowledge on designing and implementing a fully operational production grade large scale data solution on Snowflake Data Warehouse
* Familiarity with Crystal Reports, and SSRS - Query, Reporting, Analysis and Enterprise Information Management
* Experienced in Database using Oracle, XML, DB2, Teradata15/14, Netezza, server, Big Data and NoSQL.
* ed closely with other data scientists to create data driven products.
* Strong experienced in Statistical Modeling/Machine Learning and Visualization 
* Proficient in Hadoop, HDFS, Hive, MapReduce, Pig and NOSQL databases like MongoDB, HBase, Cassandra and expertise in applying data mining techniques and optimization techniques in B2B and B2C industries and proficient Machine Learning, Data/Text Mining, Statistical Analysis & Predictive Modeling.
* Experienced in Data Modeling & Data Analysis experience using Dimensional DataModeling and Relational Data Modeling, Star Schema/Snowflake Modeling, FACT& Dimensions tables, Physical & Logical Data Modeling.
	
 :

Languages
HTML5, DHTML, WSDL, CSS3, C, C++, XML, R/R Studio, SAS Enterprise Guide, SAS, R (Caret, Weka, ggplot) ,  python 
Software/Libraries
Keras, Caffe, TensorFlow, OpenCV, Scikit-learn, Pandas, NumPy, Microsoft Visual Studio, Microsoft Office.
Development 
Microsoft SQL Studio, IntelliJ, Eclipse, NetBeans.
Packages
ggplot2, caret, dplyr, RWeka, gmodels, RCurl, tm, C50, twitter, NLP, Reshape2, rjson, plyr, pandas, numpy, seaborn, scipy, Matplot lib, scikit-learn, Beautiful Soup, Rpy2, sqlalchemy.
Machine Learning Algorithms
Neural Nets, Decision trees, Support Vector Machines, Random forest, Convolutional Neural Nets, Logistic Regression, PCA, K- means, KNN.
Development Methodologies
Agile/Scrum, UML, Design Patterns, Waterfall

Database
SQL, Hive, Impala, Pig, Spark SQL, Databases SQL-Server, My SQL, MS Access, HDFS, HBase, Teradata, Netezza, Mongo DB, Cassandra.
Reporting 
MS Office (Word/Excel/PowerPoint/ Visio/Outlook), Crystal Reports XI, SSRS, Cognos 7.0/6.0.
Big Data 
Hadoop, Hive, HDFS, Map Reduce, Pig.
BI 
Microsoft Power BI, Tableau, SSIS, SSRS, SSAS, Business Intelligence Development Studio (BIDS), Visual Studio, Crystal Reports, Informatica 6.1.
Database Design  and Data Modeling
MS Visio, ERWIN 4.5/4.0, Star Schema/Snowflake Schema modeling, Fact & Dimensions tables, physical & logical data modeling, Normalization and De-normalization techniques, Kimball &Inmon Methodologies:

Bachelors of Science, Computer science Engineering                                                       2011
GITAM University

:	

Client: Century link, Littleton, CO						             Aug 2018 - Till Date
Role: Data Scientist.

 Description:  CenturyLink is the second largest U.S. communications provider to global enterprise customers. With customers in more than 60 countries and an intense focus on the customer experience, CenturyLink strives to be the world's best neting company by solving customers'? increased demand for reliable and secure connections.It also serves as its customers'? trusted partner, helping them manage increased net and IT complexity and providing managed net and cyber security solutions that help protect their business.

Responsibilities:
* Built data pipelines for reporting, alerting, and data mining. Experienced with table design and data management using HDFS, Hive, Sqoop, MySQL.
* ed with statistical models for data analysis, predictive modeling, machine learning approaches and recommendation and optimization algorithms.
* ing in Business and Data Analysis, Data Profiling, Data Migration, Data Integration and Metadata Management Services.
* ed extensively on Databases preferably Oracle 11g/12c and writing PL/SQL scripts for multiple purposes.
* AnalyzingBusiness Intelligence Reportingrequirements and translating them into data sourcing and modeling requirements including Dimensional & Normalized data models, Facts, Dimensions, Snowflake Schemas. 
* Design, coding, unit testing of ETL package source marts and subject marts using Informatica ETL processes for Oracle database.
* ed with Big Data  such Hadoop, Hive, Map Reduce
* Developed MapReduce/Spark Python modules for machine learning & predictive analytics in Hadoop on AWS. Implemented a Python-based distributed random forest via Python streaming.
* Setup storage and data analysis  in Amazon Web Services cloud computing infrastructure.
* A highly immersive Data Science program involving Data Manipulation & Visualization, Web Scraping, Machine Learning, Python programming, SQL, GIT, Linux Commands, NoSQL, MongoDB, Hadoop.
* Performed scoring and financial forecasting for collection priorities using Python, R and SASmachinelearning algorithms.
* Handled importing data from various data sources, performed transformations using Hive, MapReduce, and loaded data into HDFS and batch processing with Linux
* Managed existing team members lead the recruiting and on boarding of a larger DataScience team that addresses analytical knowledge requirements.
* Created SQL scripts and analyzed the data in MS Access/Excel and ed on SQL and SASscript mapping.
* Rapid model creation in Python using pandas, numpy, sklearn, and plot.ly for data visualization. These models are then implemented in SAS where they are interfaced with MSSQL databases and scheduled to update on a timely basis.
* Attained good knowledge in Hadoop Data Lake Implementation and HADOOP Architecture for client business data management.
* Above scoring models resulted in millions of dollars of added revenue to the company and a change in priorities of the entire company.

Environment : R, SQL, Python 2.7.x, SQL Server 2014, regression, logistic regression, random forest, neural nets, Topic Modeling, NLTK, SVM (Support Vector Machine), JSON, XML, HIVE, HADOOP, PIG, Sklearn, SciPy, Graph Lab, No SQL, SAS, SPSS, Spark, Hadoop, Kafka, HBase, MLib.



Client: Verizon, Richardson, TX							May 2017 - Jul 2018
Role: Data Scientist.	
            Description:  Verizon Communications Inc. is an American multinational telecommunications conglomerate and a corporate component of the Dow Jones Industrial Average. The Company, through its subsidiaries, provides communications, information and entertainment products and services to consumers, businesses and governmental agencies. Its segments include Wireless and Wireline. The Wireless segment offers communications products and services, including wireless voice and data services and equipment sales, to consumer, business and government customers across the United States.

           Responsibilities:
* Responsible for performing Machine-learning techniques regression/classification to predict the outcomes.
* Responsible for design and development of advanced R/Python programs to prepare to transform and harmonize data sets in preparation for Modeling.
* Identifying and executing process improvements, hands-on in various  such as Oracle, Informatica, and Business Objects.
* Designing and implementing data warehouses and data marts using components of Kimball Methodology, like Data Warehouse Bus, ConformedFacts & Dimensions, Slowly Changing Dimensions in Snowflake Schema
* Develop and implement innovative AI and machine learning  that will be used in the Risk.
* Performed the feature engineering of supervised and unsupervised machine learning models.
* Implemented SparkMLLib utilities such as including classification, regression, clustering, collaborative filtering,and dimensionality reductions
* Utilized Convolution Neural Nets to implement a machine learning image recognition componentusing TensorFlow.
* Strong in ETL and data integration experience in developing ETL mappings and scripts using Informatica
* Interaction with Business Analyst, SME and other Data Architects to understand Business needs and functionality for various project solutions.
* Designed the prototype of the Data mart and documented possible outcome from it for end-user.
* Involved in business process Modeling using UML.
* Handled importing data from various data sources, performed transformations using Hive, Map Reduce, and loaded data into HDFS.
* ed on Spark tool collaborating with ML libraries in eliminating a shotgun approach to understand customer buying patterns.
* Responsible for handling Hive queries using Spark SQL that integrates with Spark environment.
* Created SQL tables with referential integrity and developed queries using SQL, SQL*PLUS, andPL/SQL.
* Involved with Data Analysis primarily Identifying Data Sets, Source Data, Source Meta Data, Data Definitions, and Data Formats
* Performance tuning of the database, which includes indexes, and optimizing SQL statements, monitoring the server.
* Updated Pythonscripts to match trainingdatawith our database stored in AWS Cloud Search, sothat we would be able to assigneach document a response label for further classification.
* Wrote simple and advanced SQL queries and scripts to create standard and Adhoc reports for senior managers.
* Created PL/SQL packages and Database Triggers and developed user procedures and prepared user manuals for the new programs.

Environment: Python, MDM, MLLib, PL/SQL, Tableau, Git,NLP,SQL Server, MLLib, Scala NLP, SSMS, ERP,    CRM, Netezza, Cassandra, SQL, PL/SQL, SSRS, Informatica, Spark, Azure, R Studio, MongoDB, JAVA, HIVE.

 Client:  Direct Energy - Houston, TX                                                                     Jan 2016 - Apr2017          
Role: Data Scientist.	

 Description:  Direct Energy is a North American retailer of energy and energy services. The company was founded in 1986. The company has more than six million customers in Canada and the United States, making it the largest energy retailer in North America.

Responsibilities:

* Involved in extensive hoc reporting, routine operational reporting, and data manipulation to produce routine metrics and dashboards for management
* Created parameters, action filters and calculated sets for preparing dashboards and sheets in Tableau.
* Interacting with other datascientists and architects, custom solutions for data visualization using  like a tableau and Packages in Python.
* Involved in running Map Reduce jobs for processing millions of records.
* Written complex SQL queries using joins and OLAP functions like Count, CSUM, and Rank etc.
* The building, publishing customized interactive reports, report scheduling and dashboards using Tableauserver.
* Developed in Python programs for manipulating the data reading from various Teradata and convert them as one CSV Files.
* Performing statistical data analysis and data visualization using Python.
* ed on creating filters and calculated sets for preparing dashboards and sheets in Tableau.
* Created data models in Splunk using pivot tables by analyzing the vast amount of data and extracting key information to suit various business requirements.
* Created new scripts for Splunk scripted input for the system, collecting CPU and OS data.
* Implemented data refreshes on Tableau Server for biweekly and monthly increments based on a business change to ensure that the views and dashboards were displaying the changed data accurately.
* Developed normalized Logical and Physical database models for designing an OLTP application.
* Knowledgeable in AWS Environment for loading data files from on prim to Redshift cluster.
* Performed SQL Testing on AWSRedshift databases.
* Developed TeradataSQLscripts using OLAP functions like rank and rank over to improve the query performance while pulling the data from large tables.
* Developed and implemented SSIS, SSRS and SSAS application solutions for various business units across the organization.
* Designed the Data Marts in dimensional data modelling using star and snowflake schemas.
* Analyzed DataSet with SASprogramming, R and Excel.
* Publish Interactive dashboards and schedule auto-data refreshes
* Maintenance of large data sets, combining data from various sources by Excel, Enterprise, and SAS Grid, Access and SQL queries.
* Created Hive queries that helped market analysts spot emerging trends by comparing incremental data with Teradata reference tables and historical metrics.
* Design and development of ETL processes using InformaticaETL  for dimension and fact file creation.
* Develop and automate solutions for a new billing and membership Enterprise data Warehouse including ETL routines, tables, maps, materialized views, and stored procedures incorporating Informatica and Oracle PL/SQL ets.
* Performed analysis of implementing Spark uses Scala and wrote spark sample programs using PySpark.

Environment:SQL/Server, Oracle 10g/11g, MS-Office, Teradata, Informatica, ER Studio, XML, R connector, Python, R, Tableau 9.2
 Client:  Becton Dickinson - Franklin Lakes, NJ                                                                                      Mar 2014 - Dec 2015
Role:  Data Scientist/R Developer.	

 Description:  BD is a global medical technology company that is advancing the world of health by improving medical discovery, diagnostics and the delivery of care. BD leads inpatient and healthcare er safety and the  that enable medical research and clinical laboratories.

Responsibilities:

* The conducted analysis in assessing customer consuming behaviors and discover the value of customers with RMF analysis, applied customer segmentation with clustering algorithms such as K-Means Clustering and HierarchicalClustering.
* Collaborated with data engineers to implement the ETL process, wrote and optimized SQL queries to perform data extraction and merging from Oracle.
* Involved in managing backup and restoring data in the live Cassandra Cluster.
* Used R, Python, and Spark to develop a variety of models and algorithms for analytic purposes.
* Performed data integrity checks, data cleaning, exploratory analysis and feature engineer using R and Python.
* Developed personalized product recommendation with Machine learning algorithms, including Gradient Boosting Tree and Collaborative filtering to better meet the needs of existing customers and acquire new customers.
* Used Python and Spark to implement different machine learning algorithms, including Generalized Linear Model, RandomForest, SVM, Boosting and Neural Net.
* Evaluated parameters with K-Fold Cross Validation and optimized performance of models.
* ed on benchmarking Cassandra Cluster using the Cassandra stress tool.
* A highly immersive Data Science program involving Data Manipulation and Visualization, Web Scraping, Machine Learning, GIT, SQL, LINUX Commands, Python programming, No SQL.
* ed on data cleaning, data preparation, and feature engineering with Python, including Numpy, Scipy, Matplotlib, Seaborn, Pandas, and Scikit-learn.
* Identified risk level and eligibility of new insurance applicants with MachineLearning algorithms.
* Determined customer satisfaction and helped enhance customer using NLP.
* Utilized SQL and HiveQL to query, manipulate data from variety data sources including Oracle and HDFS, while maintaining data integrity.
* Performed datavisualization and Designeddashboards with Tableau and D3.js and provided complexreports, includingcharts, summaries, and graphs to interpret the findings to the team and stakeholders.


Environment: R, MATLAB, MongoDB, exploratory analysis, feature engineering, K-Means Clustering, Hierarchical Clustering, Machine Learning), Python, Spark (MLlib, PY Spark), Tableau, Micro Strategy, SAS, Tensor Flow, regression, logistic regression, Hadoop 2.7, OLTP, random forest, OLAP, HDFS, ODS, NLTK, SVM, JSON, XML and MapReduce.

 Client:  ZETA Interactive, India                                                                           Dec 2012 - Feb 2014
Role: Data Analyst.

Description: We're Zeta Global, a data-driven marketing technology company that helps brands acquire more customers, keep the ones they have for longer and grow their value. We create personalized customer experiences by leveraging strategy, analytics, machine learning, and award-winning creative that ignite a perpetual dialogue between brands and their customers. 
  

Responsibilities:

* Used SAS Proc SQLpass-throughfacility to connect to Oracle tables and created SAS datasets using various SQL joins such as left join, right join, inner join and full join.
* Performing data validation, transforming data from RDBMS oracle to SAS datasets.
* Produce quality customized reports by using PROC TABULATE, PROC REPORT Styles, and ODS RTF and provide descriptive statistics using PROC MEANS, PROC FREQ, and PROC UNIVARIATE.
* Developed SAS macros for data cleaning, reporting and to support routing processing.
* Performed advanced querying using SAS Enterprise Guide, calculating computed columns, using afilter, manipulate and prepare data for Reporting, Graphing, and Summarization, statistical analysis, finally generating SAS datasets.
* Involved in Developing, Debugging, and validating the project-specific SAS programs to generate derived SAS datasets,  tables, and data listings according to study documents.
* Created datasets as per the approved specification collaborated with project teams to complete scientific reports and review reports to ensure accuracy and clarity.
* Experienced in ing with data modelers to translate business rules/requirements into conceptual/logical dimensional models and ed with complex de-normalized and normalized data models
* Performed different calculations like Quick table calculations, Date Calculations, Aggregate Calculations, String and Number Calculations.
* Created action filters, user filters, parameters and calculated sets for preparing dashboards and sheets in Tableau.
* Used the dynamic SQL to perform some pre-and post-session task required while performing Extraction, transformation, and loading.
* Designing the ETL process using Informatica to populate the Data Mart using the flat files to Oracle database
* Expertise in Agile Scrum Methodology to implement project life cycles of reports design and development
* Combined Tableau visualizations into Interactive Dashboards using filter actions, highlight actions etc. and published them on the web.
* Gathering business requirements, creating business requirement documents (BRD /FRD).
*  closely with business leaders and users to define and design the data sources requirements and data accessCode, test, identify, implement and document  solutions utilizing JavaScript, PHP&MySQL.
* ing with themanager to prioritize requirements and preparing reports on theweekly and monthly basis. 

Environment: SQL Server, Oracle 11g/10g, MS Office Suite, PowerPivot, Power Point, SAS Base, SAS Enterprise Guide, SAS/MACRO, SAS/SQL, SAS/ODS, SQL, PL/SQL, Visio.

Client:  Karvy Global Services - IN                                					Jan 2011 - Nov 2012
Role: Data Analyst.	

Description: : Karvy Global Services is a specialist knowledge process outsourcing services firm and an arm of the Karvy Group, one of India's largest integrated financial services companies
Responsibilities:
* Participated in requirement gathering sessions with business stakeholders to understand the project goals and documented the business requirement documents(BRD)
* Studied the Requirements Specifications, Use Cases and analyzed the data needs of the Business users.
* Implemented conceptual and logical data models using Erwin 7.2 by adopting agile methodologies as per organization standards.
* Redesigned some of the previous models by adding some new entities and attributes as per the business requirements.
* Converted the Logical data models to Physical data models to generate DDL scripts.
* Reverse Engineered existed data models for analyzing and comparing the business process.
* Expertise in the Forward Engineering of logical models to generate the physical model using Erwin.
* Created the Logical data models using Erwin 7.2 and ensured that it follows the normalization rules and have the best possible traceability pattern.
* Migrated several models from Erwin 4.1/7.1 to ERWIN 7.2 and updated the previous naming standards.
* Extensively ed with enterprise data warehouse development by building data marts, staging, and restaging.
* Scheduled reports for daily, weekly, monthly reports for executives, Business analyst and customer representatives for various categories and regions based on business needs using SQL Server Reporting Services (SSRS)
* ed with business users to understand metric definitions, presentation, and user needs.


Environment: Erwin, Informatica, Cognos, Oracle 9i, SQL Server 2003, SQL, MS Office, Windows 2003.








",Data Scientist,emailed,indeed job post,resume
"US Citizen 
-------------------------------------------------------------------------------------------------------------------------------

Energetic, harding and dedicated computer science graduate seeking a full-time position as an entry level Web Developer.
:
Princess Sumaya University of 
Amman- Jordan
Bachelor of Science in Information , June 2016

	
* Good Knowledge of Java and Python
* Knowledge of Java Script, J Query, PHP, HTML, CSS
* Good experience in using Black Box test methodologies and Functional Testing
* Excellent communication 
* Team player with multi-tasking  
* Very good leadership 
* Fast learner and critical thinker
* Strong self-motivation; proactive and willing to take on new challenges



INTERNSHIP:
   Microsoft Innovation Center				Amman - Jordan
   Computer Program Intern				June 2015  Sep. 2015	

* Developed a Windows Phone application using C# and XAML.
* Created Database on Azure (Microsoft's public cloud computing platform) and connected it to my Windows Phone application.
* Trained to  within a team and to think creatively and out of the box.
* My application was selected and presented to potential investor out of more than 50 applications sponsored by MS Innovation Center and USAID.


EXPERINECE:
English Talent School					Amman-Jordan
First Grade Teacher						Aug 2017  July 2019

* Teaching a full range of subject areas (English, math, integrated studies etc.)
* Collaborate with staff members in developing school activities and joint lessons.
* Commended for ability to redirect students exhibiting behavior problems. 
* Preparing daily and long-term lesson plans according to the curriculum guidelines.
* Setting and conducting testing.
* Established open-door policy, improving communication and trust with students and parent
* Served on school committees and taskforces focused on curriculum development and textbook review.



",Data Scientist,emailed,indeed job post,resume
"
 
* 6+ years of experience solving complex business problems across multiple industry verticals by combining business strategy and tactical implementation.
* Hands-on experience and comprehensive industry knowledge of Machine Learning, Statistical Modelling, Predictive Modelling, Data Analytics, Data Mining, Text Mining and Natural Language Processing (NLP).
* ed with severalPythonpackages like Pandas, NumPy, Scikit-learn, Keras etc. 
* ed with several R packages like ggplot2, dplyr, plyr, data. tables etc.
* Strong statistical  such as Hypothesis Testing, Principle Component Analysis (PCA) etc.
* Expertise in using several Machine Learning Models including Linear Regression, Logistic Regression, Regularization, k Nearest Neighbor, Decision Trees, Random Forests, Boosting, Neural Nets.
* Experience in Natural Language Processing (NLP) using Python libraries such as NLTK and spacy 
* Experience with Deep Learning Models (ANN, CNN, RNN)
* Good experience in writing SQL queries and implementing functions, tables, views etc.
* Strong knowledge in Statistical methodologies such as Hypothesis Testing, Principal Component Analysis (PCA), Sampling Distributions, ANOVA, Chi-Square tests, Time Series, Factor Analysis, Discriminant Analysis
* Experienced in utilizing analytical applications like R and Python to identify trends and relationships between different pieces of data, draw appropriate conclusions and translate analytical findings into decision making and marketing strategies that drive value. 
* Expertise in applying data mining techniques and optimization techniques in B2B and B2C industries and proficient in Machine Learning, Data/Text Mining, Statistical Analysis, Sentiment Analysis and Predictive Modeling. 


 
Programming 
Python, R, SQL
Databases
MySQL
Statistical Software
SAS, R, Python
ETL/BI  
Tableau, MS Excel, SQL 
Statistical Methods
Hypothesis Testing, Principal Component Analysis (PCA), Sampling Distributions, ANOVA, Chi-Square tests, Time Series, Factor Analysis, Discriminant Analysis
Big Data
Hadoop, Hive, Sqoop, Spark(Scala)
Machine Learning Models
Linear Regression, Logistic Regression, Regularization, Support Vector Machines, Neural Nets, Decision Trees, Ensemble Methods like Random Forests, Gradient Boost etc., Deep Learning





SunRay Enterprise
Town Sports International, Princeton, NJ
Data Scientist                                                                                                                                          July 2018 - Present                                     

Responsibilities: 
* Developed customer re-engagement strategies and win back strategies
* Executed predictive analysis using Python on 100,000 data points to identify top customers more likely to churn next month 
* ed with severalPythonpackages like Pandas, NumPy, Scikit-learn etc.
* Various Classification where used and tested. Gradient Boosting was finalized based the feasibility and accuracy of results.
* Achieved 92% accuracy in prediction of customer churn using Gradient Boosting Algorithm
* Responsible for data aggregation, data pre-processing, missing value imputation and descriptive and inferential analysis
* Data elements validation using exploratory data analysis (univariate, bi-variate, multi-variate analysis).
* Missing value treatment, outlier capping and anomalies treatment using statistical methods, deriving customized key metrics.
* Dummy variables where created for certain datasets to into the regression. 
* Had to use SMOTE sampling technique to balance the dataset by over-sampling the minority label class
* ed closely with subject-matter experts and business analysts and investigating statistical and predictive and prescriptive patterns in the data to build business solutions.
* Used Correlation analysis to identify relation between variables, patterns, outliers and causal factors.
* Provided actionable insights for Fitness Center Management to define new business strategies geared towards improving their programs, creating effective marketing campaigns and offering personalized rewards to members
* Utilized statistical techniques to understand the data, perform descriptive statistics (mean, median, mode, density distributions, box plots etc.), inferential statistics (t-test, ANOVA, Chi square etc.) and hypothesis testing.
* Created, analyzed and presented various performance parameters to quickly spot customer behavioral aspects and preferences using Tableau.
* Data Visualization extensively performed using TABLEAU 8.3. 
* Improved visitor experience to ultimately drive higher volumes of customer leads using Google Analytics

Environment: MS Excel, Tableau, Google Analytics, Python's Pandas, NumPy, Sklearn, Seaborn, Machine Learning 







AmerisourceBergen, Dallas, Texas                                            
Data Analyst                                                                                                                                      Jan 2017  June 2018

Responsibilities: 
* Developed financial data analysis leveraging MS Excel and Apptio TBM software to demonstrate the financial impacts of business decisions to over 500 internal clients
* Involved in the complete life cycle of the project performing various tasks like Data Understanding by performing Exploratory Data Analysis, Data Cleansing, Modeling, Evaluating and Deploying. 
* Performed Data Manipulation and Aggregation as required on data collected from Apptio. 
* Identified patterns, data quality issues through Exploratory Data Analysis. 
* Optimized IT service costing platform by introducing new services to be billed, improving transparency to stakeholders
* Collaborated with IT Executives, leadership and other stakeholders on IT Total Cost of Ownership, financial metrics, and cost transparency initiatives
* Provided support to the stakeholders through  expertise on data analytics, driving process and data improvement
* Enhanced organizations expenditure forecasting ability including assessment of trends, identification of variance drivers and feasible action plans

Environment: MS Excel, Apptio, Tableau

Amazon Web Services, New Delhi, India                                                       
Data Analyst                                                                                                                                         Nov 2015  Aug 2016 

Responsibilities: 
* Produced digestible business intelligence and actionable information leading to revenue acceleration
* Captured and surfaced the best data and information to make optimal decisions, driving a rapid expansion of its world-wide sales team
* Spearheaded resource planning and analytical support to the sales team leading to better customer service and cloud product sales
* Defined, built, and scaled metrics and analytical insights to measure the success and drive the day-to-day behavior for the AWS Business Development Team 
* Provided ad-hoc analysis and reports to Executive level management team using Excel and Tableau

Environment: MS Excel, Tableau

AnalytixLabs, Gurgaon, India                                                                                
Data Analyst                                                                                                                                           Apr 2015  Oct 2015                                                                                                                           

Responsibilities:
* Architected strategy for expansion of an ice-cream company by tapping into the customer behavior and characteristics
* Gathering, leaning present and historical data in preparation for data mining;
* Interpreted complex simulation data using statistical methods as per requirements.
* Architected and implemented analytics and visualization components for data analysis.
* Performed predictive modelling to understand thel behavior and preferences of potential customers.
* ed on customer segmentation using an unsupervised learning technique - clustering. 
* Performed K-means clustering on behavioral data to segment and identify the most profitable customers
* Built logistic regression model with 89% accuracy to detect key demographic variables that help discriminate between more profitable and less profitable customers

Environment: Python, SQL, Oracle 12c, R, Tableau, Cluster analysis


Cognizant, Chennai, India
Programmer Analyst - Big Data Division                                                                                          Jun 2014  Mar 2015

Responsibilities: 
* Performed data analysis and data profiling using complex SQL on various sources systems. 
* Created SQL scripts to find data quality issues and to identify keys, data anomalies, and data validation issues.
* Involved in defining the source to target data mappings, business rules, and data definitions.
* Involved in identifying the source data from different systems and map the data into the warehouse.
* Created HBase tables to store variable data formats of input data coming from different portfolios.
* Managed excel spreadsheets, resolved discrepancies associated withmetadata.
* Strong experience in importing themetadatafrom various applications and build end-to-end Data Lineage.
* Importing and exporting data into HDFS and Hive from Teradata using Sqoop.
* Involved in creating Hive tables, loading with data and writing hive queries, which will run internally in MapReduce
* Data preparation including data sanitization, imputing missing values, dealing with outliers/anomalous data.
* Perform Text Analytics on the social media websites like Twitter to capture customer sentiment and improve customer satisfaction index by 7%  
* Gathered business requirements and prepared Software Requirement Specification (SRS) document. Created Visio charts for the flow architecture of the system. 
* Collaborated with one team member in design, analysis, coding.
* Used Team Studio and Build Manager  to develop applications and promote the new design to test environment. 
* Coordinated with the business users on the User Acceptance Tests (UAT) and to get the approval from a business on the design changes.

Environment: Python, R, SQL, BigData Technologies (Sqoop, Spark, Hive), Text Analysis 
				
Paytm, Noida, India
Data Analyst                                                                                                                                 Jan 2013   May 2014
  
Responsibilities: 
* Extracted relevant information from large databases using SQL queries to detect fraud patterns
* Conducted data analysis and identified fraud patterns related to chargebacks
* Identified, evaluated, and documented potential data sources in support of project requirements.
* Extracted, transformed and loaded present and historical data in preparation for data mining.
* Prototyped predictive models of user fraud activity using machine learning;
* Analyzed data to throw insights on effectiveness of campaigns running on Paytm marketplace
* Built content-based recommender from scratch to help users to choose products based on their previous selections.
* Recommendation system, built using Python, provided similar products based on keywords and bio

Environment: SQL, Google Analytics, Python, Machine Learning Models. 

Additional Projects:Web Analytics using Google Analytics 	 
* Designed, built and tuned google analytics reporting to understand user behavior and enhanced e-commerce metrics, increasing google merchandise stores revenue by 15% 
Tableau Data visualization for Cinemark 
* Optimized marketing strategy for Cinemark by visualizing consumer behavior based on the impact of video demand services on movie watching industry
American Heart Association (AHA), Dallas		 
* Recognized characteristics of top donors and potential markets to well align resources and generate more revenue for AHA
Machine Learning: Churn Analysis                                                      		 
* Implemented Logistic regression and Support Vector Machine algorithms to predict parameters influencing customer churn
Machine Learning: Breast Cancer Detection                     		 
* Devised Artificial Neural Net model and decision tree algorithms with 83% accuracy to detect breast cancer
Big Data Analytics		 
* Launched natural language processing (NLP) project to capture trending hashtags and conducted sentimental analysis on twitter feed related to United States presidential election using R
* Analyzed craft brewery data to determine most brewed beer styles and their properties via Spark and Sqoop
Marketing Predictive Analytics using SAS					 
* Evaluated yogurt retail market to find profitable customer segments using hierarchical clustering based on RFM scores
* Deployed logit model to understand the effect of various factors on the utility of our chosen yogurt brand



Education
* Masters, Business Analytics, University of Texas at Dallas, 2018
* Bachelor of Technology, Computer Science Engineering, VIT University-India-2014.









",Data Scientist,emailed,indeed job post,resume
" Extensive programming  in analytical and statistical programming language R, python and SQL.
 
 Experience in querying and extracting the data from various RDBMS databases and NoSQL Databaseand creating data sets for Analysis. 
 Extensive programming  in analytical and statistical programming l anguage R, python and SQL.
 
 Experience in querying and extracting the data from various RDBMS databases and NoSQL Databaseand creating data sets for Analysis. 
 Experience in writing complex SQL queries to retrieve and prepare the data for analysis. 
 Hands-on experience with discovering, analyzing, and identifying key elements in the data therebycommunicating the insights, trends and future forecasts that impact consumer behavior.  Experience ing on data preprocessing steps like exploration, aggregation, missing data imputation, sampling, feature selection, dimensionality reduction, outlier detection in both R and Python. 
 Well versed in machine learning algorithms such as Linear, Logistic and other general linear models,Decision Trees, Random Forest, Support Vector Machines, K nearest neighbors and regularization methods. 
 Experience in designing and developing several predictive models for various financial and non -financial institutions in both R and Python. 
 Significant Experience and knowledge in using python's machine learning toolbox Scikit-Learn and
Natural Language processing toolbox NLTK. 
 Experience in ing with visualization  ggplot2, Matplotlib, Seaborn, Plotly and Tableau. 
 Significant knowledge and experience in Hadoop, MapR, Hive, Impala, Spark Python API (PySpark), 
 SparkSQL and Spark's machine learning library MLLib. 
 A self-motivated and inquisitive individual with strong  ethics who thrives ing independently& in teams.
 Experience

Data Science Analyst
Bowling Green State University
August 2018 to Present
 Developed pricing model to optimize various membership charges in wellness center, resulted in
20% growth in customer base. 
 Analyzed requirements, generated reports and identified business opportunities using Tableaudashboard. 
 Created survey templates across all membership groups to capture customer experience.  Enhanced data collection procedures to include relevant information to build and continuously optimize the analytic systems. 
 Collaborated with I.T. to continuously optimize business performance. 
 Processed, cleansed and verified the integrity of data from various sources used for analysis andreporting. 
 Used predictive modeling to increase and optimize customer experiences, revenue generation, adtargeting and other business outcomes. 
 Implemented various statistical techniques to manipulate the data like missing data imputation,principle component analysis, sampling and t-SNE for visualizing high dimensional data. 
 ed on Text Analytics, developing different Statistical Machine Learning, Data Mining solutions tovarious business problems and generating Data Visualizations using R and Python. 
 Used statistical procedures and Machine Learning algorithms such as ANOVA, Clustering, Regressionand Time Series Analysis to analyze data for further Model Building. 
 Leveraged the latest data visualization  and techniques to present and communicate analysis tothe leadership team utilizing data management, analytics modeling, and business analysis. 
 Advised leadership team and stakeholders with data-driven solutions and recommended strategiesthat address business challenges. 
Environment: 
Python (NumPy, Pandas, PySpark, Scikit-learn, Matplotlib, NLTK), TSQL, MS SQL Server, XML, R Studio, Spyder, MATLAB, ETL, Machine Learning, Shiny, AWS, Redshift, Java, Tableau.
Data Scientist
Nissan North America - Franklin, TN
November 2017 to July 2018
 Performed sentimental analysis on tweets made on twitter using text mining in R Studio.  Converted unstructured pure text consumer comments data to structured dataset using NLP techniques and feature engineering. 
 Used common NLP techniques, such as pre-processing (tokenization, part-of-speech tagging, parsing,stemming) 
 Built predictive models including support Vector Machine, Decision tree, Naive Bayes Classifier,Neural Net plus ensemble methods of the models to evaluate how the likelihood to recommend of customer groups would change in different set of service by using python scikit-learn. 
 Created insightful dashboards, identified market opportunities across the United States usingTableau. 
 Provided insights for building advertising strategy, leading to 8% increase in sales. 
 Designed and developed Ad-hoc reports as per business requirements. 
 ed on Data Verifications and Validations to evaluate the data generated according to therequirements is appropriate and consistent. 
 Used libraries like BeautifulSoup, pandas, and NumPy in python to scrape the important data fromwebsites. 
 Maintained project plan and weekly status documents to keep track of project activities & timelines. 
 
Environment: 
Python, R, SQL, Pyspark, SparkR, SparkSQL, Hadoop, HDFS, Databricks, DBFS, NLP, Tableau, Web Scrapping.
Data Analyst
Accenture Solutions Pvt Ltd
February 2016 to July 2017
 Played a vital role in developing retail store Loyalty and Sales strategy, increased the sales by 15%. 
 Written SQL queries to fetch complex data from different tables in databases. 
 ed with various customer analytics such as Customer targeting, campaign sales analysis, KPIanalysis, forecasting sales, NLP models. 
 ed on Personalized marketing models to implement simplicity and targeted marketing forspecific customers. 
 ed with Clustering algorithms to target specific group of customers to generate profitablerevenue. 
 Used market basket analysis, association rules analysis to identified patterns, data quality issues andleveraged insights. 
 Designed and analyzed test campaign and recommended future incentives. 
 Derived core insights from the data, developed a hypothesis for A/B testing and performedmultivariate tests for the campaign optimization. 
 Wrote automation processes using Python and the AWS Lambda service 
 Created insightful dashboard on performance of personalization campaigns using Excel & Tableau.  ed extensively in documenting the Source to Target Mapping documents with data transformation logic. 
 Designed KPI dashboards in Google Analytics to measure the effectiveness of ad campaigns. Environment: 
AWS, Lambda, Redshift, Python, TSQL, MS SQL Server, XML, R Studio, Spyder, Jupyter, Docker, Machine Learning, Shiny, Java, Tableau, NLP, Google Analytics
Statistical Analyst
Aakash Engineers Inc
August 2014 to January 2016
 Responsible for gathering business requirements, collect data and perform data preprocessing.  Analyzed customer trends using time series decomposition method and forecast the demand using trend fit analysis. 
 Prepared comprehensive presentations and provided insights to leadership. 
 Communicated effectively in both a verbal and written manner to client team. 
 Completed documentation on all assigned systems and databases, including business rules, logic,and processes. 
 Created Test data and Test Cases documentation for regression and performance. 
 Designed, built, and implemented relational databases. 
 Determined changes in physical database by studying project requirements. 
 Developed intermediate business knowledge of the functional area and processed to understand theapplication of data information to support business function. 
 Facilitated gathering moderately complex business requirements by defining the business problem. 
 Utilized SPSS statistical software to track and analyze data. 
 Optimized data collection procedures and generated reports on a weekly, monthly, and quarterlybasis. 
 Used advanced Microsoft Excel to create pivot tables, used VLOOKUP and other Excel functions. 
 Successfully interpreted data to draw conclusions for managerial action and strategy. 
 Created Data chart presentations and coded variables from original data, conducted statisticalanalysis as and when required and provided summaries of analysis. 
 Maintained the data integrity during extraction, manipulation, processing, analysis and storage. Environment: 
Oracle, Erwin, Informatica, Data Warehousing, SQL, Tableau, MS Excel, ETL 
 
PROJECT PROFILE 
Analysis of Amazon Reviews (Electronic Category) - Big Data Concept: Spark 
 Built product recommendation engine using ALS model in PySpark, predicted average ratings of aproduct using K- Nearest neighbors, segmented items using K-means clustering in Python.  Used Pandas, NumPy, SciPy, Matplotlib, Seaborn, Scikit-learn in Python at various stages for developing machine learning model. 
 Performed preprocessing and exploratory data analysis using PySpark. 
 
Housing Sale Price Prediction - Regression Analysis 
 Built multiple regression model to predict sales price of residential properties in Ames, Iowa.  Used statistical techniques such as transformation of variables, check for regression assumptions, variable selection based on AIC, step wise regression, correlation and verified assumptions using constant variance and normality test. 
 
Recommendation Systems - Virtual sales man 
 Built a recommendation system that suggests artists to users according to their musical taste basedon user-based and artist-based filtering using Python. 
 Performed data cleaning, necessary variable transformation, used collaborative filtering technique tofind similarities. 
 Used SFrames functionality from Graphlab package to handle large datasets. 
 
User Interface and Database Design - Using SQL Server 
 Collected data and designed a best secure database for an University facing problem in storing thealumni data. 
 Drawn data models and ERD's and used them to create databases and schemes both in SQL Serverand MY SQL. 
 
 Filtered the data, normalized the relations, assigned specific data types, domains, keys, andconstraints to all fields.
Education

Master of Science in Applied Statistics
Bowling Green State University - Bowling Green, OH
 / IT 

PYTHON (4 years), SQL (4 years), CLUSTERING (4 years), MACHINE LEARNING (4 years), DATABASES (2 years)
Online Profile

https://www.linkedin.com/in/samitha-gillala
Additional Information
 
 
Languages R, Python, SQL, SAS 
AWS EC2, S3, RedShift, Glue 
Big Data Hadoop, Map Reduce, Hive, Impala, Spark (Pyspark, SparkSQL, MLLib) 
Analysis Supervised and Unsupervised Techniques, Time Series Analysis 
Databases Oracle, MySQL, MongoDB, Teradata 
 R Studio, Jupyter, Docker 
Visualization  Tableau, ggplot2 (R), matplotlib (Python), Seaborn, Plotly 
 
MACHINE LEARNING 
 
Classification Logistic Regression, LDA, KNN, SVM, Nave Bayes, Decision Tree, Random forests, Neural
Nets 
Regression 
Multiple Regression, Ridge Regression, Lasso 
Regression, Regression Trees 
 
Unsupervised learning PCA, k-Means clustering, Hierarchical clustering 
Recommendation Engines Market Basket Analysis, Collaborative filtering, Content based filtering 
Boosting ADA Boost, XGBoost 
Time Series Moving Average, ARIMA 
Text Analytics Text Pre-Processing, Classification, Topic Modeling, Clustering, Sentiment Analysis, Word cloud 
Python Libraries Numpy, Pandas, Scikit-Learn, NLTK, PyMongo",Data Scientist,emailed,indeed job post,resume
" A Passionate, team-oriented Data Scientist with experience in Data Extraction, Data Modelling,
Statistical Modeling, Data Mining, Machine Learning and Data Visualization. 
 Expertise in transforming business resources and tasks into regularized data and analytical models,designing algorithms, developing data mining and reporting solutions across a massive volume of structured and unstructured data. 
 Involved in entire data science project life cycle, including Data Acquisition, Data Cleansing, Data
Manipulation, Feature Engineering, Modelling, Evaluation, Optimization, Testing and Deployment. 
 Experienced in building various machine learning predictive models using algorithms such as Linear
Regression, Logistic Regression, Nave Bayes Classifier, Support Vector Machines (SVM), Neural
Nets, KNN, K-means Clustering, Decision Trees, Ensemble methods (Random Forest, AdaBoost,
Gradient Boosting, and Bagging). 
 Proficient with Python 3.x including NumPy, Scikit-learn, NLP, Pandas, Matplotlib and Seaborn.  Extensive experience in RDBMS such as SQL server 2012, Oracle 9i/10g and non-relational database such as MongoDB 3.x. 
 Hand on experience on Hadoop 2.x ecosystem and Apache Spark 2.x frame such as Hive, Pig,and PySpark. 
 Proficient at data visualization  such as Tableau, R ggplot, Python Matplotlib and Seaborn. 
 Experienced designing and developing T-SQL queries, ETL packages and business reports using SQL
Server Management Studio (SSMS) and BI Suite (SSIS/SSRS). 
 Adept in developing and debugging Stored Procedures, User-defined Functions (UDFs), Triggers,
Indexes, Constraints, Transactions and Queries using Transact-SQL (T-SQL). 
 Knowledge and experience ing in Waterfall as well as Agile environments including the Scrumprocess and using Project Management  like ProjectLibre, Jira/Confluence and version control  such as Github. 
 Self-motivated, Fast Learner, good team lead and player, strong managing and communication 
Willing to relocate: Anywhere
Sponsorship required to  in the US
 Experience

Data Scientist
AmeriHealth Caritas, PA
September 2018 to Present
AmeriHealth has been providing managed care, primarily HMO, products to more than 265,000 members in Delaware, New Jersey, and Pennsylvania. As a subsidiary of Independence Blue Cross, the managed care provider operates as two entities, AmeriHealth HMO and AmeriHealth Insurance, though it prefers one unified name, AmeriHealth. 
 
Responsibilities: 
 Communicated and coordinated with end client for collecting data and performed ETL to define theuniform standard format. 
 Queried and retrieved data from SQL Server database to get the sample dataset. 
 In preprocessing phase, used Pandas to clean all the missing data, datatype casting and merging orgrouping tables for EDA process. 
 Used PCA and other feature engineering, feature normalization and label encoding Scikit-learnpreprocessing techniques to reduce the high dimensional data (>150 features). 
 In data exploration stage used correlation analysis and graphical techniques in Matplotlib and
Seaborn to get some insights about the patient admission and discharge data. 
 Experimented with predictive models including Logistic Regression, Support Vector Machine (SVC),Random Forest provided by Scikit-learn, XGBoost, LightGBM and Neural net by Keras to predict showing probability and visiting counts. 
 Designed and implemented Cross-validation and statistical tests including k-fold, stratified k-fold,hold-out scheme to test and verify the models' significance. 
 Implemented, tuned and tested the model on AWS Lambda with the best performing algorithm andparameters. 
 Implemented Hypothesis testing kit for sparse sample data by wring R packages. 
 Collected the feedback after deployment, retrained the model to improve the performance. 
 Communicated and coordinated with end client for collecting data and performed ETL to define theuniform standard format. 
 Queried and retrieved data from SQL Server database to get the sample dataset. 
 In preprocessing phase, used Pandas to clean all the missing data, datatype casting and merging orgrouping tables for EDA process. 
 Used PCA and other feature engineering, feature normalization and label encoding Scikit-learnpreprocessing techniques to reduce the high dimensional data (>150 features). 
 In data exploration stage used correlation analysis and graphical techniques in Matplotlib and
Seaborn to get some insights about the patient admission and discharge data. 
 Experimented with predictive models including Logistic Regression, Support Vector Machine (SVC),Random Forest provided by Scikit-learn, XGBoost, LightGBM and Neural net by Keras to predict showing probability and visiting counts. 
 Designed and implemented Cross-validation and statistical tests including k-fold, stratified k-fold,hold-out scheme to test and verify the models' significance. 
 Implemented, tuned and tested the model on AWS Lambda with the best performing algorithm andparameters. 
 Implemented Hypothesis testing kit for sparse sample data by wring R packages. 
 Collected the feedback after deployment, retrained the model to improve the performance. 
 Designed, developed and maintained daily and monthly , trending and benchmark reportsin Tableau Desktop. 
 
Environment: 
R 3.x, gmodels, Python3.x, Scikit-Learn, Web Crawling, ETL, Root Cause Analysis, Factor Analysis, PCA,
KNN, Statistical Tests, Ensemble Model, Regularized Linear Models, Lasso Model, Random Forests, Attribution Models, Tableau 9.x
Data Quality Analyst
City of New York-Fire Department - New York, NY
June 2018 to September 2018
The Fire Department of the City of New York (FDNY) is the largest Fire Department in the United States and universally is recognized as the world's busiest and most highly skilled emergency response agency. Management Analysis and Planning (MAP) department is responsible for providing ongoing analytical support to critical Department initiatives through quantitative modeling, reporting and forecasting. 
 
Responsibilities: 
 Collaborated in a team of four to data mine FDNY records and determine the changes in racialdiversity of firefighter recruitment over the course of fourteen years. 
 Extracted, processed, and analyzed EMS travel data throughout the course of the day in New York
City to reduce response time. 
 Recommended demand forecasting models directly to the Deputy Commissioner of Management of
Analysis and Planning for EMS deployment to improve accuracy and reduce response time.  Created action filters, parameters, and calculated fields to prepare interactive dashboards and sheets in Tableau and identify trends for other colleagues. 
 Implemented advanced geographic mapping techniques in R to visualize the location of inactive firealarm boxes in all five boroughs of New York City. 
 Proposed a cost cutting solution for the diversity recruitment project by delivering a presentation tothe department supervisors. 
 
Environment: 
SQL Server 2012/2014, AWS EC2, AWS Lambda, AWS S3, AWS EMR, Linux, Python3.x (Scikit-Learn, NumPy, Pandas, Matplotlib), R, Machine Learning algorithms, Tableau.
Data Scientist
Hexistech Inc - North Brunswick, NJ October 2017 to May 2018
Hexistech was founded by s from Bell Labs, with a vision of providing quality and costeffective IT services. Which ed relentlessly to build a mature and thriving IT services firm, helping clients develop and deploy meaningful IT solutions enhancing their strategic goals. 
 
Responsibilities: 
 Developed and applied methods to identify, collect, process, and analyze large volumes of data tobuild and enhance products, processes, and systems. 
 Conducted data mining and retrieval, and apply statistical and mathematical analyses to identifytrends, solve analytical problems, optimize performance, and gather intelligence. 
 Visualized information using a range of  (e.g., GIS , RStudio), develop scripts andalgorithms, create explanatory analysis and predictive models, and perform comparative analyses to address complex problems. 
 Responsible for reporting of findings that will use gathered metrics to infer and draw logicalconclusions from past and future behavior. 
 Performed Multinomial Logistic Regression, Random Forest, Decision Tree, SVM to classify package isgoing to deliver on time for the new route. 
 Used Principal Component Analysis & Factor Analysis in feature engineering to analyze highdimensional data in MATLAB. 
 Performed data analysis by using SQL to retrieve the data from Hadoop cluster. 
 Used R machine learning library to build and evaluate different models. 
 
Environment: 
SQL Server 2012/2014, AWS EC2, AWS Lambda, AWS S3, AWS EMR, Linux, Python3.x (Scikit-Learn, NumPy, Pandas, Matplotlib), R, Machine Learning algorithms, Tableau.
Data Analyst (Python/R)
I-Next Technology
January 2015 to July 2017
I-Next Technology is a market research and analytics start-up company. The company offers market research, but there is also market analysis, competitive intelligence, product intelligence and, most importantly, the expertise to combine these elements in an expert synthesis that generates insight and recommendation rooted in cast-iron fact. 
 
Responsibilities: 
 Participated in requirement gathering, analyzed business needs for modeling and analytics andprovided roadmaps for statistical analysis. 
 Performed data visualization, data cleaning, feature engineering (categorical feature encoding,feature conjunction, normalization), model selection (Deep neural nets, gradient boosted regressor and random forests), and model ensemble (stacking). 
 Implemented ETL process and Data Cleaning for both the internal and external data sources through
Python Pandas and NumPy. 
 Identified and selected the effective features by using Principal Components Analysis and KNN byusing Python SciPy. 
 Built predictive models including Regularized Linear Models, Lasso Model, Random Forests to predictfuture claim severity by using Python Scikit-Learn. 
 Developed Ensemble Model using R gmodels to combine multiple predictive models and theirpredictions for improving the prediction accuracy. 
 Designed and implemented cross-validation and statistical tests including Hypothetical Testing,
ANOVA, Autocorrelation to verify each predictive model. 
 Evaluated and recommended the optimized time frequency and time duration for email advertisingcampaigns. 
 Used Tableau 9.x, R to create detail level  reports and dashboards to technical and businessstakeholders, by using KPI's and visualized trend analysis. 
 
Environment: 
R 3.x, gmodels, Python3.x, Scikit-Learn, Web Crawling, ETL, Root Cause Analysis, Factor Analysis, PCA,
KNN, Statistical Tests, Ensemble Model, Regularized Linear Models, Lasso Model, Random Forests, Attribution Models, Tableau 9.x
Education

Master's in Information Systems in Information Systems
Pace University - New York, NY


Anova, Boosting, Decision trees, K-means, Lda, Logistic regression, Machine learning, Nave bayes,
Neural nets, Principal component analysis, Pca, Random forest, Support vector machine, Svm, C+ +, Git, Hadoop, Hbase, Hdfs, Hive, Business Intelligence, Excel, access, testing, SQL
Additional Information

TECHNICAL  
Databases MS SQL Server 2008/2008R2/2012/2014/2016, MongoDB 3.x, MySQL 5.x, Oracle, HBase,
Amazon Redshift, Teradata 
Statistical Methods 
Hypothetical Testing, ANOVA, Time Series, Confidence Intervals, Bayes Law, Principal Component Analysis (PCA), Chi-square test, Chebyshev's inequality 
 
Machine Learning 
Linear Regressions, Logistic Regression, Nave Bayes, Decision Trees, Random Forest, Support Vector
Machine (SVM), Neural Nets, Sentiment Analysis, K-Means Clustering, K-nearest Neighbors (KNN), Ensemble Methods, Gradient Boosting Trees, Ada Boosting, PCA, LDA 
 
Hadoop Ecosystem Hadoop 2.x, Spark 2.x, MapReduce, Hive QL, HDFS, Sqoop, Pig Latin 
BI Reporting  Tableau 10.x / 9.x, MS SQL Server Integration Service and Reporting Service (SSIS/ SSRS), Power BI 
Data Visualization Tableau, Python (Matplotlib, Seaborn), R(ggplot2), Looker, Power BI, QlikView Languages 
Python 2.x/3.x (NumPy, Pandas, Scikit-learn, Matplotlib, Seaborn), R (dplyr, ggplot2, rpart, caret,
Random Forest, gbm, neuralnet), SQL (T-SQL, MySQL), C++, MATLAB, Octave 
 
Operating Systems UNIX/UNIX Shell Scripting (via PuTTY client), Linux and Windows XP/7/8/10, Mac OS
 
Other  and technologies 
Azure ML Studio, Google TensorFlow, Apache Tomcat Webserver, MS Office Suite, Lucid Chart, Stat
, ProjectLibre, Google Analytics, Google Tag Manager, Salesforce, MS SharePoint, Trello, JIRA,
Confluence, GitHub/Git, AWS - (EC2/S3/Redshift/EMR/Lambda)",Data Scientist,emailed,indeed job post,resume
"  qualified Data Scientist/Data Analyst with around 8+ years of experience in Data
Science and Analytics including Data Mining, Deep Learning/Machine Learning and Statistical Analysis  Involved in the entire data science project life cycle and actively involved in all the phases including data cleaning, data extractionanddata visualization with large data sets of structured and unstructured data, created ER diagrams and schema. 
 Experienced with machine learning algorithm such as logistic regression, KNN, SVM, random forest,neural net, linear regression, lasso regression and k - means 
 Implemented Bagging and Boosting to enhance the model performance. 
 Experience in implementing data analysis with various analytic , such as Anaconda 4.0 Jupiter
Notebook 4.X, R 3.0 (ggplot2,, dplyr, Caret) and Excel 
 Solid ability to write and optimize diverse SQL queries, ing knowledge of RDBMS like SQL Server
2008/2010/2012 , NoSql databases like MongoDB 3.2 
 Excellent understanding Agile and Scrum development methodology 
 Experienced the full software life cycle in SDLC, Agile, DevOps and Scrum methodologies includingcreating requirements, test plans. 
 Skilled in Advanced Regression Modeling, Correlation, Multivariate Analysis, Model Building, Business
Intelligence  and application of Statistical Concepts. 
 Developed predictive models using Decision Tree, Naive Bayes, Logistic Regression, Random Forest,
Social Net Analysis, Cluster Analysis, and Neural Nets. 
 Experienced in Machine Learning and Statistical Analysis with Python Scikit-Learn. 
 Experienced in Python to manipulate data for data loading and extraction and ed with pythonlibraries like Matplotlib, Scipy, Numpy and Pandas for data analysis. 
 ed with complex applications such as R,R Shiny, SAS, Plotly, ArcGIS, Matlaband SPSS to developneural net, cluster analysis. 
 Strong SQL programming , with experience in ing with functions, packages and triggers.  Expertise in transforming business requirements into designing algorithms, analytical models, building models, developing data mining and reporting solutions that scales across massive volume of structured and unstructured data. 
 Skilled in performing data parsing, data manipulation, data architecture, data ingestion and datapreparation with methods including describe data contents, compute descriptive statistics of data, regex, split and combine, merge, Remap, subset, reindex, melt and reshape. 
 Excellent knowledge of Machine Learning, Mathematical Modeling and Operations Research.Comfortable with R, Python, SAS and Weka, MATLAB, Relational databases. Deep understanding & exposure of Big Data Eco-system. ed with NoSQLDatabase including Hbase, Cassandra and
MongoDB. 
 Good Knowledge and experience in deep learning algorithms such as Artificial Neural net (ANN),Convolutional Neural Net (CNN) and Recurrent Neural Net (RNN), LSTM and RNN based speech recognition using Tensor Flow. 
 Experienced in Big Data with Hadoop, MapReduce, HDFS and Spark. 
 Experienced in Data Integration Validation and Data Quality controls for ETL process and Data
Warehousing using MS Visual Studio, SSAS, SSISandSSRS. 
 Proficient in Tableau and R-Shiny data visualization  to analyze and obtain insights into largedatasets, create visually powerful and actionable interactive reports and dashboards. 
 Automated recurring reports using SQL andPython and visualized them on BI platform like Tableau. 
 ed in development environment like Git and VM. 
 Excellent communication . Successfully ing in fast-paced multitasking environment bothindependently and in collaborative team, a self-motivated enthusiastic learner.
Willing to relocate to: vr,md,wato
Authorized to  in the US for any employer
 Experience

Data Scientist/ Machine Learning Engineer
BBA Aviation - Orlando, FL
August 2018 to Present
Description: Our people are the foundation of our success. Their service  and their functional, operational and engineering expertise are the core of our business. We are committed to investing in and empowering our employees and providing a safe  environment so that they can have rewarding careers. 
 
Responsibilities: 
 Utilized Spark, Scala, Hadoop, HQL, VQL, oozie, pySpark, Data Lake, TensorFlow, HBase, Cassandra,Redshift, MongoDB, Kafka, Kinesis, Spark Streaming, Edward, CUDA, MLLib, AWS, Python, a broad variety of machine learning methods including classifications, regressions, dimensionally reduction etc.
 
 Utilized the engine to increase user lifetime by 45% and triple user conversations for targetcategories. 
 Application of various machine learning algorithms and statistical modeling like decision trees, textanalytics, natural language processing (NLP), supervised and unsupervised, regression models, social net analysis, neural nets, deep learning, SVM, clustering to identify Volume using scikit-learn package in python, Matlab. 
 ed onanalyzing data from Google Analytics, AdWords, Facebook etc. 
 Evaluated models using Cross Validation, Log loss function, ROC curves and used AUC for featureselection and elastic  like ElasticSearch, Kibana. 
 Performed Multinomial Logistic Regression, Decision Tree, Random forest, SVM to classify package isgoing to deliver on time for the new route. 
 Performed data analysis by using Hive to retrieve the data from Hadoop cluster, Sql to retrieve datafrom Oracle database and used ETL for data transformation. 
 Performed Data Cleaning, features scaling, features engineering using pandas and numpy packagesin python. 
 Performed data cleaning and feature selection using MLlib package in PySpark and ing withdeep learning frames such as Caffe, Neon. 
 Developed Spark/Scala,R Python for regular expression (regex) project in the Hadoop/Hiveenvironment with Linux/Windows for big data resources. 
 Used clustering technique K-Means to identify outliers and to classify un-labeled data. 
 Tracking operations using sensors until certain criteria is met using AirFlow. 
 Responsible for different Data mapping activities from Source systems to Teradata using utilities like
TPump, FEXP,BTEQ, MLOAD, FLOADetc 
 Addressed over fitting by implementing of the algorithm regularization methods like L1 and L2. 
 Used Principal Component Analysis in feature engineering to analyze high dimensional data. 
 Used MLlib, Spark's Machine learning library to build and evaluate different models. 
 Created and designed reports that will use gathered metrics to infer and draw logical conclusions ofpast and future behavior. 
 Developed MapReduce pipeline for feature extraction using Hive and Pig. 
 Created Data Quality Scripts using SQL and Hive to validate successful data load and quality of thedata. Created various types of data visualizations using Python and Tableau. 
 
Environment: Python 2.x, CDH5, HDFS, Hadoop 2.3, Hive, Impala, AWS, Linux, Spark, Tableau Desktop, SQL Server 2014, Microsoft Excel, Matlab, Spark SQL, Pyspark.
Sr.Data Scientist/ Machine Learning Engineer
USAID - Washington, DC
May 2017 to July 2018
Description: USAID leads international development and humanitarian efforts to save lives, reduce poverty, strengthen democratic governance and help people progress beyond assistance. 
 
Responsibilities: 
 Extracted data from HDFS and prepared data for exploratory analysis using data munging  Built models using Statistical techniques like Bayesian HMM and Machine Learning classification models like XGBoost, SVM, and Random Forest. 
 Participated in all phases of data mining, data cleaning, data collection, developing models,validation, and visualizationand performed Gap analysis. 
 A highly immersive Data Science program involving Data Manipulation&Visualization, Web Scraping,
Machine Learning, Python programming, SQL, GIT, MongoDB, Hadoop. 
 Setup storage and data analysis  in AWS cloud computing infrastructure. 
 Installed and used CaffeDeep Learning Frame 
 ed on different data formats such as JSON, XML and performed machine learning algorithms inPython. 
 ed as Data Architects and IT Architects to understand the movement of data and its storage andER Studio 9.7 
 Used pandas, numpy, seaborn, matplotlib, scikit-learn, scipy, NLTK in Python for developing variousmachine learning algorithms. 
 Data Manipulation and Aggregation from different source using Nexus, Business Objects, Toad, PowerBI and Smart View. 
 Implemented Agile Methodology for building an internal application. 
 Focus on integration overlap and Informatica newer commitment to MDM with the acquisition ofIdentity Systems. 
 Coded proprietary packages to analyze and visualize SPCfile data to identify bad spectra andsamples to reduce unnecessary procedures and costs. 
 Programmed a utility in Python that used multiple packages (numpy, scipy, pandas) 
 Implemented Classification using supervised algorithms like Logistic Regression, Decision trees,Naive Bayes, KNN. 
 As Architect delivered various complex OLAPdatabases/cubes, scorecards, dashboards and reports. 
 Updated Python scripts to match training data with our database stored in AWS Cloud Search, so thatwe would be able to assign each document a response label for further classification. 
 Used Teradata utilities such as Fast Export, MLOAD for handling various tasks data migration/ETLfrom OLTP Source Systems to OLAP Target Systems 
 Data transformation from various resources, data organization, features extraction from raw andstored. 
 Validated the machine learning classifiers using ROC Curves and Lift Charts. 
 
Environment: Unix, Python 3.5.2, MLLib, SAS, regression, logistic regression, Hadoop 2.7.4, NoSQL, Teradata, OLTP, random forest, OLAP, HDFS, ODS, NLTK, SVM, JSON, XML and MapReduce.
Data Scientist
Bytemark, Inc - New York, NY
January 2016 to April 2017
Description:Bytemark team is a talented group of people dedicated to building a mobile payment system to serve both consumers and merchants while maintaining the highest standards of security. Built several Business Intelligence applications with their specialties in Data Science and Machine Learning. I  to integrate ML solutions into the existing business processes to improve decision making and ROI. 
 
Responsibilities: 
 Utilize a broad variety of statistical packages like SAS, R, MLIB, Graphs, Hadoop, Spark, MapReduce,Pig and others 
 Converted time lag problems in order fulfilment into Data mining tasks 
 Performed Data Profiling to assess data quality using SQL through complex internal database 
 Improved sales and logistic data quality by data cleaning using NumPy, SciPy, Pandas in Python 
 Built Data warehouse to support end-user queries with Oracle and MS Visual Studio 
 Designed and implemented Dimensional DataModelling for order fulfilment process 
 Deployed SSIS packages to complete ETL and Data Mapping process 
 Transformed data through methods like Aggregation, Slowly Changing Dimension, Splitting 
 Derived business intelligence report for order fulfilment using MS SSAS and SSRS 
 Determined regression model predictors using Correlation matrix for Factor analysis in R 
 Built Regression model to understand order fulfilment time lag issue using Scikit-learn in Python 
 Optimized predictive model by reducing insignificant variables using Stepwise Regression 
 Empowered decision makers with data analysis dashboards using Tableau and Power BI 
 Interface with other  teams to extract, transform, and load (ETL) data from a wide varietyof data sources 
 Own the functional and non-functional scaling of software systems in your ownership area. 
 Provides input and recommendations on technical issues to BIEngineers, Business&DataAnalysts andData Scientists. 
 Outstanding analytical and problem-solving  are essential. 
 
Environment: - Python, Hive, C/C++, C#, Java or Python, Bash, HTML5, PERL, Processing, Python and J Query, SOAPUI, WCF, WPF, VSO, TFS, GIT,XML, XSD, SQL Server 2008, Oracle 10/11g,.
Data Scientist
Ameriprise Financial, Inc - New York, NY
March 2014 to December 2015
Description:Ameriprise Financial, Inc. is an American diversified financial services company to help people feel confident about their financial future. It provides financial planning, products and services, including wealth management, asset management, insurance, annuities and estate planning. 
 
Responsibilities: 
 Supported MapReduce Programs running on the cluster. 
 Configured Hadoop cluster with Name node and slaves and formatted HDFS. 
 Used Oozie flow engine to run multiple Hive and Pig jobs. 
 Performed MapReduce Programs those are running on the cluster. 
 Developed multiple MapReduce jobs in java for data cleaning and pre-processing. 
 Analyzed the partitioned and bucketed data and compute various metrics for reporting. 
 Involved in loading data from RDBMS and web logs into HDFS using Sqoop and Flume. 
 ed on loading the data from MySQL to HBase where necessary using Sqoop. 
 Developed Hive queries for Analysis across different banners. 
 Extracted data from Twitter using Java and Twitter API.ParsedJSON formatted twitter data anduploaded to database. 
 Launching AmazonEC2 Cloud Instances using Amazon Images (Linux/ Ubuntu) and Configuringlaunched instances with respect to specific applications. 
 Exported the result set from Hive to MySQL using Sqoop after processing the data. 
 Analyzed the data by performing Hive queries and running Pigscripts to study customer behavior. 
 Have hands on experience ing on Sequence files, AVRO, HAR file formats and compression. 
 Used Hive to partition and bucketdata. 
 Experience in writing MapReduce programs with JavaAPI to cleanse Structured and unstructureddata. 
 Wrote Pig Scripts to perform ETL procedures on the data in HDFS. 
 Created HBase tables to store various data formats of data coming from different portfolios.  ed on improving performance of existing Pig and HiveQueries. 
 
Environment: -SQL/Server, Oracle 9i, MS-Office, Teradata, Informatica, ER Studio, XML, Business Objects, Deep learning approaches
Data Analyst/Modeler
Suditi Soft  Pvt Ltd - Hyderabad, Telangana December 2012 to February 2014
Description:Suditi Soft  is a software solutions company. ed as a Data Engineer, involved in various activities such as Data Exploration, Data Synchronization, Data Transformation, Data Governance, Data Model Creation and Performance Optimization. Involved in all phases of the SDLC of the project, starting from requirement gathering till development of data models. Created status reports and delivered action plans. 
 
Responsibilities: 
 Participated in JAD sessions, gathered information from Business Analysts, end users and otherstakeholders to determine the requirements. 
 Developed the logical data models and physical data models that confine existing condition/potentialstatus data fundamentals and data flows using ER Studio. 
 Created Data warehousing methodologies/Dimensional Data modeling techniques such as Star/
Snowflake schema using ERWIN9.1. 
 Extensively used AginityNetezzabench to perform various DDL, DML etc. operations on Netezzadatabase. 
 Designed the Data Warehouse and MDM hub Conceptual, Logical and Physical data models.  Performed Daily Monitoring of Oracle instances using Oracle Enterprise Manager, ADDM, TOAD, monitor users, table spaces, memory structures, rollback segments, logs and alerts. 
 Involved in Teradata SQL Development, Unit Testing and Performance Tuning and to ensure testingissues are resolved on the basis of using defect reports. 
 Customized reports using SAS/MACRO facility, PROC REPORT, PROC TABULATE and PROC. 
 Translate business and data requirements into Logical data models in support of Enterprise
DataModels, ODS, OLAP, OLTP, Operational Data Structures and Analytical systems. 
 ed on database testing, wrote complex SQL queries to verify the transactions and business logiclike identifying the duplicate rows by using SQL Developer and PL/SQL Developer. 
 Used Teradata SQL Assistant, Teradata Administrator, PMON and data load/export utilities like BTEQ,FastLoad, Multi Load, Fast Export, TPumpon UNIX/Windows environments and running the batch process for Teradata. 
 Hands on Data warehouse concepts like Data warehouse Architecture, Star schema, Snowflakeschema, and Data Marts, Dimension and Fact tables. 
 Developed SQL Queries to fetch complex data from different tables in remote databases using joins,database links and Bulk collects. 
 Migrated database from legacy systems, SQL server to Oracle and Netezza. 
 ed on SQL Server concepts SSIS (SQL Server Integration Services), SSAS (Analysis Services) andSSRS (Reporting Services). 
 
Environment: -ER Studio, OBIEE 11.1.1.6, Teradata13.1, SQL, PL/SQL, BTEQ, DB2, Oracle, MDM, Netezza, ETL, RTF UNIX, SQL Server2010, Informatica, SSRS, SSIS, SSAS, SAS, Aginity.
Data Analyst
Systopic Laboratories Pvt. Ltd - Hyderabad, Telangana January 2011 to November 2012
Description:Systopic Laboratories Pvt. Ltd. was incorporated in the year 1984 with an objective to provide quality innovative therapeutic solutions. The guiding principle at Systopic is pursuit of
""Excellence through People & Innovation"". This dictum forms the basis of all thinking & action at Systopic. 
 
Responsibilities: 
 Analyze business information requirements and model class diagrams and/or conceptual domainmodels. 
 Gather & Review Customer Information Requirements for OLAP and building the data mart. 
 Performed document analysis involving creation of Use Cases and Use Case narrations using
Microsoft Visio, in order to present the efficiency of the gathered requirements. 
 Calculated and analyzed claims data for provider incentive and supplemental benefit analysis using
Microsoft Access and Oracle SQL. 
 Analyzed business process flows and assisted in the development of ETL procedures formapping data from source to target systems. 
 ed with BTEQ to submit SQL statements, import and export data, and generate reports in Terra-data. 
 Enterprise Metadata Library with any changes or updates. 
 Document data quality and traceability documents for each source interface. 
 Establish standards of procedures. 
 Generate weekly and monthly asset inventory reports. 
 Managed the project requirements, documents and use cases by IBM Rational RequisitePro. 
 Assisted in building an Integrated LogicalDataDesign, propose physical database design for buildingthe data mart. 
 Document all data mapping and transformation processes in the Functional Design documents basedon the business requirements. 
 
Environment: -SQL Server 2008R2/2005 Enterprise, SSRS, SSIS, Crystal Reports, Windows Enterprise Server 2000, DTS, SQL Profiler, and Query Analyzer.
Education

Bachelor's


Db2, Microsoft sql server, Microsoft sql server 2008, Sql server, Sql server 2008, Mysql, Oracle, Sql,
Cassandra, Hdfs, Impala, Mapreduce, Oozie, Sqoop, Hbase, Kafka, Flume, Hadoop, Mongodb, Splunk
Additional Information

TECHNICAL  
 
BigData/Hadoop  Hadoop, HDFS, YARN, MapReduce, Hive, Pig, Impala, Sqoop, Flume,
Spark, Kafka, Storm, Drill, Zookeeper and Oozie 
Languages 
HTML5,DHTML, WSDL, CSS3, C, C++, XML,R/R Studio, SAS Enterprise Guide, SAS, R (Caret, Weka, ggplot), Perl, MATLAB, Mathematica, , Json, Ajax, Java, Scala, Python (NumPy, SciPy, Pandas, Gensim, Keras), Java Script, Shell Scripting 
 
NO SQL Databases Cassandra, HBase, MongoDB, MariaDB 
Business Intelligence  
Tableau server, Tableau Reader, Tableau, Splunk, SAP Business Objects, OBIEE, SAP Business Intelligence, QlikView, Amazon Redshift, or Azure Data Warehouse 
 
Development  Microsoft SQL Studio, IntelliJ, Eclipse, NetBeans. 
Development Methodologies Agile/Scrum, UML, Design Patterns, Waterfall 
Build  Jenkins, Toad, SQL Loader, Maven, ANT, RTC, RSA, Control-M, Oziee, Hue, SOAP UI 
Reporting  MS Office (Word/Excel/Power Point/ Visio/Outlook), Crystal reports XI, SSRS, cognos 7.0/6.0. 
Databases Microsoft SQL Server 2008,2010/2012, MySQL 4.x/5.x, Oracle 11g, 12c, DB2, Teradata, Netezza 
Operating Systems All versions of Windows, UNIX, LINUX, Macintosh HD, Sun Solaris",Data Scientist,emailed,indeed job post,resume
"  
MS in Business Analytics graduate student with experience in providing Audit and 
Consumer products sector 
Possess  in R, Tableau, Python, SQL & Big Data  
 
Advisory services in Industries, Infrastructure and
University of Illinois at Chicago  MS in Business Analytics 	          	          	                           
                              Aug 2018  Dec 2019 
Uttar Pradesh  University  B.Tech in Electronics and Communication Eng.    	              
                         Aug 2010  May 2014 
Statistical/Analytical 	R Studio, SPSS, RapidMiner,  	 Database 
 	MS Excel, Julia, SAS 
Visualization and 	Tableau, Google Analytics, 	 Big Data  
Reporting 	ggplot(R) 	 Languages/Programming 
Productivity 	Microsoft Office Suite 	 
 
MS SQL Server, Oracle, MySQL, 
MongoDB, MS Access, SAP HANA 
Database 
Hadoop & Spark Ecosystems 
Python, R, C, C++ 
 
  
 
 
 
 
	Consultant  IT Risk and Assurance  Ernst & Young, LLP (India)   	 	 	                                       June 2014  July 2018 
 Performed audits related to Program Risk Management, Financial Audit IT Integration, Application Risk and Controls Identification, 
Internal Audit - IT, Service Organization Control Reports (SOCR) in accordance with the attestation standards (ISAE 3402, SSAE 16 and SSAE 18) 
 Performed Fraud Investigation using Data Analytics (using Tableau, MS excel & SQL) and Software Compliance Management 
 Performed internal audits pertaining to vendor risk management, scrap sales, material management (purchase cycle), financial transaction and asset management 
 Performed Marketing collateral planning and execution, business development and contributing to EYs knowledge repository Key Assignments: 
 IT External Audit for the largest Indian Multinational Conglomerate Company 
- Devised audit plan and calendar for quarterly limited reviews and annual audit which included sub-domains like IT General Controls Identification and testing, IT Application Controls testing 
 Performed Data Analytics for Journal Entries (using MS excel) and Data Analytics on transactional data for investigating fraud (using Tableau & python (package  numpy, pandas & matplotlib)) 
 Service Organization Control Review for a Global Business Process Management Company 
- Inspected the description of IT general Controls System for the period under inspection and the suitability of the design and operating effectiveness of controls described therein to achieve the related control s stated in the Description of System (DOS) 
- Conducted examination in accordance with the attestation standards (SSAE 16) established by the American Institute of Certified Public Accountants (AICPA) 
	ACADEMIC  	 	 	 	 	 	 	 	 	 	 	 

	Spam & Ham classifier using NLP (Python  NLTK, Numpy, Pandas & Scikit learn)   	 	                       May 2019  June 2019 
 Used TF-IDF and count vectorization methods for vectoring the data 
 Developed random forest and gradient boosting model for predictions 
 Chose random forest model in model selection due to its high precision as compared to gradient boosting 
	Donor Prediction in Marketing Campaign Analysis (RapidMiner) 	 	 	 	 	        Sept 2018  Dec 2018 
 Sample data from Paralyzed Veterans of America (PVA)s training dataset has been used to predict donors, and then built and compared predictive models like decision trees, and boosted trees, random forest, etc. for identifying donors 
	Credit Score Analysis (R, RapidMiner) 	 	 	 	 	 	 	 	        Sept 2018  Dec 2018  
 Gathered past credit applicants data from a credit dataset. Explained various reasons for good/bad credit score for an applicant and obtained a decision tree-based model to determine if new credit card applicants will present a good or bad credit risk 
	Google Analytics Customer Revenue Prediction (R)  	 	 	 	                                       Sept 2018  Dec 2018 
 Predicted the natural log of the total revenue per unique users basis the exploration of data. The process included the classification as well as regression, as we first needed to fetch patterns to classify users as unique or regular and then regression for the predictions of total revenue per unique users 
	Text Mining & Sentiment Analysis (R, RapidMiner) 	 	 	 	 	 	                        Sept 2018  Dec 2018  
 Used Bag of Words (BoW) approach of tokenizing, normalizing and filtering the text to create the document term matrix 
 Used 3 dictionaries of positive and negative words to obtain aggregated sentiment scores for each movie review 
 Developed models like Lasso Logistic Regression, KNN, Random Forest and SVM to predict the review sentiment (positive or negative)  ",Data Scientist,emailed,indeed job post,resume
"Graduate in Computer Science with 2+ years experience solving challenging problems in data science and machine learning, with a focus on python programming  
 
The University of Texas at San Antonio (UTSA) 	               May 2019?	 
Masters of Science in Computer Science  
Courses - Cloud Computing, Computers and Net Security, Machine Learning, Introduction to Data Science, Database 
Management Systems, Algorithms, Data Structures	 	 
BMS College of Engineering, Bangalore, India 	May 2016 
Bachelor of Engineering in Information Science and Engineering 	 	 
  
Languages:  Python, C, C++ ?	| Analytics :?	 Pandas, SQL, NumPy, Scikit Learn,  Spark, Spacy, Jupyter Notebook?	 
Visualization : Matplotlib, Seaborn, Plotly ?	|? ? Web Scraping Frames:?	 Beautiful Soup ?	|? ? OS:?	 Linux, Windows?	 
Experience 
Data Science Fellow | The Data Incubator	 	       April 2019  May 2019?	 
 Trained a Decision Tree Classifier to predict and analyze the important features responsible for the attrition rates of a company using the data scraped from anonymous Glassdoor reviews. The training model had an accuracy of 71%, which can be used by companies to effectively address their attrition rates.  
Graduate Research Assistant | UTSA 	 	 	 	 	        Jan 2018  Mar 2019 
 Project HuddleUp : HuddleUp, a social media tool used in the university amongst students for interaction. From the data gathered, there were about 55 attributes that were divided into Social, Technological and User factors. These factors were ranked by training the Extra Trees Classifier Machine Learning Model 
 Ushahidi project : ed on collecting various datasets to train a Machine Learning model on crowdsourced data to enable efficient identification of natural disasters. 
? Email classification : Accumulated various email datasets which were used to train a Machine Learning classification model to classify emails as spam, legit and phishing 
Graduate Research Assistant | Open Cloud Institute 	 	                        Jan 2017  Dec 2017 
 Trained a k-Nearest Neighbors model using the Pima Indian Diabetes data which contained attributes that affect diabetes like Age, BMI, Pregnancy, Glucose Levels and Blood Pressure. Trained the kNN algorithm to determine the probability of a person having a risk of diabetes in the future. 
 Face emotion recognition Project: Trained a Convolution Neural Net and used the tensorflow library to detect emotions with the FER dataset 
Software Engineer | Clef Software, Bangalore 	 	 	 	                        June 2015 - June 2016 
 Researched about Drone Software modules such as Ground Control Software and Flight Control Software. Performed simulation using the Pixhawk board, Radio transmitter-receiver and Mission planner software to customize drone software. 
 Developed the companys websit?	e ? using Drupal to code and customize, leading to a user friendly website. 
 
Net Based Classification of Breast Cancer Metastasis | UTSA                                                                     June 2018 - March 2019 
 Analyzed 30 breast cancer gene based datasets and built models for different Classification algorithms. Area under the Curve was chosen as the metric to determine which algorithm performed better for each dataset. 
 Analyzed Lung, Breast and Cervical cancer datasets using statistical techniques. Supervised Machine Learning algorithms were devised to determine accuracy of each dataset. 
HR analytics | UTSA	          Aug 2018 - Dec 2018 
 Analysis of the companys attrition rates and trends using datasets from Kaggl? e using ML techniques like kNN, Linear Regression and Random forest. 
Implementation of Security Protocols in a Client-Server Model | UTSA	                         Jan 2017 - May 2017 
 To allow various clients access the server without breaking the confidentiality and integrity of the client information. This was achieved by socket programming and RSA algorithm implemented in Python. ",Data Scientist,emailed,indeed job post,resume
" 
IBM certified data scientist with 3.5+ years of experience using predictive modelling, data processing, and data mining algorithms to deliver insights and implement action-oriented solution to challenging business problems. Beat 700+ teams of deep learning scientists in a Kaggle competition, by building models that fit the best.  

* Hands-on experience in analysis like data extraction, data preprocessing, feature engineering, building predictive models, visualization and communicating insights and trends. 
* Well versed in data preprocessing steps such as exploration, aggregation, missing data imputation, sampling, feature selection, dimensionality reduction and outlier detection. 
* Proficient in machine learning algorithms such as linear regression, Gradient Boost, XGboost, Decision Trees, Random Forest, Support Vector Machines, K nearest neighbors, Neural Nets.   
* ed with big data (200GB+) in creating predictive model using Artificial neural nets (ANN) like CNN, and LSTM which are currently being implemented in small scale environment. 
* ed with various Python libraries such as NumPy, SciPy for mathematical calculations, Pandas for data preprocessing/wrangling, Matplotlib, Seaborn for data visualization, Sklearn for machine learning, PyTorch for deep learning and NLTK for NLP.  
* Developing statistical models in R using various supervised and unsupervised machine learning algorithms such as linear and logistic regression, Decision Trees, Ensemble methods, KNN, linear SVM, Naive Bayes, K-Means on structured and unstructured data. 
* Statistics methodologies such as hypothesis testing, ANOVA, principle component analysis (PCA), time series analysis. 
* Model validation and optimization with model selection, parameter tuning, and K-fold cross-validation. 
* Loading and analyzing real world datasets with Hadoop frame such as MapReduce, HDFS and SPARK. 
  
Programming 	 	 
: 
Python, R, SAS, and SPSS. 
Machine learning algorithms  
:  
Decision trees, random forest, XGBoost, SVM, KNN, and K-Mean.  
Deep learning algorithms 
:  
CNN, RNN, and LSTM.  
Big data  
:  
PySpark, Map reduce, fastai and Google Cloud Platform (GCP). 
Visualization 	 	 
:  
Tableau, Power BI, and RShiny.  
Web  	 
: 
JavaScript, jQuery, PHP, CSS, HTML, XML, JSON, and REST.  
Databases 	 	 
: 
SQL Server, MySQL, and NoSQL. 
Version Control 	 
:  
Jira, and Git/GitHub 
Machine Learning Libraries 
: 
TensorFlow, PyTorch, Scikit-learn, Pandas, NumPy, Scipy, Matplotlib,  
 	 	 	 
 
Seaborn, plotly, ggplot, dplyr, tidyverse, and NLTK. 
OS and Other softwares  
:  
Windows, Linux, MS Office (Excel, Access, Word, PPT). DATA SCIENTIST, SENSEONICS Inc., MD 
 

 
 
 
 
      March 2019 - Present 
 Created SQL database warehouse from database for data analytics team. 
 Extracting data from SQL Server and performing data wrangling in Python.  
 Kaplan-Meier survival analysis to see glucose sensor survival rates among various countries. 
 Glucose sensors life has been increased by 6% after identifying shelf-life and S0 to be the reason for early sensor retirement using K-means clustering.  
 From CGM data, built predictive algorithm using XGboost with 85% accuracy that is currently being tested in a small group of patients.  
 Using data from the SQL database, custom R scripts were written within PowerBI to manipulate data and generate automated monthly and weekly reports.  
 
DATA ANALYST INTERN, COVANCE, IN 
 
 
 
 
 
      May 2018  Aug 2018 
 Built predictive analytical models in support of organizations operational and business priorities.  
 Conducted a data regression analysis of the relationship between study cancels data and nature of the client, class of the drug, budget and length of the study, achieving a 10% more accurate prediction than earlier years.  
 Forecasted cancels data 15% better than previous years using ARIMA model for time series in Python.  
 Increased accessibility to this data by designing visualizations to include statistical graphs and information graphics in Tableau.  
 Imported, cleaned, merged and transformed datasets in R to build dashboards.  
 Built a KPI dashboard in RShiny which automatically sends monthly performance reports and restricts the access to different employees based on their position in the organization.   
 Created Access database and developed complex SQL queries to transform data sources into warehouses.  
RESEARCH DATA SCIENTIST, IUPUI, IN 
 
 
 
 
 
        Jan 2017  Dec 2018 
 Multiple claims like Dental claims, high cost claims and Emergency Department (ED) Medicaid insurance 
claims from 2017 Indiana Medicaid challenge were analyzed for average cost per recipient and average cost per claim among federally qualified & non-qualified clinics, primary care physicians (PCPs) and Non-PCPs.  
 Federally qualified health clinics and PCPs were found to have high cost claims than their counterparts which is due to preventive care and frequent visits in FQHCs and PCPs respectively.  
 Using demographics groups of individuals with high healthcare costs and more prevalent diseases were identified. Underlying conditions like job type, socioeconomic status and  level were correlated.   
 These claims were analyzed using various machine learning algorithms like:  o K-means clustering to identify patterns in amount & number of claims per diseases, physician specialties 
o Time series analysis to forecast claim amount, diseases per geographic region and  o Regression algorithms (Multivariate regression, Decision trees and Random forest) to identify underlying causes like , socio-economic status, drug abuse etc. in python.  
 Using the scores (gre, ielts, toefl) considered for admission into a Masters program at IUPUI, a using binary logistic regression prediction model was built in R with accuracy of 93% and F-1 score of 0.87.  
 Indiana Community Health Centers (CHCs) data was concatenated using MySQL and disease patterns were identified using descriptive, inferential statistics and were plotted in Tableau.  
 Negative pearsons correlation was identified between income and frequency of diseases. A predictive model was built using linear regression in Python with 84% accuracy. 
 Indiana State Department of Health (ISDH) data was normalized and correlated the no of active physicians with deaths per county in Indiana. Death patterns were visualized in all counties from 2011 to 2015 using ggplot2 in R.  
 Developed and maintained PL/SQL stored procedures, ETL and triggers for various academic based applications. 
 Built a web application using Python Flask to feed both human and machine results of analyzing Chest X-ray report which showed improved performance of human under machine guidance using statistical tests like ttests, one-way ANOVA, F-Score in python.  
 Experienced in Machine Learning Regression Algorithms like Simple, Multiple, Polynomial, SVR (Support Vector Regression), Decision Tree Regression, Random Forest Regression.  
 Experienced in Machine Learning Classification Algorithms like Logistic Regression, K-NN, SVM, Kernel SVM, Naive Bayes, Decision Tree & Random Forest classification. 
 Expertise in employing techniques for Supervised and Unsupervised (Clustering, Classification, PCA, Decision trees, KNN, SVM) learning, Predictive Analytics, Optimization methods, high-dimensional data analysis and Time Series Analysis. 
 Proficient in various statistical models like MANCOVA, two-way ANOVA, chi-square etc. 
 Performed various parametric tests like t-tests, one-way ANOVA and nonparametric statistical tests like spearmans correlation, Mann Whitney U test, Friedman test, Kruskall-Wallis test and Ad-hoc analysis.  
DATA ANALYST, DIC, KVSRSCOPS 
 
 
 
 
 
                   Oct 2015  Nov 2016 
 Slashed staffing costs 10% by predicting seasonality in patient visits per specialty, and effectively utilizing nursing and junior doctor staff without jeopardizing quality.   
 Predictive analytics were performed using time series healthcare data (patient vitals, ECG, EMR, ventilator and other clinical data) with various machine learning algorithms like support vector machine (SVM), decision trees, Naive Bayes classifier to predict adverse clinical event, health severity progression and mortality rates.  
 Collaborated with stakeholders from clinical, operations and product teams to identify analytics opportunities and leverage solutions, maintaining monitoring system in Tableau. 
 Creating queries for data extraction using SAS and reports generation using SQL, and Tableau.  

RSNA pneumonia detection challenge (Kaggle) 
* A deep learning algorithm was built to detect pneumonia with lung opacities from Chest X-rays. Along with pneumonia detection, bounding boxes were generated to identify the location of the opacities.  
* Our algorithm performed better than 700 teams participated in the competition.  
*  used: Convolutional Neural Nets (CNN), fastai, Python.  
Identification of diagnosis from discharge notes using RNN 
* Natural Language Processing (NLP) using Recurrent Neural Nets (RNN) was applied on MIMIC-III Note-events data to identify diagnosis from the discharge notes.  
* Diagnosis table was merged with note-events table; our model was trained on this table and could predict the diagnosis with 82% accuracy. 
*  used: Recurrent Neural Nets, LSTM, fastai, Python, Google Cloud Platform (GCP). 
Fraud detection in Medicare claims data from CMS 
* Medicare data (25 GB) was dumped from CMS and concatenated per each category like Part B Prescriber, Part D Prescriber, Inpatient, Outpatient, Nursing facilities etc. 
* Merging all these datasets to a single dataset, a fraud detection algorithm was created using machine learning algorithms like random forest, gradient boost descent and logistic regression with 80% accuracy. 
*  used: PySpark, SparkSQL, MLlib. 
Analysis of Substance Abuse Trends in UNITED STATES: AN EPIDEMIOLOGIC STUDY 
? Extracted 10 Million records, cleaned for missing values and preprocessing using R ? Data was analyzed by performing predictive analysis ?  used:  o R: Boruta algorithm for variable selection 
o Python: Recursive feature elimination; Logistic regression & Support Vector Machines  o Graphical representation: Choropleth using Plotly in Python 
Development of mobile application to monitor baby care and teach healthcare ers 
* A hybrid mobile application was built using Essential Care for Every Baby (ECEB) action plan and currently being tested.  
*  used: jQuery, JavaScript, CSS, HTML, Cordova, Ionic, and Frame7. 

	Masters in Health Informatics 	 	 	 	 	 	 	              Jan 2017 - Dec 2018 
	Indiana University Purdue University, Indianapolis   	 	 	 	 	 
	Doctor of Pharmacy (Pharm D) 	 	 	 	 	 	 	            Oct 2010  Aug 2016 
	Krishna University, India   	 	 	 	 	 	 	 	 

* Evaluating the implementation of deep learning in LibreHealth Radiology on Chest X-Rays.  ? Phronesis of AI in radiology: Super human meets natural stupidity.  

* IBM data science professional. 
* Mentor, Google Code-in, 2017. 
* Knowledge of ICD Codes (ICD-10/ICD-9), SNOMED, LOINC, CPT, HL-7, CCDA, HIPAA, HITECH, CMS, 
Project lifecycle, code testing, statistical analysis and predictive analysis.  ",Data Scientist,emailed,indeed job post,resume
" 8+ years of experience building interpretable machine learning models, and building end to enddata pipelines which included extracting, transforming and combine all incoming data with the goal of discovering hidden insight, with an eye to improve business processes, address business problems or result in cost savings 
 
 Hands on experience communicating business insights by dashboarding in Tableau. Developedautomated tableau dashboards that helped evaluate and evolve existing user data strategies, which include user metrics, measurement frames, and methods to measurement. Also developed and deployed dashboards in Tableau and RShiny to identify trends and opportunities, surface actionable insights, and help teams set goals, forecasts and prioritization of initiatives 
 
 Experience in architecting and building comprehensive analytical solutions in Marketing, Salesand Operations functions across , Retail and Banking industries. ed closely with functional team leaders (in Product, Operations, Marketing, etc.) to explain analysis, findings, and recommendations 
 
 Experienced in acquiring, merging, cleaning, analyzing and mining structured, semi-structured andunstructured data sets for analysis 
 
 Strong track record of contributing to successful end-to-end analytic solutions (clarifying businesss and hypotheses, communicating project deliverables and timelines, and informing action based on findings) 
 
 Expertise writing production quality code in SQL, R, Python and Spark. Hands on experience buildingregression and classification models and other unsupervised learning algorithms with large datasets in distributed systems and resource constrained environments 
 
 Expert knowledge in supervised and unsupervised learning algorithms such as Ensemble Methods
(Random forests), Logistic Regression, Regularized Linear Regression, SVMs, Deep Neural Nets, Extreme Gradient Boosting, Decision Trees, KMeans, Gaussian Mixture Models, Hierarchical models, and time series models (ARIMA,GARCH, VARCH etc.) 
 
 Led independent research and experimentation of new methodologies to discover insights,improvements for problems. Delivered findings and actionable results to management team through data visualization, presentation, or training sessions. Proactively involved in roadmap discussions, data science initiatives and the optimal approach to apply the underlying algorithms 
 
 Experience building interpretable machine learning models, and building end to end data pipelineswhich included extracting, transforming and combine all incoming data with the goal of discovering hidden insight, with an eye to improve business processes, address business problems or result in cost savings 
 
 Experience ing with large data and metadata sources ; interpret and communicate insights andfindings from analysis and experiments to both  and non- audiences in ad, service, and business 
 
 Experienced in Data Modeling retaining concepts of RDBMS, Logical and Physical Data Modeling until
3NormalForm (3NF) and Multidimensional Data Modeling Schema (Star schema, Snow-Flake Modeling, Facts and dimensions). Hands on experience in optimizing the SQL Queries and database performance tuning in Oracle, SQL Server and Teradata databases
Willing to relocate: Anywhere
Sponsorship required to  in the US
 Experience

Data Scientist
VERIZON - Colorado Springs, CO
October 2018 to May 2019
 Utilizing analytical, statistical and programming  to collect, analyze and interpret large data setsto develop data driven and  solutions to difficult business problems using  such as SQL,
Python and R for Net Transformation project. 
 Through machine learning algorithms such as Linear Regression, Logistic Regression, RandomForests, Support Vector Machines, Clustering and others identify the accurate revenue sources of billing records in Net Transformation project and clearly articulating pros and cons of various techniques. 
 Supporting the operations and maintenance of AWS Advanced Analytics platform named Finalytics. 
 Developing systems, applications, and visual dashboards using Tableau. 
 Ensuring data quality and promoting process improvement. 
 Designing, developing, implementing and maintaining a database to manage data analysis efforts. 
 Designing creative approaches to uncover the biggest opportunities for cost and time savings. 
 Defining and driving the analytics strategy and modeling approaches. 
 Communicating findings to stakeholders after data analysis. 
 Using customer's data identify opportunities and optimize conversion and revenue/profitability withinVerizon's financial business unit. 
 Analyzing customer paths across all channels - Digital / Contact Centers / Retail to help improve theoverall customer experience and business outcomes. 
 Managing model development through the various cycles of the development process 
 Managing daily, weekly, and monthly report execution and distribution, highlighting Key PerformanceIndicators 
 Partnering with Finance, Marketing and other cross functional teams in Verizon to support businessinitiatives.
Data Scientist
APPLE INC - Cupertino, CA
December 2014 to September 2018


Bachelor of Engineering in Computer Science
VIT UNIVERSITY - Vellore, Tamil Nadu
August 2005 to May 2009


Clustering (Less than 1 year), data analysis (Less than 1 year), Logistic regression (Less than 1 year), machine learning (Less than 1 year), Marketing analysis (Less than 1 year), SQL
Additional Information

  
Statistics/ML 
Exploratory Data Analysis: Univariate/Multivariate Outlier detection, Missing value imputation,
Histograms/Density estimation, EDA in Tableau 
Supervised Learning: Linear/Logistic Regression, Lasso, Ridge, Elastic Nets, Decision Trees, Ensemble
Methods, Random Forests, Support Vector Machines, Gradient Boosting, XGB, Deep Neural Nets, Bayesian Learning 
Unsupervised Learning: Principal Component Analysis, Association Rules, Factor Analysis, K-Means,
Hierarchical Clustering, Gaussian Mixture Models, Market Basket Analysis, Collaborative Filtering and
Low Rank Matrix Factorization 
Feature Engineering: Stepwise, Recursive Feature Elimination, Relative Importance, Filter Methods,
Wrapper Methods and Embedded Methods 
Statistical Tests: T Test, Chi-Square tests, Stationarity tests, Auto Correlation tests, Normality tests,
Residual diagnostics, Partial dependence plots and Anova 
Sampling Methods: Bootstrap sampling methods and Stratified sampling 
Model Tuning/Selection: Cross Validation, AUC, Precision/Recall, Walk Forward Estimation, AIC/BIC
Criterions, Grid Search and Regularization 
Time Series: ARIMA, Holt winters, Exponential smoothing, Bayesian structural time series 
 
Machine Learning / 
Deep Learning 
 
R: caret, glmnet, forecast, xgboost, rpart, survival, arules, sqldf, dplyr, nloptr, lpSolve, ggplot 
Python: pandas, numpy, scikit-learn, scipy, statsmodels, matplotlib, tensorflow 
SAS: Forecast server, SAS Procedures and Data Steps 
Spark: MLlib, GraphX 
SQL: Subqueries, joins, DDL/DML statements 
 
Databases/ETL/Query Teradata, SQL Server, Postgres and Hadoop (MapReduce); SQL, Hive, Pig and Alteryx 
Visualization Tableau, ggplot2 and RShiny 
Prototyping PowerPoint, RShiny and Tableau",Data Scientist,emailed,indeed job post,resume
"Authorized to  in the US for any employer
 Experience

Freelance
Aetna
April 2015 to August 2016
Sql Consultant (freelance )---Windward Consulting 
 
AETNA Business Analyst/Data Modelor/Testing 
 
Analyzed Aetna database structure/data across several departments (sqlserver, oracle, active directory xenapp, proprietary datasources, cloud servers etc.) and create a Data Model and Data Warehouse. Provided recommendation for a Analytical Tool after evaluating  such as Tableu, Splunk etc Analysis required research on a cost effective method to build the warehouse: (SSIS(for ETL), SSAS(analytical) and SSRS(Reproting) or use an analytical tool.
SQL Server Database Administrator/Qa
Cambridge Associates, VA
March 2001 to November 2010
Cambridge Associates ( www.cambridgeassociates.com ) provides unbiased financial management, information and advice on financial and investment issues to endowed nonprofit institutions and private clients. I perform the administration of SQL Server databases (2005/200/7.0/6.5), physical database design, schema management, database development, performance tuning, data migration, backup and recovery strategies, database software installations and upgrades, troubleshooting, resolving errors and failures, capacity planning and resource utilization. 
 
Responsibilities: 
? Installing SQL Server or upgrading an existing SQL Server 
? Clustering Servers 
? Replication Across Servers 
? Resolve errors/issues resulting from upgrade from 2000 to 2005 in ssis packages, upgrading products/, researching lastest database tips 
?Qa Testing 
? Data Analyst for Factset Data for major application--download data from external website andincorporate into ssis package, required constant manipulation as tickers and currency changed. Correct bad data. 
? Design logical and physical implementation of the database on SQL Server 
? Reorganize database structures as needed to meet new requirements and to improve performance 
? Analyze and tune the database for optimal performance using a variety of tuning strategies, including use of database traces and performance monitoring  in the evaluation process, and index evaluation/reorganization, query modification. 
? Maintaining the proper use of storage by making sure that databases and transaction logs are created correctly, monitoring space requirements, and adding new storage space when required 
? Performing the Backup and Recovery of database by scripting all the objects and its attributes (permissions, logins, roles, indexes etc). 
? Maintaining Database Users and Security for each user. Assigning permissions to the employees andclients on various database objects such as tables, views, and stored procedures. 
? Responsible for maintaining Application schema, Schema changes, Schema versions on 
Production, Testing and Development Databases. 
? Compiled scripts on Production Server 
? ed with developers to resolve data, application and performance issues: ? Monitored performance, optimized database queries, assigns proper indexes, aiding them in creating triggers & stored procedures, roles, and established and maintains 
Naming Conventions and Standards. 
? Wrote DTS/SSIS Packages and jobs to perform scheduled tasks and SQL processes 
? Migrations of SQL Server from 7.0 to 2000 to 2005 
? Migrations of SQL Server from 6.5 to 7.0 
? Setting up and maintaining data replication 
? Support to the Citrix and SQL Server 2000 Project: Citrix/Active Directory test lab ? To develop and document procedures for administration of the database. To maintain records of all changes and developments to the database. 
? Researching the web for latest news in the SQL Server World and presenting it to Sr. 
DBA 
? Created in-house training presentations 
? Providing 24-hour access 
 
Environment: SQL Server 2005/2000/7.0/6.5, SQL r, Query Analyzer, DB Artisan, SQL 
Compare, RSScripter, Sql Litespeed, Erwin, Sql Blocks
Analyst & SQL Server Database Administrator
MS SQL
2000 to 2005
I am a highly motivated Analyst & SQL Server Database Administrator with over 12 years of experience performing installation, configuration and development, and administration functions of MS SQL Server databases (2008/2000/7.0/6.5).  closely and effectively with all levels of management to satisfy project and productivity requirements. Have proven track record with successful implementation of medium to large turnkey . Capable of  analysis and skilled at making quick decisions based on experience, and judgment. Excellent team player and can  under strong pressure on multiple  and have good communication .
Consultant
VOCUS, INC - Lanham, MD
November 1999 to March 2001
Vocus, Inc. supplies On-Demand Software for Corporate Communications and Public Relations. Involved in administration of SQL Server databases, schema management, backup and recovery strategies, troubleshooting, resolving errors and failures, capacity planning and resource utilization. 
 
Responsibilities: 
? Responsible for providing knowledge and  support on all Vocus software applications and their relevance to targeted  industries including the government. 
? Maintained Customer Database 
? Performed the Backup and Recovery of database by scripting all the objects and its attributes (permissions, logins, roles, indexes etc). 
? Responsible for maintaining Application schema, Schema changes, Schema versions on 
Production, Testing and Development Databases. 
? Maintained Database Users and Security for users. Assigning permissions to the employees andclients on various database objects such as tables, views, and stored procedures. 
? ed with developers to resolve data, application and performance issues: ? Monitored performance, optimized database queries, assigns proper indexes, aiding them in creating triggers & stored procedures and established and maintains Naming 
Conventions and Standards. 
? Wrote DTS Packages to import client's data in our database, which involved writing tasks for furtherdata manipulation. 
? Exported data for clients in Flat files, Excel and Access formats. 
? Created DSNs on users' machines so the software they're using are pointing to an appropriatedatabase. 
? Identification and trouble-shooting of software, environmental, and configuration problems. 
? 
 
? Support clients to help resolving advanced  issues relating to connectivity, database related problems, Windows OS configuration, printers, and complex software issues. 
? Hands on training for employees on database, Vocus software application and other MS products. 
 
Environment: SQL Server 2005/2000/7.0/6.5, SQL r, Query Analyzer, SQL Compare,
Client/Server Coordinator & SQL Server Database Administrator
Computer Associates, Inc
October 1998 to November 1999
CA is one the world's largest IT management software providers. CA's software and expertise unify and simplify complex IT environments in a secure way across the enterprise for greater business results. 
 
Responsibilities: 
Client/Server Coordinator 
? Handled business related issues, such as product invoices, renewing maintenance 
licenses, processing purchase orders, assisting clients with their federal accounts, and placing software and documentation orders, and data entry on client database. 
 
Database Administrator 
? Updated queries, entries, I/O, addresses, creating new site id's, transfer license information, and invoice transfer, cancellations and maintenance reinstatement. 
? Researched and resolved all-star issue Verify, research and update tops database. 
 
Ritters & Co, Vienna, VA Administrative Support 
 
? Responsible for handling accounts payable, payroll, filing of quarterly and year-end tax reports, and compiling tax reports as well as financial statements. 
? Other duties included answering phone calls, interacting with clients, ordering supplies, scheduling meetings, and fax machine maintenance.
Research Assistant
George Washington Psychology Dept
August 1995 to September 1996
Research Assistant 
? Compiled and analyzed data concerning minority groups. 
? Applied statistical analysis to attain results for psychology publishing group. 
 
Assistant Analyst 
? Assisted in customization of a confidential database to keep payroll and personal records
Education

Bachelor's


sql server dba (10+ years)
Certifications/Licenses

Mcdba
Additional Information

  Summary 
Operating System DBA/Development  RDBMS Data 
Modeling 
Windows XP, 2003, 2000, DB Artisan, SQL Compare, Scripter, MS SQL Server Erwin 
Windows NT 4.0, 95/98, Redgate -Litespeed, Visual Basic 6.5, 7.0, 2000, Visio 
MVS, HP Unix, SCO Unix 6.0, IIS, HTML 2005 
MS Access, Strong in database Concepts",Data Scientist,emailed,indeed job post,resume
"
 :
* Over 10+ years of  IT experience with experience in analysis, architectural design, prototyping, development, Integration and testing of applications using Java/J2EE  and experience in Big Data Analytics as Hadoop Developer with Hadoop ecosystem .
* Delivery experience on major Hadoop ecosystem Components such as Pig, Hive, Spark Kafka, Elastic Search &Hbase and monitoring with Cloudera Manager.Extensive ing experience using Sqoop to import data into HDFS from RDBMS and vice-versa.
* In-depth experience and knowledge in developing and analyzing Map Reduce Jobs and Applications developed standalone and/or through Pig/Hive.
* Extensive experience in developing Pig Latin Scripts for transformations and using Hive Query Language for data analytics.In depth knowledge of Spark concepts and experience with Spark in Data Transformation and Processing.
* Design, implement, and maintain 3NF / dimensionaldatamodels for thedatawarehouse.
* Hands on experience ing on NoSQL databases including Hbase, Cassandra and its integration with Hadoop cluster.Experience in development and utilization of Apache SOLR with Data Computations and Transformation for use by Down Stream Online Applications.
* Good experience in ETL tool Informatica.Managing/maintaining the Hadoop cluster with the help of Apache Ambari 2.2.
* Expert in Ralph Kimball and Bill Inmon'sDataWarehousemethodologies.
* Solid experience in developing job flows and schedules with Oozie, and IBM Tivoli
* Experience in Hadoop administration activities such as installation and configuration of clusters using Apache, Cloudera and AWS.ing on Automation using Perl, Shell,Python and Scala Scripts.Developed UDFs in Java as and when necessary.
* Training and Knowledge in Mahout, Spark MLlib for use in data classification, regression analysis, recommendation engines and anomaly detection.Automating the jobs using Unix shell scripting and providing production support.
* Training and Knowledge in Mahout, Spark MLlib for use in data classification, regression analysis, recommendation engines and anomaly detection.
* Good experience in Python.Developed UDFs in Java as and when necessary to use in PIG and HIVE queries.Followed best practices for preparing and maintaining Apache Hadoop in production.
* ing knowledge of database such as Oracle 8i/9i/10g, Microsoft SQL Server, DB2, Netezza.
* Good experience in Oracle Business Intelligence Enterprise Edition(OBIEE)
* Experienced in using Version Control  like SubVersion, Git.Experience in development of logging standards and mechanism based on Log4J.Good understanding and experience with Software Development methodologies like Agile and Waterfall.
* Experienced in design, development, Unit testing,integration, debugging and implementation and production support, client interaction and understanding business application, business data flow and data relations from them.
* ed on migration project from Oracle DB to Hadoop environment thus enhancing the business to next level.

 Experience:
Panacea IT, FL (Remote)                                                                         
Mar 17  Present
Lead  Hadoop Developer/ Data Scientist 	

Responsibilities:
* ed as a Sr. Big Data/Hadoop Developer with Hadoop Ecosystems components.
* Developed Big Data solutions focused on pattern matching and predictive modeling.
* ed on analyzing Hadoop cluster and different big data analytic  including Pig, HBase database and Sqoop.
* Involved in Agile methodologies, daily scrum meetings, spring planning.
* Primarily involved in Data Migration process using Azure by integrating with GitHub repository and Jenkins.
* Migrating Hive & MapReduce jobs to EMR and Qubole with Automating the flows using Airflow.
* Used Kibana, which is an open source based browser analytics and search dashboard for Elastic Search.
* I was an order picker forUDF. I ed in a freezer and picked orders.
* ed on Oozie, Airflow & SWF for complex flows. Familiarity with HUE.
* Used Java Persistence API (JPA) frame for object relational mapping which is based on POJO Classes.
* Various tasks to supportdatawarehouseoperations includingdatamodeling, referencedatamanagement, resolving userdatatrouble tickets, and automation scripting.
* Upgraded the Hadoop Cluster from CDH3 to CDH4, setting up High Availability Cluster and integrating Hive with existing applications.
* Designed & Developed a Flattened View (Merge and Flattened dataset) de-normalizing several Datasets in Hive/HDFS.
* ed on NoSQL support enterprise production and loading data into HBase using Impala and Sqoop.
* Performed multiple MapReduce jobs in Pig and Hive for data cleaning and pre-processing.
* Build Hadoop solutions for big data problems using MR1 and MR2 in YARN.
* Handled importing of data from various data sources, performed transformations using Hive, Pig, and loaded data into HDFS.
* Involved in identifying job dependencies to design flow for Oozie& Yarn resource management.
* Designed solution for various system components using Microsoft Azure.
* Exploring with Spark to improve the performance and optimization of the existing algorithms in Hadoop using Spark context, Spark-SQL, Data Frame, pair RDD's.
* Created Hive Tables, loaded claims data from Oracle using Sqoop and loaded the processed data into target database.
* Exported data from HDFS to RDBMS via Sqoop for Business Intelligence, visualization and user report generation.
* Developed Nifi flows dealing with various kinds of data formats such as XML, JSON and Avro.
* Developed and designed data integration and migration solutions in Azure.
* ed on Proof of concept with Spark with Scala and Kafka.
* ed on visualizing the aggregated datasets in Tableau.
* ed on importing data from HDFS to MYSQL database and vice-versa using Sqoop.
* Implemented MapReduce jobs in Hive by querying the available data.
* Configured Hive Meta store with MySQL, which stores the metadata for Hive tables.
* Performed data analytics in Hive and then exported those metrics back to Oracle Database using Sqoop.
* Performance tuning of Hive queries, MapReduce programs for different applications.
* Proactively involved in ongoing maintenance, support and improvements in Hadoop cluster.
* Developed Spark code using Scala and Spark-SQL/Streaming for faster testing and processing of data.
* Used Cloudera Manager for installation and management of Hadoop Cluster.
* Developed data pipeline using Flume, Sqoop, Pig and Java MapReduce to ingest customer behavioral data and financial histories into HDFS for analysis.
* ed on MongoDB, HBase databases which differ from classic relational databases
* Involved in converting HiveQL into Spark transformations using Spark RDD and through Scala programming.
* Integrated Kafka-Spark streaming for high efficiency throughput and reliability
* ed on Apache Flume for collecting and aggregating huge amount of log data and stored it on HDFS for doing further analysis.
* ed in tuning Hive & Pig to improve performance and solved performance issues in both scripts.
Environment: Hadoop 3.0, Agile, Pig 0.17, Hbase 1.4.3, Jenkins 2.12, NoSQL, Sqoop 1.4, Impala 3.0.0, Hive 2.3, MapReduce, YARN, Oozie, Microsoft Azure, Nifi, Avro, MYSQL, Kafka, Scala 2.12, Spark, Apache Flume 1.8

Businessneeds Inc. (Remote)
Sr. Big Data/Hadoop Developer / Data Scientist 
Aug 16 - Feb 17

Responsibilities:
* As a Big Data/Hadoop Developer ed on Hadoop eco-systems including Hive, MongoDB, Zookeeper, Spark Streaming with MapR distribution.
* Developed multiple MapReduce jobs in Java for data cleaning and preprocessing.
* Involved in various phases of development analyzed and developed the system going through Agile Scrum methodology.
* ed on MongoDB by using CRUD (Create, Read, Update and Delete), Indexing, Replication and Sharding features.
* Involved in designing the row key in HBase to store Text and JSON as key values in HBase table and designed row key in such a way to get/scan it in a sorted order.
* Developed MapReduce (YARN) jobs for cleaning, accessing and validating the data.
* Created and ed Sqoop jobs with incremental load to populate Hive External tables.
* Implemented usage of Amazon EMR for processing Big Data across Hadoop Cluster of virtual servers on Amazon Elastic Compute Cloud (EC2) and Amazon Simple Storage Service (S3)
* ed with Apache Nifi to Develop Custom Processors for the purpose of processing and disturbing data among cloud systems.
* Involved in development of Hadoop System and improving multi-node Hadoop Cluster performance.
* Responsible for developing data pipeline with Amazon AWS to extract the data from weblogs and store in MongoDB.
* Creating Hive tables and ing on them using Hive QL.
* Designed and Implemented Partitioning (Static, Dynamic) Buckets in HIVE.
* Developed multiple POCs using PySpark and deployed on the YARN cluster, compared the performance of Spark, with Hive
* Used Hive to analyze the partitioned and bucketed data and compute various metrics for reporting.
* Used Spark to create the structured data from large amount of unstructured data from various sources.
* Used Apache Spark on Yarn to have fast large scale data processing and to increase performance.
* Responsible for design & development of Spark SQL Scripts using Scala/Java based on Functional Specifications.
* ed on Cluster co-ordination services through Zookeeper.
* Monitored load, job performance and capacity planning using Cloudera Manager.
* Involved in build applications using Maven and integrated with CI servers like Jenkins to build jobs.
* Developed Python scripts to find vulnerabilities with SQL Queries by doing SQL injection.
Environment: Hive 2.3, Zookeeper, Hadoop 3.0, MapReduce, MongoDB, MapReduce, Java, Apache Nifi 1.6, XML, JSON, MySQL, Apache Spark 2.3, Yarn, Scala, Kafka 1.1


Sears hoffman texas
Sr. Bigdata/Hadoop Developer								
Apr 14  Jul 16

Responsibilities:
* ed on Streamline analytics and data consolidation projects on the product Lucids Fusion.
* Integrated Kafka, Spark, Scala and HBasefor streamline analytics for creating a predictive model and implemented Machine Learning protocol.
* Automated the complex flows using the Airflow flow handler.
* Createdparent/child packages for ETL using SSIS, designed patterns using variables, package containers,for loading intoDataWarehouse
* Developed Scala scripts, UDFFs using both Data frames in Spark for Data Aggregation, queries and writing data back into OLTP system through Sqoop.
* Used Solr/Lucene for indexing and querying the JSON formatted data.
* Handled cloud operations inside Rackspacefor persistence logic.
* Monitored OOTB requests with ATG, Akamai and TIBCO.
* Used REST services for handling unfinished jobs, knowing the status and creating a dataset inside a URL.
* ed on ingesting, reconciling, compacting and purging base table and incremental table data using Hive and HBaseand job scheduling through Oozie.
* Designed and implemented streaming data on UI with Scala.js
* Utilized DevOps principle components to ensure operational excellence before deploying in production.
* Operating the cluster on AWS by using EC2, Akka, EMR, S3 and cloudwatch.
* Transported data to HBase using Flume.
* Used Java UDFs for performance tuning in Hive and Pig by manually driving the MR part.
* Used Java APIs such as machine learning library functions, graph algorithms for training and predicting the linear model in spark streaming.
* Have implemented unit testing in Java for pig and hive applications.
* Developed flow in Oozie to automate the tasks of loading the data into HDFS and pre-processing with Pig.
* Involved in source system analysis, data analysis, and data modeling to ETL (Extract, Transform and Load).
* Written Spark programs to model data for extraction, transformation and aggregation from multiple file formats including XML, JSON, CSV& other compressed file formats.
* ed on loading the RDBMS data onto Hadoop/HDFS, using sqoop.
* Developing the Pig scripts / UDFs to manipulate/transform the loaded data.
* Creating Hive tables, loading data into Hive using UDFs, partioning and bucketing principles.
* Migration Hive data into Hbase.
* Involved in integrations among Pig, Hive and Hbase.
* Instrumental in debugging. Created Hive External tables to store the processed data from Map Reduce
Environment: Hadoop0.20, HDFS, Hive,Pig, Sqoop, Java, Cloudera Distribution for Hadoop3(cdh3u5), Linux.

Citi Bank - New York, NY
Big Data  Expert / Hadoop Developer							
Sep 11  Mar 14

Responsibilities:
* Hands on experience in loading data from UNIX file system and Teradata to HDFS
* Experienced on loading and transforming of large sets of structured, semi structured and unstructured data from HBase through Sqoop and placed in HDFS for further processing.
* Installed and configured Flume, Hive, Pig, Sqoop and Oozie on the Hadoop cluster.
* Involved in creating Hive tables, loading data and running hive queries in those data.
* Extensive ing knowledge of partitioned table, UDFs, performance tuning, compression-related properties, thrift server in Hive.
* Built the EducationDataWarehouse, applications andAdhoc Reports for schools within the Charter Net
* Involved in writing optimized Pig Script along with involved in developing and testing Pig Latin Scripts
* ing knowledge in writing Pig's Load and Store functions.
* Developed Java MapReduce programs on log data to transform into structured way to find user location, age group, spending time.
* Developed optimal strategies for distributing the web log data over the cluster, importing and exporting the stored web log data into HDFS and Hive using Scoop.
* Collected and aggregated large amounts of web log data from different sources such as webservers, mobile and net devices using Apache Flume and stored the data into HDFS for analysis
* Monitored multiple Hadoop clusters environments using Ganglia.
* Developed PIG scripts for the analysis of semi structured data.
* Developed and involved in the industry specific UDF (user defined functions).
* Used Flume to collect, aggregate, and store the web log data from different sources like web servers, mobile and net devices and pushed to HDFS.
* Analyzed the web log data using the HiveQL to extract number of unique visitors per day, page views, visit duration, most purchased product on website.
* Integrated Oozie with the rest of the Hadoop stack supporting several types of Hadoop jobs out of the box (such as Map-Reduce, Pig, Hive, and Sqoop) as well as system specific jobs (such as Java programs and shell scripts).
* Monitored load, job performance and capacity planning using Cloudera Manager.
* Managing and scheduling Jobs on a Hadoop cluster using Oozie.
Environment: Amazon EC2, Apache Hadoop 1.0.1, MapReduce, HDFS, CentOS 6.4, HBase, Kafka, Scala, Elastic Search, Hive, Pig, Oozie, Flume, Java (jdk 1.6), Eclipse, Sqoop, Ganglia, Hbase.

Fidelity Investments Bangalore, IN								
Big Data/Hadoop Developer								
Jul 09  Aug 11

Responsibilities:
* ed on analyzing Hadoop cluster and different big data analytic  including MapReduce, Hive and Spark.
* Involved in loading data from LINUX file system, servers, Java web services using Kafka Producers, partitions.
* Implemented KafkaCustom encoders for custom input format to load data into Kafka Partitions.
* Migrated complex map reduce programs into SparkRDD transformations, actions.
* Implemented SparkRDD transformations to map business analysis and apply actions on top of transformations.
* Automated all the jobs from pulling data from Storage to loading data into MySQL using ShellScripts
* Automated all the jobs starting from pulling the Data from different Data Sources like MySOL and pushing the result dataset to Hadoop Distributed File System and running MR jobs and PIG/Hive using Kettle and Oozie( Flow management).
* Rendered and delivered reports in desired formats by using reporting  such as Tableau.
* ed on syncing OracleRDBMS to Hadoop DB (HBase) while retaining oracle as the main data store.
* Developed shell scripts and made the process automatic to drive the process from JSON to BSON.
* Used Kafka to stream the data with twitter4j from source to Hadoop.
* Offline Analysis was performed on HDFS and sent the results to MongoDB databases to update the information on the existing table, From Hadoop to MongoDB move was done using Mapreduce, Hive/ Pigscripts by connecting with Mongo-Hadoop connectors.
* Developed the MapReduce programs to parse the raw data and store the pre Aggregated data in the portioned tables.
* Created partitioned tables in Hive, mentored analyst and test team for writing Hive Queries.
* Developed PigLatin scripts to extract the data from the web server output files to load into HDFS.
* Involved in Installing, Configuring Hadoop EcoSystem, and Cloudera Manager using CDH4 Distribution.
* Automation of all the jobs starting from pulling the Data from different Data Sources like MySQL and pushing the result dataset to Hadoop Distributed File System and runningMR, PIG, and Hive jobs using Kettle and Oozie (Flowmanagement)
* ed with NoSQL databases like HBase in creating tables to load large sets of semi structureddata coming from various sources.
* Performed ETL process with Python-SQL Server pipelines/frame to perform data analytics and visualization in Python, NumPy, SciPy, Pandas, and MATLAB stack.
* ed with moving tables from Teradata to Hadoop using Sqoop.
* Handled importing of data from various data sources, performed transformations using Hive, MapReduce, loaded data into HDFS and extracted the data from MySQL into HDFS using Sqoop
* ed with application teams to install operating system, Hadoop updates, patches, version upgrades as required
Environment: Hadoop, HBase, HDFS, Map Reduce, Teradata, SQL, Cloudera, Ganglia, Pig Latin, Sqoop, Hive, pig, MySQL, Oozie, Flume, Informatica, Zookeeper , R, and Python.


Education: Bachelor of engineering, Computer Science, Gujrat Technical University, 2006",Data Scientist,emailed,indeed job post,resume
"

* Senior Data Scientist with 10+ years of SAS & SQL Programming expertise, Statistical Analysis, Data Modeling, Risk Analysis and reporting experience in various SAS  emphasizing on analysis, design, development, testing and implementation of various  in Financial, Health care, and Pet care industries.
* Experience ing remotely for clients such as Mastercard (100% remote), NBC Universal (100% remote), Nestle Purina (50% remote), CareSource (50% remote) and JPMorgan Chase (75% remote).
* Experience using SAS in the analysis and reporting of clinical trials data in the pharmaceutical industry 
* Experience in direct marketing processes, List Processing and marketing analytical methods.
* Expertise in creating Packages using SQL Server Integration Services (SSIS).
* Experienced in Database optimization and developing stored procedures, Triggers, Cursors, Joins, Views, Cursors and SQL on databases: MySQL, Oracle12g, OMWB tool.
* Experience in HEDIS Analytics and Reports, as well as, Gaps in Care reports and Ad-hoc reporting requests.
* Experience with Statistical Analysis & Data Modeling using JMP, MINITAB, Design Expert and Weka.
* Expertise in Transact-SQL (DDL, DML, DCL) and in Design and Normalization of the database tables.
* Experience in implementing business logic using Triggers, Indexes, Views and Stored procedures.
* Extensive experience in SAS BASE, SAS MACROS, SAS SQL, SAS/STAT, SAS/GRAPH, SAS Enterprise Miner, SAS Forecast Studio, SAS BI : SAS Enterprise Guide, SAS DI Studio, SAS Web Report Studio.
* ing knowledge of SAS High Performance Analytics (HPA) and SAS High Performance Forecasting (HPF) including PROC HP, PROC HPDMDB, PROC HPSAMPLE, PROC HPCORR and PROC HPREG.
* Hands on experience in SAS Programming, Merging SAS Data Sets, Developing SAS procedures, Macros, Preparing Data, Producing Reports, SAS Formats, SAS Functions, SAS Informats and managing dataset.
* Extensively ed on concatenating, interleaving, and merging large SAS datasets.
* Extensive industry analytics experience using statistical analysis and predictive modeling.
* Predictive modeling experience in linear and logistic regression analysis.
* Had experience in data modeling using Erwin, Star Schema Modeling, and Snowflake modeling, FACT and Dimensions tables, physical and logical modeling.
* Strong experience in Data Warehousing and ETL using Informatica Power Center and Salesforce systems.
* Experience in PROC SQL joins and PROC SQL set operators to combine tables horizontally and vertically.
* Experience in Integration of various Data Sources like MS-Access, Flat and CSV Files.
* Good experience in UNIX. ed with batch files and ran SAS programs using UNIX shell scripts. 
* Extensive experience in deployment of SAS programs/UNIX shell scripts and version controlling (PVCS)
* Used ODS to display outputs in HTML, CSV or other file formats.
* Extensive experience in writing reusable macros.
* Strong experience in testing methodologies. Involved in System Testing, Integration Testing & UAT.
* Excellent problem-solving  and having good knowledge in Finance and Credit Cards Business Domain.

 :
SAS 	SAS Base, SAS Macro, SAS SQL, SAS STAT, SAS Enterprise Miner, SAS HPA, SAS Access, SAS Graph, SAS Enterprise Business Intelligence, SAS HPF, SAS ECM, SAS Connect, SAS Enterprise Guide, SAS Data Integration Studio, SAS Web Report Studio, SAS Management Console, SAS AML, SAS ODS, SAS JMP
Statistical Software Packages	JMP, Design Expert, MINITAB, SPSS, R and Weka
Programming Languages	SQL, PL/SQL, HTML, CSS, PHP, AB INITIO, Visual Basic, Hive, Pig, Python, Java
Databases	Oracle, Teradata, SQL-Server, HADOOP, Sybase and DB2
Software Packages	MS Office, SSRS, SSMS, Tableau, DOMO, Informatica, UML, JIRA, MS Project
Operating Systems	Windows, UNIX, Salesforce application systems and Mainframe
 & Certification
Master of Science in Industrial Engineering & Statistics
SAS Certified Base Programmer V9 from SAS Institute.
Graduate Certificate in Statistics

Experience
Lead Data Scientist / SAS ECM AML Consultant				            		Oct 17  Present
Wells Fargo Bank (100% - Remote)
Environment: SAS Enterprise Guide, SAS ECM/AML, R, UNIX, SAS Macro, HADOOP, Machine Learning

Responsibilities:
* Performed advanced statistical analysis (univariate and multivariate analysis of variance, cluster and path analysis, principle component and factor analysis, analysis of covariance, survival & longitudinal analysis, logistic and linear regression modeling), created customized reports and presentation quality data  tables.
* Involved in the development of a net present value (NPV) model that was used to optimize the selection of prospects using SAS BASE, SAS/Enterprise Miner and SAS Enterprise Guide.
* Developed logistic regression models to predict subscription response rate based on customers variables like past transactions, response to prior mailings, promotions, demographics
* Utilized SAS management console for adding new user groups, LOBs for dashboards in SAS ECM portal.
* Statistically analyzed data using SAS and built Regression models for validation.
* Performed Multi-Variate Statistical Risk analysis, Eigen system Analysis, Canonical Correlation Analysis and logistic regression Analysis using PROC CANCORR, PROC MIXED, PROC MEANS and PROC UNIVARIATE.
* Used R statistical software for effective analysis by hypothesis testing to validate data and interpretations.
* Performed configuration and support of SAS ECM Metadata (Cases, Incidents, Parties, and Financial Items), configuration and support of ECM reference tables (All Lookup tables).
* Configured setting up User Accounts, Groups, Roles together with SAS Platform Administrator.
* Performed migration of UI Definitions, custom properties, flows, Scripts for UDF and Case Configurations.
* Developed Multiple Linear Regression model to determine how the factors like stock price, dividends, earnings, CPI and Interest rate affect.
* Implemented solutions for ingesting data from various sources and processing the Data-at-Rest utilizing Big Data  such as Hadoop, Map Reduce Frames, HBase, Hive.
* Implementation of Big Data ecosystem (Hive, Impala, Sqoop, Flume, Spark, Lambda) with Cloud Architecture.
* Designed and deployed full SDLC of AWS Hadoop cluster based on client's business need.
* Defined the process to build predictive model,  functions, variable selection and preparation and the statistical methodology and model validation.
* Developed and implemented logistic regression modeling for direct mail campaign by targeting responsive customers and minimizing risk.
* Implemented model validation and developed diagnostic tables and graphs that demonstrated how model can be used to improve the efficiency of the selection process for customer acquisition campaign.
* Verified program logic by overseeing the preparation of test data, testing and debugging of SAS programs.

Senior Data Scientist									             Aug 17  Oct 17
Mastercard, Purchase, NY (100% - Remote) 
Environment: Machine learning, Sqoop, SAS EG, SAS Forecast Studio, Python, DOMO, Tableau, R, Hive, Pig

Responsibilities:
* Developed SAS/Excel ad-hoc reports for Priceless management team.
* Generated comprehensive data files (MS Excel), identified trends, patterns and troubleshoot inconsistencies.
* ed in writing Hadoop Jobs for analyzing data using Hive, Pig accessing Text format files, sequence files, Parquet files.
* Performed data entry, cleansing and tagging (heavy volume of global assets)
* Generated DDL and DML scripts using Python and managed aspects of QA testing.
* Performed multiple regression modelling for load forecasting, and sequentially tested additional variables and combinations for model improvement using R programming.
* Developed predictive models and statistical analysis for external clients and internal business performance.
* Performed data acquisition, data mining, and analyzed data using valid analytical methods.
* Communicated findings using Tableau dashboard visualizations and results in a concise and actionable manner and ensure results are well understood by project sponsors.
* Experienced with batch processing of data sources using Apache Spark.
* Developing predictive analytic using Apache Spark Scala APIs.
* Involved in ing of big data analysis using Pig and User defined functions (UDF).
* Created Hive External tables and loaded the data into tables and query data using HQL.
* Used Sqoop to efficiently transfer data between databases and HDFS and used Flume to stream the log data from servers.
* Designed, developed, maintained and evaluated statistical and predictive models to identify historical trends and forecast future patterns and other performance measures using SAS.
* Accessed and compiled large amounts of data, and applied advanced statistical techniques to analyze the data, forecast, interpret, and quantify trends on various aspects of information. 
* Responsible for statistical data gathering, sophisticated analysis, and development of recommendations.
* Performed advanced quantitative assessments of all aspects of stress test models including theoretical aspects, model design and implementation, data integrity, and reliability.
* Supported revenue management using statistical and quantitative analysis, developed several statistical approaches and optimization models to save more than half million $ in program cost and by price distribution.
* ed with various business units to scope and design the statistical frame for business experiments and socialize and help integrate results of analysis of experiments.
* Diagnosed patterns and relationships in business data and developing logical methodologies to answer complex business questions through analytics.

Data Scientist											    Jul 16  Jul 17
NBC Universal, NYC (100% - Remote) 
Project: SAS Forecast Models Generation for NBCU Nets     
Environment: SAS Enterprise Guide, SAS Forecast Studio, MySQL, Informatica Power Center, SQL Server, SAS Time Series Studio, MS Excel, Access, DOMO, Tableau, Spark, Hadoop, AWS

Responsibilities:
* Define and create SAS/Excel ad-hoc reports for NBC nets.
* Used the PL/SQL procedures for Informatica mappings for truncating the data in target tables at run time.
* Performed data cleansing and validation on ing with several millions of records and data points.
* Statistical validation of the forecast results using error estimation techniques.
* Developed SQL Server Stored Procedures, Tuned SQL Queries (using Indexes and Execution Plan).
* Installed and Configured MS Build Server, created build agents and Build Controllers.
* ed with AWS to implement the client-side encryption as Dynamo DB does not support at rest encryption. 
* Exploring with the Spark for improving the performance and optimization of the existing algorithms in Hadoop using Spark Context, Spark-SQL, Data Frame, Pair RDD's, Spark YARN.
* Performance tuning daily for preventing issues & providing capacity planning using MySQL Enterprise Monitor.
* Developed DOMO dashboards with customer data trends and plots for visual analytics.
* Used Informatica Power Center 8.6 for extraction, transformation and load (ETL) of data in data warehouse.
* Developed stored procedures, triggers in MySQL for lowering traffic between servers & clients.
* I've performed correlation analysis using PROC HPCORR and generated pairwise Pearson correlation statistics and developed statistical predictive data models to predict the net impressions based on the past data provide business with forecast insights.
* Developed Tableau data visualization using Cross tabs, Heat maps, Box and Whisker charts, Scatter Plots, Geographic Map, Pie Charts and Bar Charts and Density Chart.
* Prepare detailed statistical reports in MS Access and MS Excel to track progress of weekly, monthly and quarterly customers and sales growth.
* Performance Tuning in SQL Server using SQL Profiler and Data Loading.
* ed with High Performance procedures like PROC HPSAMPLE, PROC HPREG, PROC HPF and HPBIN.
* Developed forecast models from scratch with SAS Forecast Studio for NBCs various nets E!, BRAVO, OXYG, SPROUT, USA, CHILLER, NBCSN, SYFY and MSNBC
* Developed ARIMA, Exponential Smoothing models (ESM), Intermittent Demand Models (IDM) and fine-tuned them by identifying outliers, creating pulse or level shift events and automated them.
* Performed data manipulations for data quality assurance/validation of table, listing and summaries
* Developed SAS programs for querying data from large datasets using SAS Enterprise Guide
* Summarized the datasets to generate daypart models with quarterly and monthly forecast estimates.

Data Scientist / SAS Data Integration Consultant					Feb 14  Jun 16
Nestle Purina, St Louis, MO (50% - Remote)							
Project: Demand Planning Migration to SAS module Demand driven Planning & Optimization (DDPO)
Environment: SAS Data Integration Studio, SAS Enterprise Guide, Informatica Power Center, SAS Enterprise Miner, SAS Macros, SAS ODS, UNIX, SAS Forecast Studio, SAS Forecast Server, Azure

Responsibilities:
* Primarily responsible for extracting, transforming and data loading (ETL) into SAS datasets and creating flat files, reports and excel files.
* Developed data structures, performed data manipulation and application development.
* Designing, developing and implementing the DI Studio job flows to load the data into SAS.
* Performed Double and Seasonal Exponential Smoothing using Time Series Exponential Smoothing (TS ESM) node, and data mining using SAS Enterprise Miner. 
* Utilized MS Azure services with focus on big Data Engineer /analytics / enterprise data warehouse and business intelligence solutions to ensure optimal architecture, scalability, flexibility, availability, performance, and to provide meaningful and valuable information for better decision-making.
* Gathering data requirements, designing and developing data marts using SAS Data Integration Studio.
* Generated Baseline and promoted demand planning forecasts using SAS Forecast Studio.
* Produced use cases and data flow diagrams as supporting documentation using MS Visio.
* Assisted conceptual, logical, and physical data modeling, and interface effectively with data modelers
* Involved in deployments, automation, scheduling of DI jobs and process documentation. 
* Creating Stored Processes to provide users of reporting DataMart easy access using SAS Enterprise Guide.
* Integrating and managing the data mappings from the clients data sources (mainly Oracle) to the SAS Analytical data mart using SAS DI Studio.
* Supported Deployment to all testing and production environments & automated test cases using MS Build.
* Designed flows with many sessions with decision, assignment task, event wait, and event raise tasks, used Informatica scheduler to schedule jobs. 
* Documented and setup files, SAS program, and log files for new production box.  
* Oversaw production jobs, production server, running daily/quarterly process jobs to support the business in reporting capability and day to day functions. 
* ed and collaborated with project team members and other relevant knowledge experts.
* Performing analytical and programming activities including analysis, design, development, unit & integration testing, implementation, and documentation of integrating the data management solutions. 

Data Scientist											Oct 12  Jan 14
CareSource, Dayton, OH (50% - Remote)
Environment: SAS Base/Macros, SAS Enterprise Guide, SQL Server, SAS ODS, Mainframe, R

Responsibilities:
* Develop complex automated reporting products using BASE SAS that meet the internal and external information needs of the organization. 
* Automated the manual exporting of data to complex Excel layouts and customized using SAS ODS Tagsets -EXCELXP utility.
* Assist in the design of Business Intelligence products for both internal and external use. Participate in testing activities as needed.
* Responsible for adhering to proper coding standards and IT change control process.
* Evaluate HEDIS requirements, obtain data for reporting and prepare HEDIS reports.
* Generate graphics that effectively describe, explore and summarize information for communication to appropriate parties.
* Used R to help with Turn-around Time Distribution Analysis and Forecast on shipped and cancelled referrals, and visualized the result using R Shiny.
* Used Machine Leaning techniques (e.g. Logistic Regression, Bayesian Linear regression) to build up predictive models on probability of cancellation of each new referral.
* Writing SAS and SQL code for querying data from large datasets of the SQL Server Database.
* Incorporate critical thinking  and judgment in the process to determine best course of action for each inquiry/problem. 
* Generated batch files for regulatory reporting via e-filing using SAS Enterprise Case Management (SAS ECM).
* Investigation of data that shows a pattern or indicates a significant variation from expected.
* Represent the department in project meetings and other meetings that require subject matter knowledge.
* Managing HEDIS and CAHPS reporting  for multiple markets with year-round monitoring and annual required reporting and submission using Data Transformation Services (DTS).
* Assist the manager in directing and coordinating  within the department.

Statistical Analyst / SAS Programmer								Aug 11  Aug 12
JP Morgan Chase Bank, Columbus, OH (75% remote)					            
Project: Consumer Model Governance - Score Monitoring System 
Environment: SAS Base/Macros, Oracle, SQL, DB2, Teradata, UNIX, Mainframe, SAS/EG, SAS ECM, SAS AML

Responsibilities:
* Writing SAS & SQL code for querying data from large datasets and extracted data from Oracle DB
* Involved in campaign auditing and generating water fall reports for management approvals 
* Used Dynamic LIBNAME, PROC SQL, PROC SORT, PROC EXPORT, PROC COMPARE, PROC IMPORT, PROC APPEND, PROC REPORT AND PROC FORMAT
* Performed basic SAS analysis on existing customers data, and database segmentation using PROC MEANS, FREQ, PROC  AND TABULATE.
* Performed configuration and support of SAS ECM Metadata (Cases, Incidents, Parties, Financial Items), configuration and support of ECM reference tables (All Lookup tables).
* Imported existing records and historical information by custom ETL to prepopulate customer fields and prevent rekeying errors using SAS ECM.
* Developed forecasts for Cross sectional time series data using forecasting node in SAS Enterprise Miner.
* Design, development and implementation of a new model validation and tracking process
* Support of Business Intelligence validation reports used by other risk groups.
* Visually created customer segments for behavior monitoring using SAS Anti-Money Laundering (SAS AML).
* Design, produce and implement capabilities to execute file processing (ETL) required for production of SAS data sets for modeling and analytics that include data from credit records
*  with model developers and the model implementation teams to develop logical data models and ensure efficient flows of information through automated business processes
*  with the model reviewers to review business needs and strategies and assess the implications to the information/data architecture
* Utilized SAS Enterprise Guide for Data Management and increased data consistency that enabled data-driven decisions and streamlines administration & ran ad-hoc SAS queries.
* Performed Data de-duplication on ing with several millions of records and data points.
* Performed Extraction, transformation & loading (ETL) & Data Cleansing with SAS Data Integration Studio.
* Scheduling daily, weekly and monthly reports using SAS/MACROS.
* Formatted data using SAS defined and user defined formats using PROC FORMAT.
* Designed file directory built out structure for both production and development areas, so production job could be run in a controlled environment by AUTOSYS with users also being able to develop, code, test, and run ad-hoc reports against production data.
* Managed AUTOSYS performance and re-runs if needed in SAS version 9.2 UNIX  
* Set up log check program to review logs of production jobs to look for errors/warning and sent out emails if problems were found, eliminating manual checking of program logs.
* Involved in development and enhancement of SAS programs.

Statistical Analyst										 Jan 11  Jul 11
Office of Statewide Health Planning & Development (OSHPD), Sacramento, CA (75% - Remote)
Environment: SQL Server, SAS Base/EBI/EG/OLAP Cube Studio, SAS Management Console, Oracle, R

Responsibilities:
* Involved in requirement gathering from different groups by conducting JAD sessions. 
* Conducted regression analysis on impact of Medicaid expansion on population of Patient Assistance Program (PAP).
* Developed SAS programs for querying data from large datasets using SAS Enterprise Guide
* Extracting data from SQL Server, Oracle and created tables & populated them to centralize comprehensive data using SQL Server Management Studio
* Provided forecast analysis on monthly basis to operation manager on patients volume, referral volume to help with labor planning.
* Used multiple statistical methodologies with evaluation the impact of a nurse intervention project over time. (Propensity Score Matching Technique to generate control and treatment group population and did survival analysis and Cox Regression analysis).
* Involved in Design of  Requirements Documents and implementation plan of the Clearinghouse
* Used PROC SQL, PROC SORT, PROC EXPORT, PROC COMPARE, PROC MEANS, PROC FREQ, PROC TABULATE and PROC FORMAT in developing SAS programs.
* Performed basic SAS analysis on existing customers data using PROC MEANS
* Evaluate HEDIS requirements by monitoring and reporting on key performance indicators on a weekly and monthly basis, identifying trends, issues and potential solutions to those issues.
* Performed Extraction, transformation & loading (ETL) & Data Cleansing with SAS Data Integration Studio.
* Set up Log-Check and error-check program to review logs/error of production and development jobs.
* Created intermediate data marts for storage and loaded with the validated data.
* Build cubes using SAS OLAP Cube Studio & summarized datasets to generate region level reports.
* Involved in development and enhancement of SAS programs and formatted data using SAS defined and user defined formats using PROC FORMAT.
SAS Data Analyst									             Jun 10  Dec 10
Fannie Mae, Herndon, VA (50% remote)
Environment: SAS Base/Macros, SAS Forecast Studio, Oracle, UNIX, SQL, Ab Initio, Visio

Responsibilities:
* Involved in requirement gathering from different groups 
* Writing SAS and SQL code for querying data from large datasets of the SQL Server & Teradata Database
* Extracting data from Oracle (Hemisphere Database) 
* Generating excel reports, Visio diagrams and delivering to different groups
* Involved in design and implementation plan and Unit Testing of the SDLC
* Used Dynamic LIBNAME, PROC SQL, PROC SORT, PROC EXPORT, PROC COMPARE, PROC MEANS, PROC FREQ, PROC TABULATE and PROC FORMAT
* Monitored production jobs, production server and running daily/quarterly/monthly process jobs.
* Developed large quantities of accurate monthly forecasts using SAS Forecast Studio.
* Involved in development and enhancement of SAS programs in Mainframe environment.
* Analyzed existing system logic (Ab Initio, SAS), and translating the logic into baseline functional requirements and managed the requirements using Rational Requisite Pro
* ed with business unit or other subject matter experts, in addition to IT staff to refine BRD and FSD.
* Communicating clearly and precisely in functional (vs. ) terms 
* Produced use cases and data flow diagrams as supporting documentation using MS Visio.
* Performed data step manipulations for data quality assurance/validation of table, listing and summaries
* Formatted data using SAS defined and user defined formats using PROC Format.

SAS Programmer / Analyst								            Aug 08  May 10
Comerica Bank, Phoenix, AZ (50% remote)
Project: Campaign List processing/auditing 
Environment: SAS Base/Macros, Oracle, Teradata, UNIX, Mainframe, SAS/EG SAS/EM, SQL, SAS AML

Responsibilities:
* Involved in requirement gathering from different groups, design, implementation plan and Unit Testing
* Writing SAS & SQL code for querying data from large datasets and extracted data from Oracle & Teradata
* Involved in campaign auditing and generating water fall reports for management approvals 
* Used Dynamic libname, PROC SQL, PROC SORT, PROC EXPORT, PROC COMPARE, PROC IMPORT, PROC APPEND, PROC REPORT and PROC FORMAT
* Performed basic SAS analysis on existing customers data, and database segmentation using PROC MEANS, FREQ,  and TABULATE.
* Compared & analyzed entitys current behavior to its historical behavior and behavior of peers using SAS AML.
* Build Logistic Regression models and Multiple Linear Regression models to identify the control group cohorts using SAS/STAT and other SAS modules.
* Statistical validation of the forecast results using error estimation techniques
* Administered SAS system, SQL Server, Teradata & Oracle Database system and managed the system requirements, user access management & administration on UNIX and Windows server.
* Developed forecasting models from scratch with SAS and used them for consumer prediction and control. 
* Utilized SAS Enterprise Guide for Data Management and increased data consistency that enabled data-driven decisions and streamlines administration & ran adhoc SAS queries.
* Developed Multiple Linear Regression model to determine how the factors like stock price, dividends, earnings, CPI and Interest rate affect Price-Earnings Ratio in stock market.
* Doing Statistical Analysis with statistical procedures and univariate procedures from Base SAS, SAS/STAT
* Performed Data de-duplication on ing with several millions of records and data points.
* Performed Design of Experiments & Analysis (DOE) and optimized the combination of Model inputs.
* Performed Multi-Variate Statistical Risk analysis, Eigen system Analysis, Canonical Correlation Analysis and logistic regression Analysis using PROC CANCORR, PROC MIXED, PROC MEANS and PROC UNIVARIATE 
* Performed Data Mining and collected intricate details using SAS Enterprise Miner & Weka.
* Performed Extraction, Transformation & Loading (ETL) & Data Cleansing with SAS Data Integration Studio.
* Involved in development and enhancement of SAS programs.
* Statistically analyzed data using MINITAB, R and S-Plus statistical software packages and built Regression models for validation.
* Scheduling daily, weekly and monthly reports using SAS/MACROS.


	 


",Data Scientist,emailed,indeed job post,resume
"Willing to relocate: Anywhere
 Experience

Data Scientist
Cincinnati Children's Hospital Medical Center - Cincinnati, OH
April 2018 to Present
 used: Python, Keras, TensorFlow, Rstudio, MATLAB 
 Developed a deep learning model based on U-Net to segment cells from the biomedical images. Usedthe model to segment HEp-2 cells from the specimen images and achieved an F-Score of 0.8752.  Developed an algorithm to automatically segment regions of interest (ROI's) from the ultrasound images of the liver using deep neural nets. 
 Developed a classifier which uses clinical data of the patient and statistical data extracted fromtexture features of liver ultrasound images to classify the liver as healthy and unhealthy. 
 Implemented Random forest and state of the art deep learning techniques to predict whether thegiven antibiotic would  on people and achieved an overall accuracy of 91% and 95% respectively.
Data Scientist
University of Cincinnati - Cincinnati, OH
January 2017 to April 2017
 used: R, Python, SQL, Tableau, ggplot2 
 Analyzed conversations between recruiters and students and developed a large variety of variablesto predict the time of response from a recruiter using XGBoost. 
 Constructed a Latent Dirichlet Allocation model to segment a conversation between recruiters andstudents into different topics. Used Word2Vec to find similar words that are present in a given topic for further analysis. 
 Extracted, compiled and interpreted student data which is stored in various spreadsheets usingstatistical and data visualization techniques to generate ongoing campus placement reports and developed forecasting models to analyze and extrapolate financial resources and budget related to various divisions in college of law. 
 ed on developing a complete database of BAR examination reports of different students overthe years.
Data Analyst
Angryengine  - Visakhapatnam, Andhra Pradesh
October 2015 to April 2016
 Performed data mining, data cleaning & explored data visualization techniques on a variety of datastored in spreadsheets and text files using R and plotting the same using ggplot2 function 
 Developed a binary classification model with 83% accuracy to accept or reject future cash advanceapplications using regression in R based on 8 parameters and 8,000 data points processed from MySQL database  Created visually impactful interactive dashboards in Excel and Tableau for data reporting by using pivot tables and VLOOKUP


Master of Science in Computer Science in Artificial Intelligence
University of Cincinnati
July 2019
Software Engineering
Gitam University May 2016


SQL, Excel, Business Intelligence
Links

",Data Scientist,emailed,indeed job post,resume
" 
 Looking for full time opportunities in Data Science or Data Analytics Domain. 
 2.5 years of experience in Java development and visualization and 8 months experience in Data Science and Machine Learning domain. 
 Expertise in Machine Learning, Fraud Detection, Deep Learning, Python, Java, SQL, Artificial Intelligence, Image and Text Processing.  
 

University of South Florida 	 
MS in Business Analytics and Information Systems GPA: 3.89/4 
                Tampa, FL Aug 2018 -  Expected Dec 2019 
University of Pune  	            Pune, India B. Tech in Electronics and Telecommunication GPA: 8.06/10      Aug 2011- May 2015 

	Programming Languages:- 	Python, R, SQL, MATLAB, JAVA, C, C# 	Database:- 	MS-SQL, Oracle, Cassandra 
	Data Mining :- 	WEKA, Microsoft Azure ML Studio, SAS 	Development :- 	Spyder, Eclipse, SQL Developer 
	Visualization :- 	Tableau, VISIO, Argo UML, Qlik 	 	Web :- 	HTML, CSS, Web Services, XML
 	Frame:- 	Keras, Tensor Flow, MVC 
DATA SCIENCE EXPERIENCE
 
Suncoast Credit Union                                                                                                                                                               Jan 2019  August 2019   
Data Science Intern 
 Building Statistical models to Automate the Car Loan process by predicting risky borrowers. Predicted default customers with 94% accuracy and 0.98 F1 Score. 
 Cost Based Analysis on highly skewed data to find the tradeoff between True Positive and False Positive values of Customer Default. 
 Developed a Genetic (Adaptive) algorithm for modeling User behavior which can be used for boosting a models forecasting accuracy. 
 PCA, Feature engineering using applied statics technique to determine the features influencing the loan application. 
 Implemented Hypothesis Testing, P value analysis, A/B testing, Bandit Testing, Cross Validation to find the best model for prediction. 
 Created end to end system to predict the individual probability of Loan going Delinquent, Possible Recovery of Loan and Loan Default. 
 Developed the Sun Logix Score that will aid in both Loan Decision Process and Portfolio Risk Analysis. 
 Designed Customer Segmentation and Loan Strategies using Sun Logix Score for profit maximization. 
 Similar analysis on Credit Card dataset to predict fraud, default and delinquent customers. 
 Models Used: Linear/Logistic Regression, XG Boost, Classification Algorithms, Deep Learning, Ensemble, ADASYN, SMOTE. 
 
Cognizant  Solutions                                                                                                                                                    Feb 2016  July 2018   
 Programmer Analyst 
 Achieved 45% reduction in time requirement by designing new tool for weekly tickets reporting. 
 Engineered complete website for Hospital Management System with Team Lead role. It included Java, Hibernate & MVC.  
 ed on HMH eCommerce domain with order processing, application development and code enhancement. 
 Data visualization with Tableau and Microsoft Visio on eCommerce Database. 
 Voluntary value adds like customer segmentation, predicting sales and customer buying pattern using machine learning. 
 

 
Loan Defaulter Analysis and Modeling 
 Analyzed data from lending club for predicting whether given variables and conditions a person would default loan or not. 
 Built a Cost Based matrix using Precision and Recall which will help business understand the Dollar value for actual profit after implementation of model. Models accuracy was 86%. 
 Models Used: Random Forest, Logistic Regression, PCA, SVM and XG Boost, Logit and Probit Modelling, Stacking Classifier. 
 
Sentiment Analysis on Bank of America  
 Extracted tweets from twitter using NLTK library of Python. Performed data cleaning by removing stop words, punctuations and stemming.  
 Converted data into TFIDF to determine most frequent words and classified the words in Positive and Negative Sentiments. 
 Developed interactive visualizations in Plotly and Matplotlib to determine the positive and negative tweets and the patterns of comments. 
 Models Used: Bayesian Classifiers, SVM, Decision Tree, KNN, Word Embedding, Glove. 
 
Image Classification on Yelp Dataset using Transfer Learning 
 Developed a model to classify the Food Images into different six food types using Transfer Leaning on Inception V3 model and CNN 
 Models Used: CNN, Auto Encoder, Transfer Learning, Machine Learning, Deep Learning. 
 
Employee Attrition Prediction on IBM Dataset  
 Created model to predict employee attrition on IBM Dataset, got 74% accuracy with ensemble of Neural Net and Decision Tree. 
 Feature selection done by Chi-Square, PCA via SAS Enterprise Miner.  
 Models Used: Neural Net, Decision Tree, MBR, KNN ",Data Scientist,emailed,indeed job post,resume
"Master of Science in Information Management, Syracuse University          GPA 3.89/4.0 	                           August 2019 Certification of Advanced Study in Data Science 
Awarded Merit-based Diversity Scholarship for exemplary academic and extra-curricular performance 
Peer note-taker for a visually impaired student; Contributor to InfoSpace, the official blog of iSchool 
Bachelor of , Charotar University of Science and    GPA 3.41/4.0 	                         May 2015 
 Experience 	  	         
Data Analyst and Business Intelligence intern, Covanta Energy, Morristown, New Jersey               January 2019  August 2019 
 Improved operational efficiency of waste management plants by analyzing data, building dashboards on Power B.I. and presenting actionable insights to help stakeholders take data-driven decisions 
 Performed prescriptive analytics to identify KPIs of waste management plants by writing Python scripts and implemented association rules machine learning algorithm to provide strategic direction and improved the plants capability 
 Collaborated with the team and stakeholders by holding daily SCRUM meetings, weekly meetings for status update and built automated notification system to reduce carbon emission and ensured federal compliance 
Analyst & Customer Relationship Manager, Interactive Manpower Solution, Ahmedabad 	May 2016  June 2017 
 Assisted and collaborated on data visualization using R and Salesforce to perform digital transformation for an international client to improve market segmentation and online customer engagement 
 Programed, trained and tested different classification models (neural nets, Bayesian classifier, support vector machine, decision tree) in R language, to get the most accurate algorithm, predict customer pain point, and decreased customer support response time by 21% 
Relational Database developer and Analyst, ShipMyBox, Ahmedabad 	May 2015  April 2016 
 Designed coded, tested and supported database solutions for ShipMyBox in Microsoft SQL server, setup the technological backbone and built a ing product from scratch 
 Wrote python scripts for the extraction and analysis 19,100 Indian zipcodes to determine the best shipping companies based on geography using extensive data science techniques 
Relevant Course 	 
 Data Science: Applied Data Science, Data Analytics, Big Data Analytics, Text Analytics, System Analysis and Design, Database Management 
 Management: Project Management, Enterprise Risk Management, Management Principles for Information s, Strategic Management of Information Resources, Information Policy 
Academic and Real-world  	         
Database Management 
 Designed and built a database management system for public bus transportation system by creating entity relationship diagram, normalizing the database to 3rd normal form and satisfied business rules 
 Wrote complex SQL queries using joins, grouping aggregation, nested subqueries to extract relevant data on Microsoft SQL server, answer business questions and improved customer retention by 24% 
 Designed user-interface along with backend database to improve data extraction, transformation and loading 
Applied Data Science 
 Built a bag of words model to analyze restaurant reviews and determined factors that drive customers to give negative feedback and helped restaurants in identifying areas of improvement 
 Extracted relevant data using SQL on nation-wide zipcodes and wrote python scripts for the extraction and analysis of open data available on Indian shipping industry Data Analytics/Machine Learning 
 Built robust machine learning customer churn models to compare the behavior of customers who churn and customers who 
dont churn, predict the customers who will churn in future based on their behavior to improve customer attrition 
 Deployed KNN algorithm and built a model to classify user response thereby significantly improved and optimized a social media marketing campaign 
	Publications 	 
 Santani, S., (2019). The Government vs Citizens: The Fight for Personal Data. InfoSpace The Official Blog of the iSchool, SU 
 Santani, S., (2018). How to Protect Your Privacy on Social Media. InfoSpace The Official Blog of the Syracuse University 
 Deshpande, N., Santani, S., (2015). The End Where I Begin. Blackbuck Publications 
	 	 
 Programming languages: SQL, R, Python (Numpy, Matplotlib, Pandas, Sckit learn) 
 Data Analysis & Visualization: Power BI, Tableau, R, Gephi, Qlik, D3 Library, Machine Learning, Statistical Analysis, D3 
 Applications: Hadoop, Apache spark, AdWords, MS Project, MS Office, Microsoft SQL Server, Salesforce, Visio, Access, ggmap in R, Advanced Excel with Pivot Tables 
Leadership / Involvement 	 
 
 On-boarding for IMS: Accelerated on-boarding for IMS by training new hires & mentored them in orientation 
 Peers note-taker: Volunteered for office of disability to assist a visually impaired peer for class note-taking at the iSchool, Syracuse University ",Data Scientist,emailed,indeed job post,resume
"* Above 8+ years of experience in large datasets of Structured and Unstructureddata,Data Visualization ,DataAcquisition, Predictive modeling,NLP/NLU/NLG/AI/machinelearning/Computer vision/Probabilistic Graphical Models/Inferential statistics/Graph,DataValidation.
* Hands on experience indata mining algorithms and approach.
* Good at algorithm and design techniques
* Expert level understandingin ApplicationDesign, Development and testing in MainframeenvironmentsusingPL/1,COBOL,EGL,Easytrieve,DB2, JCL, QC&VAG.
* Experience in developing different Statistical Machine Learning, Text Analytics,DataMining solutions to various business generating and problemsdatavisualizations using Python, R and Tableau.
* Expertise in transforming business requirements into building models, designing algorithms, developingdatamining and reporting solutions that scales across massive volume of unstructureddataand structured.
* Regressionanalysis,Statisticaltestanalysis,Reportand Dashboardgeneration, Datamanagement.
* Git,Java,MySQL,MongoDB,Neo4J,AngularJS,SPSS,Tableau.
* Python,Numpy, Scikit-Learn,genism,NLTK,Tensorflow,keras.
* Experience inMachineLearning, Statistics, Regression- Linear, Logistic, Poisson, Binomial.
* Experience in designing visualizations using Tableau software and Storyline on web and desktop platforms, publishing and presenting dashboards.
* Single handed built a model to replace the job of doer in the pension sector. This model (Patent under progress) generates experience from structured data and learns through a bootstrapping mechanism new experience from unseen data. 
* Single handed Built and designed a whole Information extraction botPOC for KYC extraction. This bot is using adaptivelearningtechniques and uses some custom supervised classifiers for entity and relation extraction
* Proficient in Machine Learning techniques (Decision Trees, Linear, Logistics, Random Forest, SVM, Bayesian, XG Boost, K-Nearest Neighbors) and Statistical Modeling in Forecasting/ Predictive Analytics, Segmentation methodologies, Regression based models, Hypothesis testing, Factor analysis/ PCA, Ensembles.
* Hands on experience in implementing LDA, Naive Bayes and skilled in Decision Trees, Random Forests, Linear and Logistic Regression, SVM, Clustering, neural nets and good knowledge on Recommender Systems.
* Develop Azure POC for Retailer Data Research tool. Develop and Data Factory Micro-service data movement from Azure SQL Server to Cosmos NoSQL Document database.
* Experience on advanced SASprogramming techniques, such as PROC APPEND, PROC DATASETS, and PROC TRANSPOSE.
* Highly skilled in using visualization  like Tableau, Matplotlib for creating dashboards.
* Extensive ing experience with Python including Scikit-learn, Pandas and Numpy.
* Developeddatavariation analysis anddatapair association analysis in the Bioinformatics field.
* Regularly accessing JIRA tool and other internal issue trackers for the Project development.
* Having good domain knowledge on Retail and Airlines.
* Experience in foundational machine learning models and concepts( Regression, boosting, GBM, NNs, HMMs, CRFs, MRFs, deep learning).
* Well experienced in Normalization&De-Normalization techniques for optimum performance in relational and dimensional database environments.
* Analyzeddatausing R, Perl, Hadoop and querieddatausing structured and unstructured databases
* Strong programming expertise  Python and strong in Database SQL.
* Integration Architect &DataScientistexperience in Analytics, BigData, SOA, ETL and Cloud .
* ed and extracteddatafrom various database sources like Oracle, SQL Server and Teradata.
* Skilled in System Analysis, DimensionalDataModeling, Database Design and implementing RDBMS specific features.
* Facilitated and helped translate complex quantitative methods into simplified solutions for users.
* Knowledge of ing with Proof of Concepts and gap analysis and gathered necessarydatafor analysis from different sources, prepareddatafordataexploration usingdatamunging.
* Solid coding and engineering  in Machine Learning
* Experience with file systems, server architectures, databases, SQL, and data movement (ETL).
* Proficient in Python, experience building, and product ionizing end-to-end systems
* Knowledge of Information Extraction, NLP algorithms coupled with Deep Learning

 

Bachelors of Engeneering.

 








Statistics/ML
Exploratory Data Analysis: Univariate/MultivariateOutlier detection, Missing value imputation, Histograms/Density estimation, EDA in Tableau 
Supervised Learning: Linear/Logistic Regression, Lasso, Ridge, Elastic Nets, Decision Trees, Ensemble Methods, Random Forests, Support Vector Machines, Gradient Boosting, XGB, Deep Neural Nets, Bayesian Learning
Unsupervised Learning: Principal Component Analysis, Association Rules, Factor Analysis, K-Means, Hierarchical Clustering, Gaussian Mixture Models, Market Basket Analysis, Collaborative Filtering and Low Rank Matrix Factorization
Feature Selection: Stepwise, Recursive Feature Elimination, Relative Importance, Filter Methods, Wrapper Methods and Embedded Methods
Statistical Tests: T Test, Chi-Square tests, Stationarity tests,Auto Correlation tests, Normality tests, Residual diagnostics, Partial dependence plots and Anova
Sampling Methods: Bootstrap sampling methods and Stratified sampling
Model Tuning/Selection: Cross Validation, Walk Forward Estimation, AIC/BIC Criterions, Grid Search and Regularization
Time Series: ARIMA, Holt winters, Exponential smoothing, Bayesian structural time series
Machine Learning /
Deep Learning
R:caret, glmnet, forecast, xgboost, rpart, survival, arules, sqldf, dplyr, nloptr, lpSolve, ggplot
Python:pandas, numpy, scikit-learn, scipy, statsmodels, matplotlib,tensorflow
SAS: Forecast server, SAS Procedures and Data Steps
Spark: MLlib, GraphX
SQL:Subqueries, joins, DDL/DML statements
Databases/ETL/Query
Teradata, SQL Server, Redshift, Postgres and Hadoop (MapReduce); SQL, Hive, Azure Data Factory ,Pig and Alteryx.
Visualization
Tableau, ggplot2 and RShiny
Prototyping
PowerPoint,RShiny and Tableau





Client: State of MA ,Boston,MA.				                                                                    Aug 2018- Till date
Role:Data Scientist

Description: Massachusetts, officially the Commonwealth of Massachusetts, is the most populous state in the New England region of the northeastern United States. It borders on the Atlantic Ocean to the east, the states of Connecticut and Rhode Island to the south, New Hampshire and Vermont to the north, and New York to the west.

Responsibilities:
* Involved in defining the source to target data mappings, business rules, and data definitions. 
* Performing data profiling on various source systems that are required for transferring data to ECH using 
* Defining the list codes and code conversions between the source systems and the data mart using Reference Data Management (RDM). 
* Involved in data collection and induction to Teradata
* Conducted data cleaning, data preparation, and outlier detection
* Finding insights from millions of customer chat and calls records
* Gathering requirements from business.
* Involved in creating pipelines that move, transform, and analyze data from a wide variety of sources using multiple methods like the Azure Power shell utility that allows control of many other Azure resources,Python,  a REST API, and the Azure Portal UI.
* Reviewing business requirements and analyzing data sources 
* Developed predictive models for sales and Finance teams using various ML and DL algorithms 
* Utilizing Informatica et (InformaticaData Explorer, and Informatica Data Quality) to analyze legacy data for Data Profiling. 
* Created a Handler function in Python using AWS Lambda that can invoke when the service is executed.
* Implemented statistical modeling with XGBoost machine learning software package using Python to determine the predicted probabilities of each model.
* ed on DTS Packages, DTS Import/Export for transferring data between SQL Server 2000 to 2005 
* Involved in upgrading DTS packages to SSIS packages (ETL). 
* Involved in Training and Testing the ML Supervised and Unsupervised models
* Researching on Deep Learning to implement NLP
* Presented to the higher management the discovered trends and analysis, forecast data, recommendations, model results and risks identified.
* Design and Develop Data Warehouse for Tax Information Factory using Oracle PL/SQL.
* Performing an end to end InformaticaETL Testing for these custom tables by writing complex SQL Queries on the source database and comparing the results against the target database. 
* Using HP Quality Center v 11 for defect tracking of issues.
* Involved in applying data mining techniques and optimization techniques in B2B and B2C industries and proficient in Machine Learning, Data/Text Mining, Statistical Analysis and Predictive Modeling.
* Created and presented executive dashboards to show the patterns & trends in the data using Tableau Desktop
* Developed NLP models for Topic extraction, Sentiment Analysis
* Developed Executive  KPI, Key value programs, NPI dashboards in Tableau
* Created customized Calculations, Conditions and Filters (Local, Global) for various analytical reports and dashboards.
* Experience in Analysis, Design, Development, Implementation, Testing and Support of Data Warehousing and Data Integration Solutions using Informatica Power center.
* Was able to identify emerging issues using the models
* Developing & evaluating Machine Learning models
* Developed different visualizations using advanced features and deep analytics in Tableau
* Used algorithms and programming to efficiently go through large datasets and apply treatments, filters, and conditions as needed 
* Developed Cross Tab, Chart, Funnel charts, Donut charts, Heat Maps, Tree Maps and Drill Through Reports, 100% stacked bar charts etc. in Tableau Desktop
* Involved in publishing, scheduling and subscriptions with Tableau Server andcreating and managing users, groups, sites in Tableau Server.
* Involved in developing and testing the SQL Scripts for report development, Tableau reports, Dashboards and handled the performance issues effectively
* Tested dashboards to ensure data was matching as per the business requirements and if there were any changes in underlying data

Environment: Data Governance, SQL Server, ER Studio 9.7, Tableau 9.03, AWS, Teradata 15, ETL, MS Office Suite - Excel(Pivot, VLOOKUP), DB2, R, Python, Visio, HP ALM, Agile, Azure, Data Quality, Tableau and Reference Data Management.

Client: Enbridge,Houston,TX .		                                                                                                 May 2017- Jul2018
Role: Data Scientist

Description: Enbridge Inc. is a Canadian multinational energy transportation company based in Calgary, Alberta. It focuses on the transportation, distribution and generation of energy, primarily in North America.
Responsibilities:
* A highly immersive DataScience program involving DataManipulation&Visualization, Web Scraping, MachineLearning, Python programming, SQL, GIT, Unix Commands, NoSQL, MongoDB, Hadoop.
* Installed and used CaffeDeepLearningFrame
* ed on different data formats such as JSON, XML and performed machinelearningalgorithms in Python.
* Participated in all phases of datamining; datacollection, datacleaning, developingmodels, validation, visualization and performed Gapanalysis. 
* Developing Voice Bot  using AI (IVR ), improving the interaction between Human and the Virtual Assistant .
* Implemented Event Task for execute Application Automatically. 
* Involved in developing Patches & Updates Module. 
* Setup storage and dataanalysis  in AmazonWebServices cloud computing infrastructure. 
* Used pandas, numpy, seaborn, scipy, matplotlib, scikit-learn, NLTK in Python for developing various machinelearningalgorithms. 
* Development and Deployment using Google Dialogflow Enterprise.
* ed asDataArchitectsand ITArchitectsto understand the movement ofdataand its storage and ERStudio9.7
* Data visualizationusingElasticsearch ,Kibana and Logstash in python.
* Used Kibana an open source plugin for Elasticsearch  in analytics and Data visualization.
* DataManipulation and Aggregation from different source using Nexus, Toad, BusinessObjects, PowerBI and SmartView.
* ed on backing up and restoring the Azure Data Factory.
* Performed Data Cleaning, features scaling, features engineering using pandas and numpy packages in python and build models using deep learning frames.
* Implemented application of various machine learning algorithms and statistical modeling like Decision Tree, Text Analytics, Sentiment Analysis, Naive Bayes, Logistic Regression and Linear Regression using Python to determine the accuracy rate of each model.
* Implemented Agile Methodology for building an internal application.
* Extracting the source data from Oracle tables, MS SQL Server, sequential files and excel sheets. 
* Migrating Informatica mappings from SQL Server to Netezza Foster culture of continuous engineering improvement through mentoring, feedback, and metrics 
* Broad knowledge of programming, and scripting (especially in R / Java / Python) 
* Developing and maintaining Data Dictionary to create metadata reports for  and business purpose. 
* Predictive modeling using state-of-the-art methods 
* Developed MapReduce/Spark Python modules for machine learning & predictive analytics in Hadoop on AWS. 
* Parse and manipulate raw, complex data streams to prepare for loading into an analytical tool. 
* Build and maintain dashboard and reporting based on the statistical models to identify and track key metrics and risk indicators. 
* Proven experience building sustainable and trustful relationships with senior leaders
* Focus on integration overlap and Informatica newer commitment to MDM with the acquisition of Identity Systems. 
* Development level experience in Microsoft Azure providing data movement and scheduling functionality to cloud-based  such as Azure Blob Storage and Azure SQL Database.
* Data analysis using regressions, data cleaning, excel v-look up, histograms and TOAD client and data representation of the analysis and suggested solutions for investors 
* Rapid model creation in Python using pandas, numpy, sklearn, and plot.ly for data visualization. These models are then implemented in SAS where they are interfaced with MSSQL databases and scheduled to update on a timely basis. 
* Attained good knowledge in Hadoop Data Lake Implementation and HADOOP Architecture for client business data management. 
* Extracted data from HDFS and prepared data for exploratory analysis using datamunging

Environment:ER Studio 9.7, Tableau 9.03, AWS, Teradata 15, MDM,  GIT, Unix, Python 3.5.2, , MLLib, SAS, regression, logistic regression, Hadoop, NoSQL, Teradata,  OLTP, random forest, OLAP, HDFS, ODS, NLTK, SVM, JSON, XML, MapReduce, Google Dialog Flow.

Client: DAK Americas LLC,Moncks Corner,SC.					                       Jan 2016- Apr 2017
Role:DataAnalyst/Data Scientist

Description: DAK Americas LLC manufactures and supplies polyethylene terephthalate resins (PET), polyesters staple fibers (PSF), monomers/ingredients (TPA/PTA), and specialty polymers for customers in the United States and internationally.

Responsibilities:
* Assisting business by being able to deliver a machine learning project from beginning to end, aggregating and exploring data, building and validating predictive models and deploying completed models to deliver business impacts to the organization
* Created data modeling and data mapping document containing source, formulate transformational rules to populate target fields.
* Performed Data Cleaning, features scaling, features engineering using pandas and numpy packages in python and build models using SAP Predictive Analytics
* Used R and python for Exploratory Data Analysis, A/B testing, HQL, VQL, Data Lake, AWS Redshift, oozie, pySpark, Anova test and Hypothesis test to compare and identify the effectiveness of Creative Campaigns.
* Created impact & gap analysis documents specifying changes introduced as part of the program and lead the business process team
*  with big data consultants to analyze, extract, normalize and label relevant data using Statistical modeling techniques like Logistic regression, decision trees, Support vector machine, Random forest, Naive Bayes and neural nets
* Developed ETLs for data sources used in production reporting for marketing and operations teams.
* Write SQL queries to perform data analysis, data modeling and prepare data mapping documents to explain the transformation rules from source to target tables
* Led the Change Management stream of an HR/Payroll project resulting from a $16.5 billion acquisition and the formation of UTAS, created Change Management Plan, and ensured team was on target to deliver both communication and training to HR, finance and Payroll staff.
* Review business data for trends, patterns or casual analysis to assist in identifying model drift and retraining models 
* Created customized reports and processes in SAS and Tableau Desktop
* Performed data analysis to create reporting requirements by specifying inclusion & exclusion criteria, conditions, business rules and data elements to be included into the report
* Scheduled and facilitated requirements gathering with HR, Payroll, finance and accounting teams to implement ADP eTime and ADP Enterprise v5 and ADP General Ledger and drove requirements for data collection and data modeling with data engineers
* Performed SQL query for data analysis and integration
* Support PMO governance activities; defining and maintaining Project Management standards. 
* Responsible for generating ideas for product changes that improve key metrics
* Provided data analytics of the web-portal to the team for feedback and improvement. 

Environment:Python, HTML5, CSS3, AJAX,Teradata,  OLTP, random forest, OLAP, HDFS, ODS,  JSON, jQuery, MySQL, NumPy, SQL Alchemy, Matplotlib,Hadoop, Pig Scripts.

Client:Emblem Health. Newyor,NY.							May 2014- Dec2015
Role: Data Analyst/Data Modeler

Description: Emblem Health is one of the United States' largest non-profit health plans. It is headquartered at 55 Water Street in Lower Manhattan, New York City. It is a $10 billion company with 3.1 million members.

Responsibilities:
* Involved in defining the source to target data mappings, business rules, data definitions. 
* Involved in defining the business/transformation rules applied for sales and service data. 
* ed with project team representatives to ensure that logical and physical ER/Studio data models were developed in line with corporate standards and guidelines. 
* Define the list codes and code conversions between the source systems and the data mart. 
* Coordinate with the business users in providing appropriate, effective and efficient way to design the new reporting needs based on the user with the existing functionality. 
* ed with BTEQ to submit SQL statements, import and export data, and generate reports in Teradata. 
* Responsible for defining the key identifiers for each mapping/interface. 
* Used Python, R, SQL to create Statistical algorithms involving Multivariate Regression, Linear Regression, Logistic Regression, PCA, Random forest models, Decision trees, Support Vector Machine for estimating the risks of welfare dependency.
* Responsible for defining the key identifiers for each mapping/interface. 
* Implementation of Metadata Repository, Maintaining Data Quality, Data Cleanup procedures, Transformations, Data Standards, Data Governance program, Scripts, Stored Procedures, triggers and execution of test plans 
* Performed data quality in TalendOpenStudio. 
* Coordinated meetings with vendors to define requirements and system interaction agreement documentation between client and vendor system. 
* Responsible for defining the functional requirement documents for each source to target interface. 
* Remain knowledgeable in all areas of business operations in order to identify systems needs and requirements. 
* Document the complete process flow to describe program development, logic, testing, and implementation, application integration, coding. 
* Enterprise Metadata Library with any changes or updates. 
* Generate weekly and monthly asset inventory reports.

Environment: Erwin r7.0, SQL Server 2012/2008, Windows XP/NT/2000, Oracle 10g/9i, MS-DTS, UML, UAT, SQL Loader, OOD, OLTP, PL/SQL, MS Visio, Informatica.

Client: GD Research centre Hyderabad,India.			                                              Dec2012- Apr2014
Role: Data Analyst

Description: GD Research Center is a leading provider of global business intelligence including competitor, market, product, and customer information. It provides in-depth research, analysis, data and forecasts through a range of interactive online databases.

Responsibilities:
* Analyze business information requirements and model class diagrams and/or conceptual domain models. 
* Managed the project requirements, documents and use cases by IBM Rational RequisitePro. 
* Assisted in building an Integrated LogicalDataDesign, propose physical database design for building the data mart. 
* Gather & Review Customer Information Requirements for OLAP and building the data mart. 
* Responsible for defining the key identifiers for each mapping/interface 
* Responsible for defining the functional requirement documents for each source to target interface. 
* Coordinated meetings with vendors to define requirements and system interaction agreement documentation between client and vendor system. 
* Enterprise Metadata Library with any changes or updates. 
* Document data quality and traceability documents for each source interface. 
* Performed document analysis involving creation of Use Cases and Use Case narrations using Microsoft Visio, in order to present the efficiency of the gathered requirements. 
* Analyzed business process flows and assisted in the development of ETL procedures for mapping data from source to target systems. 
* ed with BTEQ to submit SQL statements, import and export data, and generate reports in Terra-data. 
* Calculated and analyzed claims data for provider incentive and supplemental benefit analysis using Microsoft Access and Oracle SQL. 
* Establish standards of procedures. 
* Generate weekly and monthly asset inventory reports. 
* Document all data mapping and transformation processes in the Functional Design documents based on the business requirements

Environment: SQL Server 2008R2/2005 Enterprise, SSRS, SSIS, Crystal Reports, Windows Enterprise Server 2000, DTS, SQL Profiler, and Query Analyzer.

Client: ABP Engitech solutions PVT Ltd,Hyderabad				               Feb 2011- Nov 2012
Role: Data Analyst

Description: ABP is a global Engineering company firm that provides total engineering solutions for a variety of types and sized jobs to a substantial and diversified client base that includes Oil and Gas, Steel, Power plants, Rail, and Aerospace industries.The firm is in Hyderabad, India and maintains offices in UK, Kuwait,and Jamshedpur around the world.

Responsibilities:
* Developed an Object modeling in UML for Conceptual Data Model using Enterprise Architect. 
* Developed logical and Physical data models using Erwin to design OLTP system for different applications. 
* Facilitated transition of logical data models into the physical database design and recommended  approaches for good data management practices. 
* ed with DBA group to create Best-Fit Physical Data Model with DDL from the Logical Data Model using Forward engineering. 
* ed with the ETL team to document the transformation rules for data migration from OLTP to Warehouse environment for reporting purposes. 
* Developed Data Migration and Cleansing rules for the Integration Architecture (OLTP, ODS, DW).
* Performed K-meansclustering, Multivariate analysis, and Support Vector Machines in R. 
* Extensive system study, design, development and testing were carried out in the Oracle environment to meet the customer requirements. 
* Written complex Hive and SQL queries for data analysis to meet business requirements. 
* Written complex SQL queries for implementing business requirements

Environment: DB2, Teradata, SQL-Server 2008, Enterprise Architect, Power Designer, MS SSAS, Crystal Reports, SSRS, ER Studio, Lotus Notes, Windows XP, MS Excel, word and Access.",Data Scientist,emailed,indeed job post,resume
" Above 8+ years of experience in large datasets of Structured and Unstructured data, DataVisualization , Data Acquisition, Predictive modeling, NLP/NLU/NLG/AI/machine learning/Computer vision/Probabilistic Graphical Models/Inferential statistics/Graph,Data Validation. 
 Hands on experience indata mining algorithms and approach. 
 Good at algorithm and design techniques 
 Expert level understanding in ApplicationDesign, Development and testing in
Mainframeenvironmentsusing PL/1, COBOL, EGL, Easytrieve, DB2, JCL, QC &VAG. 
 Experience in developing different Statistical Machine Learning, Text Analytics, Data Mining solutionsto various business generating and problems data visualizations using Python, R and Tableau.  Expertise in transforming business requirements into building models, designing algorithms, developing data mining and reporting solutions that scales across massive volume of unstructured data and structured. 
 Regressionanalysis, Statisticaltest analysis, Report and Dashboard generation, Datamanagement. 
 Git, Java, MySQL, MongoDB, Neo4J, AngularJS, SPSS, Tableau. 
 Python, Numpy, Scikit-Learn, genism, NLTK, Tensorflow, keras. 
 Experience in MachineLearning, Statistics, Regression- Linear, Logistic, Poisson, Binomial. 
 Experience in designing visualizations using Tableau software and Storyline on web and desktopplatforms, publishing and presenting dashboards. 
 Single handed built a model to replace the job of doer in the pension sector. This model (Patentunder progress) generates experience from structured data and learns through a bootstrapping mechanism new experience from unseen data. 
 Single handed Built and designed a whole Information extraction botPOC for KYC extraction. Thisbot is using adaptive learning techniques and uses some custom supervised classifiers for entity and relation extraction 
 Proficient in Machine Learning techniques (Decision Trees, Linear, Logistics, Random Forest, SVM,
Bayesian, XG Boost, K-Nearest Neighbors) and Statistical Modeling in Forecasting/ Predictive Analytics,
Segmentation methodologies, Regression based models, Hypothesis testing, Factor analysis/ PCA, Ensembles. 
 Hands on experience in implementing LDA, Naive Bayes and skilled in Decision Trees, Random
Forests, Linear and Logistic Regression, SVM, Clustering, neural nets and good knowledge on
Recommender Systems. 
 Develop Azure POC for Retailer Data Research tool. Develop and Data Factory Micro-service datamovement from Azure SQL Server to Cosmos NoSQL Document database. 
 Experience on advanced SASprogramming techniques, such as PROC APPEND, PROC DATASETS, and
PROC TRANSPOSE. 
 Highly skilled in using visualization  like Tableau, Matplotlib for creating dashboards. 
 Extensive ing experience with Python including Scikit-learn, Pandas and Numpy. 
 Developed data variation analysis and data pair association analysis in the Bioinformatics field. 
 Regularly accessing JIRA tool and other internal issue trackers for the Project development. 
 Having good domain knowledge on Retail and Airlines. 
 Experience in foundational machine learning models and concepts( Regression, boosting, GBM, NNs,
HMMs, CRFs, MRFs, deep learning). 
 Well experienced in Normalization&De-Normalization techniques for optimum performance inrelational and dimensional database environments. 
 Analyzed data using R, Perl, Hadoop and queried data using structured and unstructured databases 
 Strong programming expertise Python and strong in Database SQL. 
 Integration Architect & Data Scientist experience in Analytics, Big Data, SOA, ETL and Cloud. 
 ed and extracted data from various database sources like Oracle, SQL Server and Teradata.  Skilled in System Analysis, Dimensional Data Modeling, Database Design and implementing RDBMS specific features. 
 Facilitated and helped translate complex quantitative methods into simplified solutions for users.  Knowledge of ing with Proof of Concepts and gap analysis and gathered necessary data for analysis from different sources, prepared data for data exploration using data munging. 
 Solid coding and engineering  in Machine Learning 
 Experience with file systems, server architectures, databases, SQL, and data movement (ETL). 
 Proficient in Python, experience building, and product ionizing end-to-end systems 
 Knowledge of Information Extraction, NLP algorithms coupled with Deep Learning
Willing to relocate: Anywhere
Authorized to  in the US for any employer
 Experience

Data Scientist
State of MA - Boston, MA
August 2018 to Present
Description: Massachusetts, officially the Commonwealth of Massachusetts, is the most populous state in the New England region of the northeastern United States. It borders on the Atlantic Ocean to the east, the states of Connecticut and Rhode Island to the south, New Hampshire and Vermont to the north, and New York to the west. 
 
Responsibilities: 
 Involved in defining the source to target data mappings, business rules, and data definitions.  Performing data profiling on various source systems that are required for transferring data to ECH using 
 Defining the list codes and code conversions between the source systems and the data mart using
Reference Data Management (RDM). 
 Involved in data collection and induction to Teradata 
 Conducted data cleaning, data preparation, and outlier detection 
 Finding insights from millions of customer chat and calls records 
 Gathering requirements from business. 
 Involved in creating pipelines that move, transform, and analyze data from a wide variety of sourcesusing multiple methods like the Azure Power shell utility that allows control of many other Azure resources,Python, a REST API, and the Azure Portal UI. 
 Reviewing business requirements and analyzing data sources 
 Developed predictive models for sales and Finance teams using various ML and DL algorithms 
 Utilizing Informatica et (InformaticaData Explorer, and Informatica Data Quality) to analyzelegacy data for Data Profiling. 
 Created a Handler function in Python using AWS Lambda that can invoke when the service isexecuted. 
 Implemented statistical modeling with XGBoost machine learning software package using Python todetermine the predicted probabilities of each model. 
 ed on DTS Packages, DTS Import/Export for transferring data between SQL Server 2000 to 2005
 
 Involved in upgrading DTS packages to SSIS packages (ETL). 
 Involved in Training and Testing the ML Supervised and Unsupervised models 
 Researching on Deep Learning to implement NLP 
 Presented to the higher management the discovered trends and analysis, forecast data,recommendations, model results and risks identified. 
 Design and Develop Data Warehouse for Tax Information Factory using Oracle PL/SQL. 
 Performing an end to end InformaticaETL Testing for these custom tables by writing complex SQL
Queries on the source database and comparing the results against the target database. 
 Using HP Quality Center v 11 for defect tracking of issues. 
 Involved in applying data mining techniques and optimization techniques in B2B and B2C industriesand proficient in Machine Learning, Data/Text Mining, Statistical Analysis and Predictive Modeling. 
 Created and presented executive dashboards to show the patterns & trends in the data usingTableau Desktop 
 Developed NLP models for Topic extraction, Sentiment Analysis 
 Developed Executive  KPI, Key value programs, NPI dashboards in Tableau 
 Created customized Calculations, Conditions and Filters (Local, Global) for various analytical reportsand dashboards. 
 Experience in Analysis, Design, Development, Implementation, Testing and Support of Data
Warehousing and Data Integration Solutions using Informatica Power center. 
 Was able to identify emerging issues using the models 
 Developing & evaluating Machine Learning models 
 Developed different visualizations using advanced features and deep analytics in Tableau 
 Used algorithms and programming to efficiently go through large datasets and apply treatments,filters, and conditions as needed 
 Developed Cross Tab, Chart, Funnel charts, Donut charts, Heat Maps, Tree Maps and Drill Through
Reports, 100% stacked bar charts etc. in Tableau Desktop 
 Involved in publishing, scheduling and subscriptions with Tableau Server andcreating and managingusers, groups, sites in Tableau Server. 
 Involved in developing and testing the SQL Scripts for report development, Tableau reports,
Dashboards and handled the performance issues effectively 
 Tested dashboards to ensure data was matching as per the business requirements and if there wereany changes in underlying data 
 
Environment: Data Governance, SQL Server, ER Studio 9.7, Tableau 9.03, AWS, Teradata 15, ETL, MS Office Suite - Excel(Pivot, VLOOKUP), DB2, R, Python, Visio, HP ALM, Agile, Azure, Data Quality, Tableau and Reference Data Management.
Data Scientist
Enbridge,Houston,TX - Houston, TX
May 2017 to July 2018
Description: Enbridge Inc. is a Canadian multinational energy transportation company based in
Calgary, Alberta. It focuses on the transportation, distribution and generation of energy, primarily in
North America. 
Responsibilities: 
 A highly immersive DataScience program involving DataManipulation&Visualization, Web Scraping,
MachineLearning, Python programming, SQL, GIT, Unix Commands, NoSQL, MongoDB, Hadoop. 
 Installed and used CaffeDeepLearningFrame 
 ed on different data formats such as JSON, XML and performed machinelearningalgorithms inPython. 
 Participated in all phases of datamining; datacollection, datacleaning, developingmodels, validation,visualization and performed Gapanalysis. 
 Developing Voice Bot using AI (IVR ), improving the interaction between Human and the VirtualAssistant 
 Implemented Event Task for execute Application Automatically. 
 Involved in developing Patches & Updates Module. 
 Setup storage and dataanalysis  in AmazonWebServices cloud computing infrastructure. 
 Used pandas, numpy, seaborn, scipy, matplotlib, scikit-learn, NLTK in Python for developing variousmachinelearningalgorithms. 
 Development and Deployment using Google Dialogflow Enterprise. 
 ed as Data Architects and IT Architects to understand the movement of data and its storage andERStudio9.7 
 Data visualizationusingElasticsearch ,Kibana and Logstash in python. 
 Used Kibana an open source plugin for Elasticsearch in analytics and Data visualization. 
 DataManipulation and Aggregation from different source using Nexus, Toad, BusinessObjects,
PowerBI and SmartView. 
 ed on backing up and restoring the Azure Data Factory. 
 Performed Data Cleaning, features scaling, features engineering using pandas and numpy packagesin python and build models using deep learning frames. 
 Implemented application of various machine learning algorithms and statistical modeling like
Decision Tree, Text Analytics, Sentiment Analysis, Naive Bayes, Logistic Regression and Linear
Regression using Python to determine the accuracy rate of each model. 
 Implemented Agile Methodology for building an internal application. 
 Extracting the source data from Oracle tables, MS SQL Server, sequential files and excel sheets.  Migrating Informatica mappings from SQL Server to Netezza Foster culture of continuous engineering improvement through mentoring, feedback, and metrics 
 Broad knowledge of programming, and scripting (especially in R / Java / Python) 
 Developing and maintaining Data Dictionary to create metadata reports for technical and businesspurpose. 
 Predictive modeling using state-of-the-art methods 
 Developed MapReduce/Spark Python modules for machine learning & predictive analytics in Hadoopon AWS. 
 Parse and manipulate raw, complex data streams to prepare for loading into an analytical tool.  Build and maintain dashboard and reporting based on the statistical models to identify and track key metrics and risk indicators. 
 Proven experience building sustainable and trustful relationships with senior leaders 
 Focus on integration overlap and Informatica newer commitment to MDM with the acquisition of
Identity Systems. 
 Development level experience in Microsoft Azure providing data movement and schedulingfunctionality to cloud-based  such as Azure Blob Storage and Azure SQL Database. 
 Data analysis using regressions, data cleaning, excel v-look up, histograms and TOAD client and datarepresentation of the analysis and suggested solutions for investors 
 Rapid model creation in Python using pandas, numpy, sklearn, and plot.ly for data visualization.These models are then implemented in SAS where they are interfaced with MSSQL databases and scheduled to update on a timely basis. 
 Attained good knowledge in Hadoop Data Lake Implementation and HADOOP Architecture for clientbusiness data management. 
 Extracted data from HDFS and prepared data for exploratory analysis using datamunging 
 
Environment:ER Studio 9.7, Tableau 9.03, AWS, Teradata 15, MDM, GIT, Unix, Python 3.5.2, , MLLib, SAS, regression, logistic regression, Hadoop, NoSQL, Teradata, OLTP, random forest, OLAP, HDFS, ODS, NLTK, SVM, JSON, XML, MapReduce, Google Dialog Flow.
DataAnalyst/Data Scientist
DAK Americas LLC - Moncks Corner, SC
January 2016 to April 2017
Description: DAK Americas LLC manufactures and supplies polyethylene terephthalate resins (PET), polyesters staple fibers (PSF), monomers/ingredients (TPA/PTA), and specialty polymers for customers in the United States and internationally. 
 
Responsibilities: 
 Assisting business by being able to deliver a machine learning project from beginning to end,aggregating and exploring data, building and validating predictive models and deploying completed models to deliver business impacts to the organization 
 Created data modeling and data mapping document containing source, formulate transformationalrules to populate target fields. 
 Performed Data Cleaning, features scaling, features engineering using pandas and numpy packagesin python and build models using SAP Predictive Analytics 
 Used R and python for Exploratory Data Analysis, A/B testing, HQL, VQL, Data Lake, AWS Redshift,oozie, pySpark, Anova test and Hypothesis test to compare and identify the effectiveness of Creative Campaigns. 
 Created impact & gap analysis documents specifying changes introduced as part of the program andlead the business process team 
  with big data consultants to analyze, extract, normalize and label relevant data using Statisticalmodeling techniques like Logistic regression, decision trees, Support vector machine, Random forest,
Naive Bayes and neural nets 
 Developed ETLs for data sources used in production reporting for marketing and operations teams.  Write SQL queries to perform data analysis, data modeling and prepare data mapping documents to explain the transformation rules from source to target tables 
 Led the Change Management stream of an HR/Payroll project resulting from a $16.5 billionacquisition and the formation of UTAS, created Change Management Plan, and ensured team was on target to deliver both communication and training to HR, finance and Payroll staff. 
 Review business data for trends, patterns or casual analysis to assist in identifying model drift andretraining models 
 Created customized reports and processes in SAS and Tableau Desktop 
 Performed data analysis to create reporting requirements by specifying inclusion & exclusion criteria,conditions, business rules and data elements to be included into the report 
 Scheduled and facilitated requirements gathering with HR, Payroll, finance and accounting teamsto implement ADP eTime and ADP Enterprise v5 and ADP General Ledger and drove requirements for data collection and data modeling with data engineers 
 Performed SQL query for data analysis and integration 
 Support PMO governance activities; defining and maintaining Project Management standards. 
 Responsible for generating ideas for product changes that improve key metrics 
 Provided data analytics of the web-portal to the team for feedback and improvement. 
 
Environment:Python, HTML5, CSS3, AJAX,Teradata, OLTP, random forest, OLAP, HDFS, ODS, JSON, jQuery, MySQL, NumPy, SQL Alchemy, Matplotlib,Hadoop, Pig Scripts.
Data Analyst/Data Modeler
Emblem Health. Newyor,NY
May 2014 to December 2015
Description: Emblem Health is one of the United States' largest non-profit health plans. It is headquartered at 55 Water Street in Lower Manhattan, New York City. It is a $10 billion company with 3.1 million members. 
 
Responsibilities: 
 Involved in defining the source to target data mappings, business rules, data definitions. 
 Involved in defining the business/transformation rules applied for sales and service data. 
 ed with project team representatives to ensure that logical and physical ER/Studio data modelswere developed in line with corporate standards and guidelines. 
 Define the list codes and code conversions between the source systems and the data mart. 
 Coordinate with the business users in providing appropriate, effective and efficient way to design thenew reporting needs based on the user with the existing functionality. 
 ed with BTEQ to submit SQL statements, import and export data, and generate reports inTeradata. 
 Responsible for defining the key identifiers for each mapping/interface. 
 Used Python, R, SQL to create Statistical algorithms involving Multivariate Regression, LinearRegression, Logistic Regression, PCA, Random forest models, Decision trees, Support Vector Machine for estimating the risks of welfare dependency. 
 Responsible for defining the key identifiers for each mapping/interface. 
 Implementation of Metadata Repository, Maintaining Data Quality, Data Cleanup procedures,Transformations, Data Standards, Data Governance program, Scripts, Stored Procedures, triggers and execution of test plans 
 Performed data quality in TalendOpenStudio. 
 Coordinated meetings with vendors to define requirements and system interaction agreementdocumentation between client and vendor system. 
 Responsible for defining the functional requirement documents for each source to target interface.  Remain knowledgeable in all areas of business operations in order to identify systems needs and requirements. 
 Document the complete process flow to describe program development, logic, testing, andimplementation, application integration, coding. 
 Enterprise Metadata Library with any changes or updates. 
 Generate weekly and monthly asset inventory reports. 
 
Environment: Erwin r7.0, SQL Server 2012/2008, Windows XP/NT/2000, Oracle 10g/9i, MS-DTS, UML, UAT, SQL Loader, OOD, OLTP, PL/SQL, MS Visio, Informatica.
Data Analyst
GD Research centre - Hyderabad, Telangana
December 2012 to April 2014
Description: GD Research Center is a leading provider of global business intelligence including competitor, market, product, and customer information. It provides in-depth research, analysis, data and forecasts through a range of interactive online databases. 
 
Responsibilities: 
 Analyze business information requirements and model class diagrams and/or conceptual domainmodels. 
 Managed the project requirements, documents and use cases by IBM Rational RequisitePro. 
 Assisted in building an Integrated LogicalDataDesign, propose physical database design for buildingthe data mart. 
 Gather & Review Customer Information Requirements for OLAP and building the data mart. 
 Responsible for defining the key identifiers for each mapping/interface 
 Responsible for defining the functional requirement documents for each source to target interface.  Coordinated meetings with vendors to define requirements and system interaction agreement documentation between client and vendor system. 
 Enterprise Metadata Library with any changes or updates. 
 Document data quality and traceability documents for each source interface. 
 Performed document analysis involving creation of Use Cases and Use Case narrations using
Microsoft Visio, in order to present the efficiency of the gathered requirements. 
 Analyzed business process flows and assisted in the development of ETL procedures formapping data from source to target systems. 
 ed with BTEQ to submit SQL statements, import and export data, and generate reports in Terra-data. 
 Calculated and analyzed claims data for provider incentive and supplemental benefit analysis using
Microsoft Access and Oracle SQL. 
 Establish standards of procedures. 
 Generate weekly and monthly asset inventory reports. 
 Document all data mapping and transformation processes in the Functional Design documents basedon the business requirements 
 
Environment: SQL Server 2008R2/2005 Enterprise, SSRS, SSIS, Crystal Reports, Windows Enterprise Server 2000, DTS, SQL Profiler, and Query Analyzer.
Data Analyst
ABP Engitech solutions PVT Ltd - Hyderabad, Telangana
February 2011 to November 2012
Description: ABP is a global Engineering company firm that provides total engineering solutions for a variety of types and sized jobs to a substantial and diversified client base that includes Oil and Gas, Steel, Power plants, Rail, and Aerospace industries.The firm is in Hyderabad, India and maintains offices in UK, Kuwait,and Jamshedpur around the world. 
 
Responsibilities: 
 Developed an Object modeling in UML for Conceptual Data Model using Enterprise Architect.  Developed logical and Physical data models using Erwin to design OLTP system for different applications. 
 Facilitated transition of logical data models into the physical database design and recommendedtechnical approaches for good data management practices. 
 ed with DBA group to create Best-Fit Physical Data Model with DDL from the Logical Data Modelusing Forward engineering. 
 ed with the ETL team to document the transformation rules for data migration from OLTP to
Warehouse environment for reporting purposes. 
 Developed Data Migration and Cleansing rules for the Integration Architecture (OLTP, ODS, DW). 
 Performed K-meansclustering, Multivariate analysis, and Support Vector Machines in R. 
 Extensive system study, design, development and testing were carried out in the Oracle environmentto meet the customer requirements. 
 Written complex Hive and SQL queries for data analysis to meet business requirements.  Written complex SQL queries for implementing business requirements 
 
Environment: DB2, Teradata, SQL-Server 2008, Enterprise Architect, Power Designer, MS SSAS, Crystal Reports, SSRS, ER Studio, Lotus Notes, Windows XP, MS Excel, word and Access.
Education

Bachelor's


Data analysis, Ddl, Sql server, Postgres, Sql, Mapreduce, Bayesian, Clustering, Etl, Hadoop, Machine learning, Teradata, Sas, Tableau, Hadoop, Hive, Bootstrap, Mapreduce, Pig, Python, Business Intelligence
Additional Information

TECHNICAL  
 
Statistics/ML 
Exploratory Data Analysis: Univariate/MultivariateOutlier detection, Missing value imputation,
Histograms/Density estimation, EDA in Tableau 
Supervised Learning: Linear/Logistic Regression, Lasso, Ridge, Elastic Nets, Decision Trees, Ensemble
Methods, Random Forests, Support Vector Machines, Gradient Boosting, XGB, Deep Neural Nets, Bayesian Learning 
Unsupervised Learning: Principal Component Analysis, Association Rules, Factor Analysis, K-Means,
Hierarchical Clustering, Gaussian Mixture Models, Market Basket Analysis, Collaborative Filtering and
Low Rank Matrix Factorization 
Feature Selection: Stepwise, Recursive Feature Elimination, Relative Importance, Filter Methods,
Wrapper Methods and Embedded Methods 
Statistical Tests: T Test, Chi-Square tests, Stationarity tests,Auto Correlation tests, Normality tests,
Residual diagnostics, Partial dependence plots and Anova 
Sampling Methods: Bootstrap sampling methods and Stratified sampling 
Model Tuning/Selection: Cross Validation, Walk Forward Estimation, AIC/BIC Criterions, Grid Search and Regularization 
Time Series: ARIMA, Holt winters, Exponential smoothing, Bayesian structural time series 
 
Machine Learning / 
Deep Learning 
 
R:caret, glmnet, forecast, xgboost, rpart, survival, arules, sqldf, dplyr, nloptr, lpSolve, ggplot 
Python:pandas, numpy, scikit-learn, scipy, statsmodels, matplotlib,tensorflow 
SAS: Forecast server, SAS Procedures and Data Steps 
Spark: MLlib, GraphX 
SQL:Subqueries, joins, DDL/DML statements 
 
Databases/ETL/Query Teradata, SQL Server, Redshift, Postgres and Hadoop (MapReduce); SQL, Hive,
Azure Data Factory ,Pig and Alteryx. 
Visualization Tableau, ggplot2 and RShiny 
Prototyping PowerPoint,RShiny and Tableau",Data Scientist,emailed,indeed job post,resume
" 
CALIFORNIA STATE UNIVERSTY, FULLERTON	
MASTER OF SCIENCE IN STATISTICS  GPA: 3.51
- Advance Theory in Probability and Statistics	- Spatial Statistics
- Statistical Computation		              - Statistical Consulting
- Applied Biostatistics	   	                             - Bayesian Analysis
- Categorical Data Analysis		              - Multivariate Analysis
- Statistical/ Machine Learning	                             - Experiential Design

CALIFORNIA STATE POLYTECHNIC UNIVERSITY, POMONA
BACHELOR OF SCIENCE IN APPLIED MATH/STA  GPA:3.14

Passed SOA/CAS for exam P
Passed SOA/CAS for exam FM
Candidate for SOA/CAS for exam IFM

STATISTICAL CONSULTANT PANASONIC AVIONICS CORP	ORATION
* Manage team of seven to develope model to predict price to maximize profit
 for Panasonic for their on-flight WIFI.
* Design machine learning algorithm in python to predict profit at difference 
price point for on fight WIFI.
* Utilize SQL to organize more than 4M rows and more than 50 features of data.
* Performed data mining, Data visualization
* Assigned proper values for missing values and incorrect values.
* Provide high level recommendation on price to maximize profit base on
 difference regions.
MATH QUALITY CONTROL SPECIALIST ANSRSOURCE
* Part of team to create brand new online home platform for McGraw-Hill.
* ed closely with programmers to ensure quality for each algorithm.
* Implemented operational  into existing algorithms.
* Fostered and facilitated communications form QC team in Irvine to QC team and programmers team in India.
STUDENT ASSISTANT IV Mount San Antonio college
* Analyze data on students success and learning gain
* Specialized as personal tutor on Algebra, Calculus, Statistics, physics.

Languages: Vietnamese, English
Computer Science: 
Advance: R, Python, Microsoft suite
Intermediate: SQL, EXCEL, C++, MATLAB
Data Science: Machine learning, Statistical Computation, Experimental Design, Biostatistics, Classification, Regression.

VIETNAMESE STUDENT ASSIOCIATION TREASURER
Organize events, talks and meeting.
Effectively managed yearly budget.

		




08/2017  08/2019







09/2013  06/2016



11/2015
08/2016
11/2019

01/2019-06/2019







11/2016-12/2017





03/2013 - 06/2016
09/2018  06/2019








	
08/2012  08/2013


",Data Scientist,emailed,indeed job post,resume
"______________________________________________________________________________  
__________________________________________________________________________________________________________________________________________________________________________________________________________________________________________ 
California State University, Fullerton	           ?Cumulative GPA: 3.09 
Bachelor of Science in Computer Science	         Expected Graduation Date: May 2020 
______________________________________________________________________________ 
RELEVANT COURSE 
__________________________________________________________________________________________________________________________________________________________________________________________________________________________________________ 
? Algorithm Engineering 	? Introduction to Data Science 
? Data Structure 	? Software Dev w/ Open Source 
? File Structure & Database 	? Software Engineering 
______________________________________________________________________________   
__________________________________________________________________________________________________________________________________________________________________________________________________________________________________________ 
 
	? Developer Language:? Python, R, C++, 	? Operating Systems: Windows 7, 8, 10,?	 
	C 	Linux, Mac 
	? Software: ?RStudio, PyCharms, Visual 	? Visualization Tool:? Excel, Tableau 
	Studio, MySQL 	 
______________________________________________________________________________ RELATED EXPERIENCE 
__________________________________________________________________________________________________________________________________________________________________________________________________________________________________________ 
Bytes and Bots	     Irvine, CA 
Lead Mentor 	       June 2018 - August 2019 
? Collaborate with other mentors and teachers to develop a ing environment that keeps primary school students engaged which allowed a team to win first place at a district competition 
? Educate primary school students about coding to promote careers in   
? Set up and test station equipment such as ultrasonic sensors and servo motors to be used with Raspberry Pi  
Quantum Automation	 	 Anaheim,CA 
Technician                                                                                            ?August 2015 - January 2016 
? Assembled solar power battery shells together to be shipped and transported to other manufacturers  
? Cooperate with software developers and engineers to produce solar powered devices  
? Operated with terminal shell that ran scripts which installed operating files and established a connection to a neting server 
______________________________________________________________________________   
__________________________________________________________________________________________________________________________________________________________________________________________________________________________________________ 
 
College vs High School Graduates Life-Time Revenue                      September 2019 - Present?	 
? Wrangle out College Scorecard database to pick up financial statistics from 1996 to 2017 to compare the revenue difference of High School Graduates to College Graduates 
? Transform the College Scoreboard database to a clustered bar graph to represent the changes in college tuition with Tableau  
Highway and Social Media Data Analysis?                                                 August 2019 - Present 
? Filter out PeMS databases to extract out valuable information such as traffic speed and number of active vehicles on a freeway to be analyzed with Twitter's API that tracks tweet activities 
Battleship Game Development                                                           ?July 2018 - December 2018 
? Develop an AI system that keeps track of its previous moves in order to establish a move selection foundation  
? Use Python and PyGames to implement a GUI that allows for the user and AI to interact with the field ",Data Scientist,emailed,indeed job post,resume
"A position in data analytics where I can utilize my al background, more than 5 years' experience, communication , and analytical solving  to contribute to the organization
 Experience

Analytics mentor at Thinkful
Data Science
January 2019 to Present
 support students by acting as an advisor and counselor as they complete the course and land theirfirst industry job 
 Host online video sessions on topics of my expertise: python, sql, statistics, etc 
 Review student checkpoints submissions and deliver written feedback, including on  andportfolios 
 Meet with students 1-on-1 in an online video session to provide  and  support asthe student progresses through the curriculum.
Business Analyst
Wells Fargo - Charlotte, NC
August 2018 to February 2019
 Act as a liaison between client area and  organization by planning, conducting, and directingthe analysis of complex business problems to be solved with automated systems 
 Provide  assistance in identifying, evaluating, and developing systems and procedures thatare cost effective and meet business requirements 
  with user groups to provide training, resolve questions, assess user needs, and recommendchanges 
 Prepare specifications for system changes 
 Develop systems test plan components and test scripts
Senior Analyst
Nissan - Nashville, TN
October 2017 to April 2018
 Prepared monthly internal management reports 
 Reviewed quarterly and annual financial information prepared by Senior Analysts 
 provided cash flow analytics for the reporting unit 
 performed account analysis and statistical analysis on various products 
 implemented and administer accounting systems in a variety of functional areas to produceappropriate management reports 
 analyzed financial metrics in Microsoft Excel and SAP 
 created dashboards and models on securitized products for the FPA team
Accounting Analyst
Callidus Construction - Atlanta, GA August 2015 to June 2017
 Managed complex Excel books utilizing macros and VBA scripts 
 Experienced reconciling report data between systems 
 Mapped data between different systems & validate when needed 
 Developed ad hoc reporting as needed 
 Assisted in the planning of major financial statements
Procurement Financial Analyst
ALCOA - Atlanta, GA
September 2013 to April 2015
 ed as a member of the Corporate analytics team by conducting analyses on spend data,business intelligence, designing modeling templates and analyzing data 
 Studied the world economic supply markets related to industry manufacturing products and makepredictions about exchange rates on the present financial condition 
 Performed Data mining and cleansing for large amounts of data across all procurement categoriesand financial systems (oracle and jde) 
 Provided a critical level of support on general and special , inventory, financial analysis,problem resolution and other business/financial analysis related needs that affect Procurement  Designed and present analytical models and supporting documentation to senior management 
 Managed saving multiple schedules and track monthly savings for sourced initiatives 
 Queried relevant spend data using SQL
Database Analyst
MCKESSON CORPORATION - Alpharetta, GA
March 2012 to June 2013
 Responsible for the development and administration of analytical data constructs/structures 
 Translated information into meaningful analysis using SQL within MS ACCESS and MS SharePoint
(certified SharePoint administrator at McKesson) 
 Visualized and maintained current organizational metrics and force data 
 Managed, manipulated, and analyzed data in Microsoft Excel 
 Effectively communicated the analysis and conclusions
Budget Analyst
US FISH AND WILDLIFE - Atlanta, GA December 2010 to December 2011
NACI GOVERNMENT CLEARANCE] 
 Sampled documentation and spending variances to determine compliance 
 Built quantitative financial models for the budget and finance team using MS Excel 
 Helped with the transition of the overall FFS system to an ERP system functionality(FBMS)-driven SAPsolution 
 Revised payment documentation related to the Deepwater Horizon Oil spill 
 Created and administrated spending targets for 10 programs based on the revisions of the federalbudget object codes


Master of Science in Business Analytics in MS-BA
Arizona State University - Tempe, AZ May 2019
Bachelor of Business Administration in Finance
Kennesaw State University - Kennesaw, GA May 2011


Excel (4 years), MICROSOFT SHAREPOINT (1 year), MS SharePoint (1 year), SAP (1 year), sql (3 years)
Additional Information

Management & Leadership 
 Member of the I school initiative at Kennesaw State University 
 Participated in SIFE (Students in free enterprise) 
 Member of the University of Hawaii football team 
 Member of football and basketball teams at Allen High School in Texas 
 Excellent written and verbal and all- encompassing communication  
 Mentored elementary school children as part of the Reading With The Eagle's high school community
initiative 
 
  
 Proficient in computers from: MS Office Suite, MS Visio, MS SharePoint, SAP financials, Tableau,
EazyBi, PostgreSQL, MySQL, SQL, SPSS, Essbase, Quickbooks, Minitab, Linear programing, PYTHON,
Text mining, Hadoop, AWS and Internet 
 Machine Learning : MS Azure ML, Python data science stack (Numpy, Pandas, Scikit, Matplotlib,etc.), A/B hypothesis testing, feature engineering and selection, dimensionality reduction, linear regression and regression classification algorithms, Nave bayes models, prescriptive analytics, and expert knowledge of most common machine learning algorithms (KNN, SVM, logistic regression, decision trees, ensemble models, clustering, etc.) 
 Extensive use of Excel, including complex formulas such as VLOOKUP, ""if"" statements, VBA, Solver,macros and formatting, Palisade's Stat, @RISK, PrecisionTree, pivot tables to manipulate and analyze data",Data Scientist,emailed,indeed job post,resume
"My life  experience has been in assistant manager and management positions. My past experiences have been in a business oriented atmosphere and always ed independantly. The  that mainly interest me is sales, manager, criminal justice field,or any business oriented atmosphere. I enjoy people and love helping people as much as I can.
Authorized to  in the US for any employer
 Experience

Massage Therapist
Thai massage - Ontario, CA
June 2019 to Present
Brand ambassador
Interactions marketing - Killeen, TX
June 2014 to Present
Responsibilities 
My Responsibilities are to promote and to demonstrate product, I demonstrate it to customers and let our customer know about the benefits of the product. To manage other brand ambassador about the product they are representing for the day. To make sure they know all their responsibilities as brand ambassadors and make sure they knew all the benefits of the products. My others responsibility was to manage, sales, count inventory, to set up and breakdown material.  
 
Accomplishments 
I met a lot of different people and whether they said no to the product I always tried my best to let them know how good it was and how they could save money, and how it could help them in the future. My main accomplishment was to always make sure the left as a satisfied customer. My main priority is always to make sure they knew the benefits of the product they purchased through the day. My main focus was also to make sure product sold out thru out the day. 
 
 Used 
Customer service, bilingual, lots of confidence and knowledge of product.
Massage Therapist - Independent Contractor
Intermission spa - Upland, CA
October 2017 to June 2019
Perform healing therapeutic massages, helped with little girls spa parties and was responsible for my own transaction fees. Also Performed couples massage.
Pool designer
Premier Pools & Spas - Ontario, CA
April 2016 to August 2016
Selling pools, doing home pool presentations, attending trade shows, and designing pools.
Skin care /sales manager
Infinite aloe/ skin care - Los Angeles, CA March 2015 to January 2016
Roadshow sales Manager: responsible for sales, and promoting skin care products at all Costcos locations. Responsible for assigning schedules, inventory, closing and opening store. Responsible for engaging with sales, and helping my peers with closing their sales, responsible with interacting with customers, and sellung the merchandise. Responsible for setting up and taking down booth.
Seasonal sales, engraver
Things remembered - Fort Hood, TX
January 2012 to November 2013
Sales, customer service, cashier. Responsible for meeting sales quote, and meeting requirements and always up selling product.
Sales Assistant Manager
More than Less - Fort Jackson, SC January 2010 to December 2012
Responsibilities 
Customer service,Merchandise sales such as hats, sunglasses, purses, and shirts . Checked and verified inventory, closed and open up kiosk store.
Assistant Manager
Lucky Wok Chinese Food - Mira Loma, CA
August 2001 to August 2003
Responsibilities 
Customer service, verified all food was done in a timely manner, prepared employees schedules, open and closed store.


Bachelors in Science in leadership
Trident University International - Anaheim, CA
January 2016 to October 2018
Associates in Criminal justice
CTC - Killeen, TX 2010 to 2014
Diploma in Massage Therapy
Bryman college - Ontario, CA
2004 to 2005
Certificate in Private investigator Health and Human Services - Riverside, CA 2001 to 2001


Marketing, Training, Receptionist, Swedish Massage, Deep Tissue, Microsoft Word, Customer Service
Certifications/Licenses

Citizens police academy
August 2014 to Present
Citizens police academy
January 2010 to Present
Adult and infant child CPR February 2012 to February 2015
120 hours of basic security and self defense tactics
January 2006 to Present
Massage Therapist
January 2018 to January 2021
Licensed Massage Therapist
Life and Health Insurance
Additional Information

I have done Volunteer  through military police, substance abuse program, youth ministry for summer vacation bible studies, and Red Cross.",Data Scientist,emailed,indeed job post,resume
"? Data Scientist with over 7+ years of experience in building data products powered by data scienceand machine learning algorithms. Developed data products using R, Python, SQL and Tableau by leveraging machine learning based algorithms, Statistical Modeling, Data Mining, Natural Language Processing (NLP) and Data Visualization techniques 
 
? Experienced doing hands on analytics in several industries like Retail, , Banking and
Airlines. Experienced building user centric analytics in several functions of the business such as Marketing, Operations and Supply Chain 
 
? Proactive participation in product roadmap discussions, data science initiatives and the optimalapproaches to the underlying business problems 
 
? Experience ing with large data and metadata sources; interpret and communicate insights andfindings from analysis and experiments to both  and non- audiences in across various functions in business
Willing to relocate: Anywhere
Sponsorship required to  in the US
 Experience

Data Scientist
ADOBE - San Jose, CA
October 2017 to Present


Bachelor's",Data Scientist,emailed,indeed job post,resume
" Experience

Data Scientist
Computer Sciences Corporation - Tysons Corner, VA
December 2014 to Present


Masters of Science in Electronics and Computer Engineering
Temple University - Philadelphia, PA
Bachelors of Engineering in Electronics and Computer Engineering
RV College of Engineering - Bengaluru, Karnataka


Analysis of variance (Less than 1 year), Anova (Less than 1 year), Apache hadoop mapreduce (Less than 1 year), Association rules (Less than 1 year), Bayesian (Less than 1 year)",Data Scientist,emailed,indeed job post,resume
"Willing to relocate: Anywhere
Authorized to  in the US for any employer
 Experience

Consultant, Data Scientist/Data Engineer
Cigna - Plano, TX
August 2018 to Present
Goal: Applying machine learning capabilities in order to improve business performance. To create a predictive model using the neural net algorithm for the customer's data to predict whether a customer will stay with the company in the future. 
 
Responsibilities: 
* Performed Data Collection, Data Cleaning, Data Clustering, Data Visualization using Python. 
* Preprocessed data by recognizing missing values, outliers, invalid values. 
* Used tableau desktop for creating data visualizations. 
* Captured data into pandas in-memory and fast data-frames to perform data wrangling and be utilizedby other libraries efficiently. 
* Used python scientific library stack. 
* Was involved in creating a neural net models using Python and Tensor-flow on cloud. 
* Performed hyper-parameter tuning for increasing efficiency and accuracy. 
* Implemented regularization techniques for reducing overfitting in the data. 
* Used Amazon S3 for storage and EC2 for training models. 
Environment: Python, Tableau Desktop, Tableau Prep, Amazon S3, Matplotlib, NumPy, Jupyter Notebooks
Consultant, Data Scientist/ Data Engineer
USAA - Dallas, TX
February 2018 to July 2018
Goal: To Integrate data from structured and unstructured sources (text) and enable correlated analytical solutions. Generating value from the data using advanced analytics. Creating various AI/ML models such as sentiment analysis and entity extraction. 
 
Responsibilities: 
* Captured data from databases and integrated required data into one data source. 
* Pre-processed text data using nltk library for tokenization, stop-word removal, stemming etc. 
* Performed feature extraction and reduction by using bigrams and trigrams. 
* Created word vectors using BOW, TF-IDF and word2vec algorithms. 
* Developed sentiment analysis and entity extraction model using RNN in Keras. 
* Reiterated multiple times to create automated end-to-end ML pipeline. 
* Deployed model which was giving results using web APIs. 
* Created model summaries to compare results of various models. 
* Built reports and visualization for easy analytics. 
Environment: Python, Jupyter Notebooks, Tensorflow, AWS EC2, R programming, nltk.
Consultant, Data Scientist
Amdocs - Pune, Maharashtra
June 2016 to July 2017
Goal: To find a click probability using machine learning models. Performed analysis on click through impressions. Implemented binary classification algorithm for prediction purpose. 
 
Responsibilities: 
* ed on around 20TB of click data. 
* Captured data from AWS S3 buckets using the Python and PySpark. 
* Used Spark SQL to query the data. 
* Performed data preprocessing by removing missing data, imputing required values in place ofmissing data. 
* Created visualizations and dashboard for the stakeholder in a tableau desktop. 
* Performed feature engineering by dropping irrelevant variables for better model generalization. 
* Created extra features using features from an original dataset. 
* Implemented random forest and logistic regression models using a sparks machine learning library. 
* Used mini-batch gradient descent for optimizing the algorithm. 
* Created general as well as campaign specific models. 
* Achieved 0.80 of F-score on the final logistic regression model. 
* Developed models in an anaconda environment using Jupyter notebooks. 
* Used lasso and ridge regularization using sparks regularization methods. 
Environment: Python, PySpark, SparkSQL, T-SQL, AWS S3, Jupyter Notebooks, Tableau Desktop.
Python Programmer
PTC - Pune, Maharashtra
June 2015 to June 2016
Goal: To perform web server log analysis using Python and Apache Spark. Performing transformation on data by using regular expressions. Creating visualizations using matplotlib. Monitoring servers using gathered information and insights. 
 
Responsibilities: 
* Cleaned data using a python regular expressions package. 
* Created sparks RDDs for distributing computation. 
* Performed analytics on logs to capture frequent hosts, top endpoints, unique daily hosts etc. 
* Involved in creating visualizations for analyses of data. 
* Used PyCharm IDE to create end-to-end analytics pipeline. 
* Involved in debugging python applications. 
Environment: Python, PyCharm, PySpark


Master of Science in Computer Science in Machine Learning and Artificial Intelligence
Texas A&M University - Commerce, TX


Microsoft visual studio, Visual studio, Algorithm, Apache spark, C++, Git, Natural, Python, Keras,
Matplotlib, Numpy, Pandas, Clustering, Data science, Machine learning, Natural language processing,
Nlp, Tableau desktop, Anomaly detection, Deep learning, SQL, testing, access, Business Intelligence, Excel
Additional Information

Computer Certifications 
* IBM Data Science Specialization (https://www.coursera.org/account/accomplishments/specialization/certificate/CZQR7HLB54TC) 
* Deeplearning.ai Specialization (https://www.coursera.org/account/accomplishments/certificate/
K5JHZ3EG667M) 
* SQL for Data Science by UCDAVIS (https://www.coursera.org/account/accomplishments/certificate/97P3XVW85J6D) 
* Neural Nets and Deep Learning by deeplearning.ai (https://www.coursera.org/account/accomplishments/certificate/K5JHZ3EG667M) 
* Improving Deep Neural Nets: Hyper-parameter tuning Regularization and Optimization bydeeplearning.ai (https://www.coursera.org/account/accomplishments/certificate/K2STL4FNKKKB) * Structuring Machine Learning  with deeplearning.ai (https://www.coursera.org/account/ accomplishments/certificate/GTDJZCMJDCMB * Hands-on Tableau for Data Science 
* AWS Cloud Practitioner on linux academy 
 
 
* Languages: Python, C, C++, SQL 
* Libraries and Frames: Keras, NumPy, Pandas, Matplotlib, SQLAlchemy, Scikit-Learn, Tensor-flow 
* Programing Software: Microsoft Visual Studio, Microsoft SSMS, Azure Data Studio, Jupyter Notebooks,
PyCharm, Tableau Desktop, Tableau Prep, AWS S3, EC2, AWS lambda, Databricks, Apache Spark, Scala, Git 
* Machine Learning Algorithms & Models: Convolutional Neural Net, Recurrent Neural Nets,
Linear Regression algorithms, Logistic Regression model, Anomaly Detection, K-Means Clustering,
Decision Tree algorithm, Random Forest, Time series analysis. 
* Research Papers: Classification of document in NLP (Natural Language Processing) using RNN(Performed text preprocessing to clean text data by removing stop words, using nltk for tokenization and lemmatization, using BOW, word2Vec, TF-IDF for vectorization, and creating natural language models using Recurrent Neural Model)",Data Scientist,emailed,indeed job post,resume
" Data Scientist with an experience of around 8 years, ing through Telecom and Retail industries,holding master's degree in Computer Science and . 
 Extensive ing experience in data science  and  like R, Python, SQL, Tableau andPower BI. 
 Enough experience in agile methodology and ability to manage all phases of SDLC ranging fromrequirement analysis, design, development, testing to deployment. 
 Proficiency in developing story boards and advanced visualizations using Tableau, Python and Power
BI. 
 Maintained a key role in management of team by gathering requirements from clients and discusswith the team. Also, turnback the output with zero-defect. 
 Enough practical knowledge in performing Data Analysis process using Python like Importingdatasets, Data wrangling, Exploratory Data Analysis. 
 Ability to participate in long hour calls to understand and gather the business requirements.  ed on Adobe campaign tool to create and run the campaign flows and gather all the requirements for creating segments in campaign flows. 
 Proficient in creating the segments & suppressions to run the campaign flows and upload them backin production mode after encryption of files. 
 Skilled in implementing techniques like Regression, Classification, Clustering and Recommendersystems, Random forest, decision trees, K means clustering using packages of Python and R studio.  Analyzed pre-existing predictive model for predicting the conversion rate of customers from retail to mail developed by Advanced Analytics team and re-built predictive model using machine learning algorithms by considering factors that better influenced the conversion rate. Increase in the conversion rate is beneficial for both customers and company. 
 Designed and developed Big Data analytics platform for processing customer viewing preferencesand social media comments using Java, Hadoop, Hive and Pig. 
 Develop excellent quality software using agile techniques such as Test-Driven Development and PairProgramming 
 Skilled in implementing machine learning techniques like Regression, Classification, Clustering andRecommender systems including Random forest, decision trees, Support Vector Machines, K means clustering using packages of Python and R studio. 
 Strong experience in ETL data warehousing and implementing all phases of SDLC which includesrequirement gap analysis, design, Datawarehouse implementation, development, testing, deployment and production support maintenance. 
 Proficient with RDBMS like Oracle, and SQL developer. Possess hands on experience of creating UNIXshell scripts required to control the ETL flow and implementing complex ETL logic. 
 Hands on experience in Azure Development, ed on Azure web application, App services,Azure storage, Azure SQL Database, Virtual machines, Fabric controller, Azure AD, Azure search, and notification hub. 
 ed on IBM Watson NLP language to develop the mailbot grammerly which fixes the mail bodydepending on the type of mail. (Official / Personal / informal). 
 Skilled in pulling large datasets to run the manual flows and refresh the dashboards and setup theconnections for different databases. 
 Ability to multi-task the  with zero-defect. 
 Experience in ing with customers to determine their needs, gather, analyze and documentrequirements, communicate with customers throughout the development cycle, manage customer expectations, resolve issues and provide project status. 
 Good communication, interpersonal and quick learning  with proven ability to adapt to differentproject environments.
 Experience

Data Scientist
Comcast Center (HQ) - Philadelphia, PA
September 2017 to Present
Description: 
Comcast corporation is the largest cable TV company and largest home Internet service provider in the
United States, and the nation's third-largest home telephone service provider. ed with marketing & strategic clients to find the better opportunity and understanding the performance of segments in different circumstances. 
 
Responsibilities: 
 Key role in managing a team for a growing relationship through zero defect and aggressive delivery. 
 Responsible for Project Management, Coordination with offshore team and project delivery. 
 Developed solutions to enhance the DW/BI capabilities aligning with the steadily changing businessrequirements. 
 Developed offer calendar to track advertised offers across geography and marketing channels. Thedashboard includes calendar view of advertised offers and performance of the offers in individual marketing channel across time. 
 ed on gathering all the requirements fromUNICA and created the flowcharts for the transition.  Design and development of camping management tool as an additional feature to Segment Lab suite of dashboards. ed on segmentation and opportunity finding. 
 Participated in the transition of project and turnback the client requests with zero defect. 
 ed on pilots like Strawman PPT, KEB Express & DM dashboard requirements gathering, develop
& maintain dashboards deployment. 
 Used K-Means cluster analysis to identify the opportunity for upgrade and lower the churn rate.  Participate in the Adobe campaign development calls on creation of campaign flows and help clients understand the usage of tool. 
 Performed parameter tuning procedures to achieve optimal performance of the model. 
 ed on Machine learning algorithms like logistic regression, Decision trees, Support Vector
Machine and Random forest to achieve best accuracy for the propensity model 
 Extensive ing experience in RStudio packages and Python libraries like SciKit-Learn to improvethe model accuracy from 65% to 86%. 
 Strong practical experience in various Python libraries like Pandas, One dimensional NumPy and Twodimensional NumPy 
 Developed data visualizations in Tableau to display day to day accuracy of the model with newlyincoming data. 
 Identified factors to be considered for phase 2 development of the project and documented thosefindings with clear explanations 
 Implemented complete data science project involving data acquisition, data wrangling, exploratorydata analysis, model development and model evaluation. 
 
Environment: Teradata, Advanced SQL, RStudio (ggplot2, choroplethr, dplyr, caret), Python (Pandas,
NumPy), Machine Learning (Logistic Regression, Decision trees, SVM, Random forest), PyTorch, Keras, Knime Analytics tool, Tableau, Excel, SharePoint, Unix, Scala,
Data Scientist
Intelli InfoTech Inc - Plano, TX May 2016 to August 2017
Responsibilities: 
 Understanding business context and strategic plans and develop a data-driven business plan tosupport the attainment of business goals. 
 Designed and developed Power BI graphical and visualization solutions with business requirementdocuments and plans for creating interactive dashboards. 
 Gathered usage reports of Microsoft applications (Word, Excel, SharePoint online, Teams) of allemployees from Microsoft office portal in the form of excel sheets 
 Considered one-month period at a time and analyzed usage report data using RStudio packagesggplot2 to identify the patterns and trends of usage. 
 Extensively used PyTorch and Keras to build and train deep learning models. 
 ed with the data science team to build and deploy machine learning based models to predictcustomer churn and optimize customer acquisition using Teradata, Oracle, SQL, BTEQ, and UNIX.  Created story boards in Tableau and PowerBI for each application usage report categorized country, region and state wise. 
 Created Macros, to generate reports daily, monthly basis and moving files from Test to Production.  Analyzed feedbacks from employees regarding Microsoft applications usage in their day to day tasks and built predictive models using machine learning algorithms to understand the main issues those are hindering usage of these apps by the employees. 
 Documented results obtained and supplied Digital fluency reports of each individual team to theirrespective team leads. 
 Suggested individual teams' better practices of using these apps to improve their overall efficiency.  Responsible for ing with stakeholders to troubleshoot issues, communicate to team members, leadership and stakeholders on findings to ensure models are well understood and optimized. 
 
Environment: SQL*Plus, RStudio, Python (NumPy, Pandas), Machine learning algorithms (Logistic Regression, Decision trees), SharePoint Online, PyTorch, Tableau, PowerBI, Excel.
Data Analyst /Engineer
Raysoft Business Solution - Hyderabad, Telangana
January 2012 to December 2014
Responsibilities: 
 Responsible for reporting of findings that will use gathered metrics to infer and draw logicalconclusions from past and future behavior. 
 Implemented and managed several ETL  using Informatica PowerCenter by loading data froma variety of Data sources like flat files, JSON, XML files to Oracle for reporting. Source and target data were synced using Informatica and finally transformed data was stored in staging tables. 
 Performed Multinomial Logistic Regression, Random Forest, Decision Tree, SVM to classify package isgoing to deliver on time for the new route. 
 Created reporting tables for comparing source and target data and report data discrepancies
(mismatch, missing scenarios) found in the data. 
 Performed validations not received in the requirement document from the customer end and learntthe SQL queries which helped to attend defect triage calls. 
 Results obtained from report mappings were displayed using MicroStrategy which is a better UserInterface tool. 
 Implemented rule-based expertise system from the results of exploratory analysis and informationgathered from the people from different departments. 
 Created test plans for conducting unit testing of developed code. 
 Created deployment groups in QA environment and deployed flows from DEV to QAenvironment. 
 Performed debugging of the code as per inputs given by IST (Integrated System Testing) team anddeployed code into PROD environment after receiving approval from IST team. 
 Created and maintained complete documentation of project from beginning till end. 
 Performed internal enhancements of the jobs running in PROD environment. 
 Successfully maintained and managed all the jobs running in production environment by offeringproduction support. 
 Extensive hands on experience of HP Quality Center tool used for performing production supportactivities. 
 
Environment: Informatica Power Center 9.1(Repository Manger, Designer, flow Monitor, flow Manager), Oracle 11g, Toad for Oracle, SQL, UNIX, Shell scripting, SQL*Plus, MS Visio, Erwin Data Modeler, MicroStrategy.
Data Analyst
LMSOFT Business Solutions - Hyderabad, Telangana
July 2010 to December 2011
Responsibilities: 
 Communicated and coordinated with other departments to gather business requirements. 
 Gathering all the data that is required from multiple data sources and creating datasets that will beused in analysis. 
 Performed Exploratory Data Analysis and Data Visualizations using R, and Python. 
 In Preprocessing phase, used Pandas and Scikit-Learn to remove or impute missing values, detectoutliers, scale features, and applied feature selection (filtering) to eliminate irrelevant features.  Conducted Exploratory Data Analysis using Python Matplotlib and Seaborn to identify underlying patterns and correlation between features. 
 Used Python (NumPy, SciPy, Pandas, Scikit-Learn, Seaborn to develop variety of models andalgorithms for analytic purposes. 
 Setup storage and data analysis  in Amazon Web Services cloud computing infrastructure.
Education

Bachelor of  in Computer Science in  & Science
Texas A&M University - Kingsville, TX


Decision trees, Linear regression, Logistic regression, Machine learning, Random forest, Support vector machines, Apache spark, Hadoop, Python, Scripting, Visio, Ms access, Oracle, Sql, Clustering, Hadoop, Informatica, Teradata, Microstrategy, Sas
Additional Information

: 
 Programming Languages: R, Python 
 RDBMS: Teradata, Oracle 11g, SQL*Plus, MS Access, SQL developer 
 Machine Learning algorithms: Linear Regression, Logistic Regression, Decision Trees, 
Support Vector Machines, Random Forest, K Means 
Clustering 
 Frames: Hadoop Ecosystem, Apache Spark, Scala 
 : RStudio, Jupyter notebooks, Knime Analytics Tool, Informatica Power Center, Teradata,SQL, Toad for Oracle, PowerBI, Tableau, UNIX, Shell scripting, SAS e-miner, Microsoft Azure ML,
MicroStrategy, HP quality center, MS Azure, MS Visio",Data Scientist,emailed,indeed job post,resume
"
Proactive data science candidate experienced at developing an end-to-end data ecosystem and producing data driven 
solutions to solve business problems. Specialized at predictive modelling and effective communicator of findings in data.
EXPERIENCE
Live in Bing, Binghamton, New York, U.S.
Data Scientist Intern	(Python, MySQL, GCP, BigQuery, Tableau, Dataflow)			May 2018  May 2019
* Live in Bing is a Real Estate domain tech startup, where, fashioned surveys and analyzed trends to gain insight into the Binghamton University student housing market behavior to increase leasing rates by 5%.
* Led a team of 3 on Predictive Maintenance project (predictive modeling), where developed an end-to-end IoT system.
* Defined goals and KPIs for product development and constructed data pipeline to ingest sensor data into GCP BigQuery. 
* Validated data and visualized by creating dashboard on Tableau for analysis and stakeholder presentation.
* Developed and deployed a predictive model by applying time series ARIMA technique to predict consumption and behavior of assets and to gain information about anomalies in the system.
Vikas Consultancy, Thane, India
Business Analyst	(Advanced Excel)								June 2014 - Aug 2016
* Vikas Consultancy being an Insurance consulting firm, managed client database on company portals and built on Excel.
* Handled execution of various client quality initiatives and marketing programs and supported ad-hoc requests.
* Ensured business process problems / requests are prioritized and communicated to the supporting organization.
* Increased referrals by 5%, revenue by 10% and secured a good efficiency in business.

Binghamton University, State University of New York
Master of Science in Industrial & Systems Engineering (GPA: 3.22)				   Aug 2017 - May 2019
Relevant courses: Operation Research, Probability & Statistics, Multivariate Data Analysis, Modelling & Simulation
University of Mumbai, Mumbai, India
Bachelor of Engineering in Instrumentation (Electrical) Engineering (GPA: 3.15) 			   Aug 2013 - May 2017	

Walmart Store Sales Forecasting using Advanced Regression techniques	(Python, pandas, sklearn)
* Forecasted sales of each department in each Walmart store by using Extremely Randomized Trees. Performed exploratory data analysis and used cross-validation techniques to improve model for best results.
Customer Segmentation using Cluster Analysis		(Python, pandas, sklearn, matplotlib)
* Segmented customers into categories by analyzing sales data of an online retailer using K-means algorithm. Performed data validation and feature engineering to transform unstructured data into a usable format for developing model.
Business Intelligence Analysis using Tableau	(Tableau)
* Assessed US Cities population dataset for Startup Expansion to see which of the regions are performing better.
* Identified which of the 10 new locations have better potentials for company to invest more funds into marketing.
Developing ETL pipeline using Apache Airflow (Apache Airflow, PostgreSQL, Python, API, json)
* Developed ETL pipeline using Airflow by setting up DAG to query the openweathermap.org API every day, process the json data and store it in a PostgreSQL database.
 
: Python, SQL, Tableau, R, Excel, Google Cloud Platform (GCP), BigQuery, Minitab
Packages and Libraries: pandas, numpy, scikit-learn, matplotlib, seaborn, XGBoost, TensorFlow
Machine Learning: regression, classification, clustering, random forest, decision trees, SVM, KNN, boosting, PCA, regularization, time series analysis, gradient descent, inferential statistics
: data analysis, visualization, feature engineering, advanced statistics, hypothesis testing, BI, stakeholder management
Certification: Lean Six Sigma Green Belt Certified  KPMG India Pvt Ltd.
Visa Status: F-1 OPT EAD 36-months STEM extension (up to 07/2022 without sponsorship)

",Data Scientist,emailed,indeed job post,resume
" 
 
Dynamic data  with proven academic and  in . Skilled in variety of functions inc 	luding data analysis, data engineering, and machine learning techniques, applied in the urban domain. Experience ing in cross 	-culture and functional teams who possesses a keen ability to think strategically and execute tactically.  
 
 	 
Sep 2018 	-	M.S. in Applied Urban Informatics, New York University - GPA 3.86 	Personal Info 
Sep 2019  	 	 Excelled in Data Science and Machine Learning course with focuses on Urban Science 	 
 	 	 Completed a capstone project to analyze the impact of Manhattan Congestion Surcharge 		on For-Hire vehicles using a Nested Multinomial Logistic Model 	 
 	 	 	 
Sep 2014 -  	B.S. in Informatics, University of California, Irvine - GPA 3.83 	 
Mar 2018  	 Completed courses in Computer Science and Software Design 	 
 Mastered 5 programming languages 	
 Completed a senior project building a doctor-scheduling online frame for 	 
	NeoGenomics Laboratories 	 
 Minored Business Accounting 	 
	 	 
Project Experience 	 
Jan 2019 - 	Impact of Manhattan Congestion Surcharge on For-Hire-Vehicles 	 
Present  	 Evaluated the impact of the congestion surcharge policy implemented in Manhattan and 
 	assessing whether the current price is appropriate or not. 	 Data Analysis 
 Built a nested multinomial logistic model to simulate peoples transportation choices 	 Data Engineering  	between public transportation, Taxi, private/shared For-Hire-Vehicles, and walking, which 	 Machine Learning  	will be used to predict the mode shift after the surcharge. 
 Statistical Modeling 
 
Feb 2019 - 	Felony Amount Prediction based on Street views and Demographics data                	 Anomaly Detection 
May 2019  Extracted Street view features from Google images using Pyramid Scene Parsing Net. 	 Data Visualization 
 Developed and optimized machine learning models (Decision Trees, Random Forest,  Human-Centered Design  Gradient Boost, SVM, Native Based) to predict the felony amount using the merged street 
 	view data and demographics data.  	 UI/UX Design 
 A/B Testing 
Feb 2019 - 	Text Analysis on Movie Reviews 	 Prototyping                          
May 2019   Explored the differences between Marvel and DC movies from the audiences perceptive 
  	that is, through text analysis of movie reviews. 	Programming/Software 
 Conducted sentiment analysis, term frequency analysis, machine learning models, and topic models on Marvels and DCs reviews to assess the differences between their movies 	 Python in terms of styles, themes, and audience experiences. 
 R 
 Experience 	 SQL 
 Spark 
Dec 2018 - 	Data and Web Development Intern - United Nations Development Programme 	 Hadoop Apr 2019    	(UNDP), New York 
                                                                                      JavaScript  	 Supported data collection, data cleaning, data analytics, and live data integrations in  
 	that analyze social neting behaviors and usefulness of knowledge products.  	 HTML, CSS 
                                                                                               Assisted the data specialists with training a machine learning system to understand and extract 	 Java 
 	the context of lessons learned from project reports. 	 Jupyter 
                                                                                               Leaded UI/UX design in data visualizations and web reports. 	 GitHub 
 	 
May 2018 - Web Usage Analysis & UI/UX Design Intern - Real Int'l SCM Corp., CA  	 Power Bi 
Jul 2018 	 Designed and built a  website and a commercial slogan for the company. 	 SharePoint 
 	 Collected weekly website usage data, summarized and presented the data in weekly reports.  	 Tableau  	 Analyzed the web data and conducted Search Engine Optimization (SEO) to improve online 	  	visibility for the website. 
 	 	 Languages 
Jun 2016 - 	Assistant Intern in User Research Group - Topway Video Communication Co. Ltd, 
Aug 2016 	China                                                                      	 English 
 Performed user interviews, user testing on companys new system and interface. 	 Chinese/Mandarin 
 Documented, analyzed and presented the collected user data using Microsoft Words, Excel 	 Cantonese and PowerPoint. 
 ",Data Scientist,emailed,indeed job post,resume
"EXPERIENCE 

	Niagara Bottling, LLC, Ontario, California  	 	 	 	 	 	 	 	 	 	 	 	 	 	 	 	01/2019  05/2019 
Practicum Data Analyst (Project Team Lead) 
 Ensured project milestones were met by managing a team of 5 to add business values and provide actionable insights.  
 Predicted missed pickup or transportation failures and identified the strongest indicators that a load will be late. 
 Implemented classification models including logistic regression, decision tree, and support vector machine using R and Python. 
 Performed data cleaning, exploratory analysis and scraped weather information as external factor to include in our model; leveraged Tableau to create data visualization dashboards; presented business issues and viable suggestions. 
Deloitte Touche Tohmatsu CPA LLP, Shanghai, China     	 	 	 	 	       	 	 	   10/2016  04/2018  Audit & Assurance, Associate 
 Analyzed financial statements of both listed and non-listed Chinese and international companies in , Media & Telecommunications industry(formulas, pivot tables).  
 Applied analytical thinking and  skepticism to surface possible audit issues and documented judgements. 
 Improved clients internal control environment after identifying risks by conducting interviews with clients key personnel from different departments. 
 Offered analysis report of significant audit issues as well as their impacts on the entity and the engagement. 
 Supervised tax memo delivery and ensured high-quality report to clients by collaborating with teams from other departments.  
	Nielsen, Guangzhou, China     	 	 	 	 	      	 	 	 	       	 	 	 	 	   11/2015  04/2016  
	CPG Vertical, Market Analyst Intern 	 	 
 Responsible for drafting monthly/quarterly insights reports for 4 local FMCG clients under the supervision of senior managers. 
 Extracted clients and competitors sales/marketing related data from database, Analyzed raw data using advanced Excel functions like conditional formatting, Pivot Tables and VLOOKUP. 
 Assisted in developing analytical solutions and utilized consumer insights to help clients grow their market.  
 Visualized marketing data to assist in the preparation of proposals and presentations.  
  

	Airbnb Homes Pricing Strategy Analysis  	 	 	 	 	 	 	 	 	 	 	 	 	 	 	 	 	 	 	 	 2019 
 Analyzed Seattles Airbnb Homes listings information to explore how to remain competitive in the Airbnb Homes market. 
 Identified the key features that determine the listing price, as well as the attributes that can serve as determinants of customer influx, by applying regression and decision tree model (accuracy rate: 83%). 
 Performed data cleaning, data analysis, and data visualization using Pandas, NumPy, Seaborn, Matplotlib, Scikitlearn libraries in python and in Tableau. 
	Customer Insights at Sun Country Airlines Analysis  	 	 	 	 	 	 	 	 	 	      	 	 	 	 	 2018 
 Applied RMySQL, dplyr, ade4, and lubridate packages in R for data manipulation and data cleanup. 
 Used K-Means clustering algorithm to cluster customer data, summarized the key characteristics of each customer segment, and offer targeted advice to better achieve their business s. 
 Visualized our findings using Tableau and ggplot2 in R, and presented the insights derived from the analysis, aligned with the airlines business . 
 

Strengths: Data Analytics; Predictive analytics; Data mining; Data visualization; Financial analysis; Communication ; Team Collaboration ;  under tight timelines and pressure; Time management; Fast learning; Organization  
Computer: Advanced Excel(formulas, pivot tables, macros, etc.); Microsoft PowerPoint; Microsoft Word; SQL; 
Alteryx; Tableau; R; Python; NLP(TFIDF, N-gram, sentiment analysis, etc); Power BI; Arena; Weka; MegaStat; ETL 
 

	University of California, Irvine | Irvine, CA | Master of Science in Business Analytics, GPA 3.76  	 	 06/2019 
 	Beta Gamma Sigma member; UCI 2018 Faculty Fellowship Recipient; Most Inspirational Award 
   Jinan University | Guangzhou, China | Bachelor of Management, Accounting, GPA 3.79 	 	 	 	 	 06/2016   	Jinan University 2014 Scholarship Recipient ",Data Scientist,emailed,indeed job post,resume
" 	 	 	 	 	 	 	 	  	 	 	 	 	 	 
The University of Texas at Austin 	Master of Science in Information  and Management 	May 2019     
 	Course Includes: Big Data & Distributed Programming, 	 
Business Data Science, Advanced Data Mining & Web Analytics, 
Cognitive Computing, User Generated Content Analytics  Overall GPA: 3.6/4.0 
 	 	 
Beijing University of Posts and 	Bachelor of Engineering in Telecommunications Engineering with 	June 2017    
Telecommunications  	Management 
Dual-degree program with Queen Mary University of London Overall GPA: 3.5/4.0 
 
  	 	 	 	 	 	 	 	 	 	 
* Programming Languages: R, Python, SQL, PL/SQL, C, JAVA, Kotlin, Html, CSS, JavaScript 
* Computer Softwares/Platforms: MS Office, Git, Jupyter Notebook, Amazon AWS, Azure, Google Cloud Platform 
* Data Science/Analysis : Machine Learning, Natural Language Processing (NLP), NumPy, Pandas, Scikit-Learn, Apache Spark, Hadoop Map/Reduce, Tensorflow, Excel Pivot table, Matplotlib, Seaborn, Power BI, Tableau 
 
EXPERIENCE 	  	 	 	 	 	 	 	 	 	 	 	 	 	 UTIMCO  Data Analyst Intern, IT Capstone Project; Austin, Texas, United States                                  Jan. 2019  May 2019 
* Collected time series data of financial factors from Bloomberg and refine the raw data into datasets for analysis ? 	Conducted data cleaning and transformation including linking split time series and getting percentage change 
* Applied machine learning techniques PCA and K-means clustering to find key factors among financial categories ? 	Created data visualizations, wrote reports and presented findings from data analysis to clients  
 
Baidu, Inc.  Product Manager Intern, Baidu Map Client Division; Beijing, China                                 Nov. 2017  Apr. 2018 ? 	Conducted whole software development process and created business cases to improve employees  efficiency 
* Collaborated with software engineers, business development and operating teams to create product positioning ? 	Designed the product architecture and website prototype for content management system (CMS) with Axure 
* Collected and summarized daily page view data from A/B test with Excel to evaluate products and get insights  
 
ACADEMIC  	  	 	 	 	 	 	 	 	 	 	 	 	 Stock Price Data Analysis  University of Texas at Austin, Austin, Texas, United States                        Apr. 2019  May 2019 ? 	ed on historical stock data of 10 tech companies from 2010 to 2015 and predicted the stock price trends  
* Made visualization for the historical stock data and the fundamental financial data of different companies in Python 
* Conducted fundamental analysis with Linear Regression and applied Arima, LSTM models for time series prediction  
 
User Generated Content Data Analysis on Twitter University of Texas at Austin, United States                            Dec. 2018 
* Wrote Python script to scrape from Twitter and collected 4k tweets involving content of 2018 Texas Senate Race ? 	Conducted sentiment analysis on the tweets regarding to the candidates collected from large versus small cities 
* Visualized sentiment data of candidates with plot in Python to show how people feel about different candidates ? 	Provided findings and recommendations to different candidates based on topic modelling and NLP techniques 
 
Database Design and SQL Programming Project  University of Texas at Austin, Austin, Texas, United States     Nov. 2018 
* Created a web management system with Python and Django frame that allows all employees to manage their points and exchange rewards as well as administrator to view all the transaction records and data reports 
* Designed and created an Oracle database with SQL commands to store user information and transaction records ? 	Wrote SQL queries to fetch data from the database and SQL commands to create triggers, views and reports 
 
Kaggle Competition for Binary Classification  University of Texas at Austin, Austin, Texas, United States            Oct. 2018 ? 	Researched individually to predict binary outcome based on the given 3.6MB dataset with unknown features 
* Described the dataset with Python and Pandas package and visualized data with matplotlib and seaborn libraries 
* Conducted feature engineering then implemented models such as Logistic Regression, Random Forest, XGBoost, etc  ? 	Tuned hyperparameters to improve models performance and achieved AUC score of 89.39 % 
 
Movie Recommender using Map-Reduce  University of Texas at Austin, Austin, Texas, United States                   Sep.2018 
* Developed a movie recommendation system with Hadoop Map/Reduce frame and Spark RDDs in Python ? 	Generated and provided users customized recommendations based on users ratings and genres 
 
HONORS 	 	 	 	 	 	 	 	 	 	 	 	 	 	 ? 	Award for excellent final year capstone project                         	 	 	                                Summer 2017 
* Scholarships to Beijing University of Posts and Telecommunications                                   Fall 2014, Fall 2015, Fall 2016 ",Data Scientist,emailed,indeed job post,resume
"

 and Experience:
* Ten years of experience in developing different Statistical Machine Learning, Text Analytics,and DataMining solutions across various business functions: providing BI, Insights and Reporting frame to optimize business outcomes throughdataanalysis.
* Experience in Machine Learning, Deep Learning,DataMining with large datasets of structured and unstructureddata,DataValidation,Dataacquisition,DataVisualization, Predictive Modeling and developed predictive models that help to provide intelligent solutions.
* Strong mathematical knowledge and hands on experience in implementing Machine Learning algorithms like K-Nearest Neighbors, Logistic Regression, Linear regression, Nave Bayes, Support Vector Machines, Decision Trees, Random Forests, Gradient Boosted Decision Trees, Stacking Models.
* Good knowledge of Hadoop Architecture and various components such as HDFS, Job Tracker, Task Tracker, Name Node,DataNode, Secondary Name Node, MapReduce concepts, and ecosystems including Hive and Pig.
* Experience withdatavisualization using  like GGplot, Matplotlib, Seaborn, Tableau, R Shiny and using Tableau software to publish and presenting dashboards, storyline on web and desktop platforms. 
* Experienced in pythondatamanipulation for loading and extraction as well as with python libraries such as NumPy, SciPy and Pandas and Spark 2.0 (PySpark, MLlib) to develop variety of models and algorithms for analytic purposes.
* Well experienced in Normalization and Standardization techniques for optimal performance in relational and dimensional database environments.
* Thorough grounding in all phases of data analysis (both structured and unstructured), including definition and analysis of questions with respect to available data and resources, overview of data and assessment of data quality, selection of appropriate predictive models, statistical tests and presentation of results.
* Proficient in Natural Language Processing (NLP), Text Analytics etc. using R and Python (NLTK, genism, TextBlob etc.). Used NLP- Bag of words / N-gram algorithms, Term-document matrices, Text categorization and text routing,
* Processing unstructured text, Processing speech / using speech-to-text algorithms and, data mining and big data algorithms and methods.
* Developed machine learning solutions in Natural Language Processing (NLP), document classification, Named Entity Recognition (NER), topic modelling, document summarization, computational linguistics, advanced and semantic information search, extraction, induction, classification and exploration.
* Excellent knowledge of Machine Learning, Mathematical Modeling and Operations Research. Comfortable with R, Python, SAS and Weka, MATLAB, Relational databases. Deep understanding & exposure of Big Data Eco-system.  
* Excellent knowledge and experience in using open source NLP packages such as NLTK, Word2Vec, SpaCy, Gensim, Standford CoreNLP.
* Experience in understanding of NLP/ML & algorithms and models (GLMs, SVM, PCA, NB, Clustering, DTs) and their underlying computational and probabilistic statistics.
* Experience building Machine Learning & NLP solutions over open source platforms such as SciKit-Learn,Tensorflow,TensorBoard, Keras, PyTorch, SparkML, Torch, Caffe, H2O
* Understanding of different components of Hadoop ecosystem such as Hue, Pig, Hive, HBase, HDFS, Map-reduce, Flume etc.
* Hands on Experience on Customer Churn, Sales Forecasting, Market Mix Modeling, Customer Classification, Survival Analysis, Sentiment Analysis, Text Mining, Recommendation Systems.
* Experience in using Statistical procedures and Machine Learning algorithms such as ANOVA, Clustering, Regression and Time Series Analysis to analyzedatafor further Model Building.
* Installed and configured Kubernetes and Docker on google cloud platform (GCP) in Dev, Test, Stage and Production Environments  
* Configured Microservices on GitHub with Kubernetes deploy file, service file and config maps.
* Hands on experience in implementing Dimensionality Reduction Techniques like Truncated SVD, Principal Component Analysis, t-Stochastics Neighborhood Embedding (t-SNE).
* Hands on experience on Deep Learning Techniques such as Back Propagation, Choosing Activation Functions, Weight Initialization based on Optimizer, Avoiding Vanishing Gradient and Exploding Gradient Problems, Using Dropout, Regularization and Batch Normalization, Gradient Monitoring and Clipping Padding and Striding, Max pooling, LSTM.
* Expertise writing production quality code in SQL, R, Python and Spark. Hands on experience building regression and classification models and other unsupervised learning algorithms with large datasets in distributed systems and resource constrained environments, along with that used MongoDB for extractiondata.
* Experience inDataModeling retaining concepts of RDBMS, Logical and PhysicalDataModeling until 3NormalForm (3NF) and MultidimensionalDataModeling Schema (Star schema, Snow-Flake Modeling, Facts and dimensions). Hands on experience in optimizing the SQL Queries and database performance tuning in Oracle, SQL Server and Teradata databases.
* Experience using multiple ETL  inDataAnalysis,DataMigration,DataCleansing, Transformation, Integration,Data Import, andDataExport such as Ab Initio and Alteryx.



Temple University, Philadelphia, PA 
Masters of Science, Electronics and Computer Engineering

RV College of Engineering, Bangalore, India 
Bachelors of Engineering, Electronics and Computer Engineering

 
Statistics/ML
Exploratory Data Analysis: Univariate/Multivariate Outlier detection, Missing value imputation, Histograms/Density estimation, EDA in Tableau 
Supervised Learning: Linear/Logistic Regression, Lasso, Ridge, Elastic Nets, Decision Trees, Ensemble Methods, Random Forests, Support Vector Machines, Gradient Boosting, XGB, Deep Neural Nets, Bayesian Learning
Unsupervised Learning: Principal Component Analysis, Association Rules, Factor Analysis, K-Means, Hierarchical Clustering, Gaussian Mixture Models, Market Basket Analysis, Collaborative Filtering and Low Rank Matrix Factorization
Feature Engineering: Stepwise, Recursive Feature Elimination, Relative Importance, Filter Methods, Wrapper Methods and Embedded Methods
Statistical Tests: T Test, Chi-Square tests, Stationarity tests, Auto Correlation tests, Normality tests, Residual diagnostics, Partial dependence plots and Anova
Sampling Methods: Bootstrap sampling methods and Stratified sampling
Model Tuning/Selection: Cross Validation, AUC, Precision/Recall, Walk Forward Estimation, AIC/BIC Criterions, Grid Search and Regularization
Time Series: ARIMA, Holt winters, Exponential smoothing, Bayesian structural time series

Machine Learning /
Deep Learning
R: caret, glmnet, forecast, xgboost, rpart, survival, arules, sqldf, dplyr, nloptr, lpSolve, ggplot
Python: pandas, numpy, scikit-learn, scipy, statsmodels, matplotlib, tensorflow
SAS: Forecast server, SAS Procedures and Data Steps
Spark: MLlib, GraphX
SQL: Subqueries, joins, DDL/DML statements
Databases/ETL/Query
Teradata, SQL Server, Postgres and Hadoop (MapReduce),Cloudera stack; SQL, Hive, Pig and Alteryx,NOSQL,Source Code Management
Visualization
Tableau, ggplot2 and RShiny
Prototyping
PowerPoint, RShiny and Tableau EXPERIENCE
Computer Sciences Corporation (CSC), Tysons Corner, VA                	              June 2015  Till date 
Lead Data Scientist/ SME  Artificial Intelligence + Machine Learning

KEY 
App Recommender System:
> Developed a personalized recommender system using recommender algorithms (collaborative filtering, low rank matrix factorization) that recommended best apps to a user based on similar user s. The recommendations enabled users to engage better and helped improving the overall user retention rates at CSC
Sales Forecasting:
> Forecasted sales and improved accuracy by 10-20% by implementing advanced forecasting algorithms that were effective in detecting seasonality and trends in the patterns in addition to incorporating exogenous covariates. Increased accuracy helped business plan better with respect to budgeting and sales and operations planning

Anomaly Detection:
> Created interactive dashboard suite that illustrated outlier characteristics across several sales-related dimensions and overall impact of outlier imputation in R (Shiny).Used iterative outlier detection and imputation algorithm using multiple density-based clustering techniques (DBSCAN, kernel density estimation)

Cross Sell and Upsell Opportunity Analysis:
> Implemented market basket algorithms from transactional data, which helped identify items used/purchased together frequently. Discovering frequent item sets helped unearth cross sell and upselling opportunities and led to better pricing, bundling and promotion strategies for sales and marketing teams

Customer Churn Prediction:
> Predicted the likelihood of customer churn based on customer attributes like customer size, RFM loyalty metrics, revenue, type of industry, competitor products and growth rates etc. The models deployed in production environment helped detect churn in advance and aided sales/marketing teams plan for various retention strategies in advance like price discounts, custom licensing plans. 

Customer Purchase Propensity Modeling:
> Built machine learning based regression models using scikit-learn python frames to estimate the customer propensity to purchase based on attributes such as customer verticals they operate in, revenue, historic purchases, frequency and recency behaviors. These predictions helped estimate propensities with higher accuracy improving the overall productivity of sales teams by accurately targeting the prospective clients.

Price Elasticity Analysis:
> Measured the price elasticity for products that experienced price cuts and promotions using regression methods; based on the elasticity, CSC made selective and cautious price cuts for certain licensing categories.

Customer Segmentation:
> Measured the price elasticity for products that experienced price cuts and promotions using regression methods; based on the elasticity, CSC made selective and cautious price cuts for certain licensing categories.
> Customer segmentation based on their behavior or specific characteristics like age, region, income, geographical location and applying Clustering algorithms to group the customers based on their similar behavior patterns.
> The results from the segmentation helps to learn the Customer Lifetime Value of every segment and discover high value and low value segments and to improve the customer service to retain the customers.
> Used Principal Component Analysis and t-SNE in feature engineering to analyze high dimensionaldata.

BB & T Bank, Greensboro, NC                                                                                                                                                        
Data Scientist  						                                       July 2013  May 2015

Played a key role in developing and maintaining statistical and machine learning models that mine, analyze and turn BB & T customer and sales data into insights that helped BB & T make strategic decisions that led to growth in their user base and revenue

Customer Participation Analysis:
> Built relational databases in SQL server of several flat files of partner information from several large flat files in Python. Used logistics regression and random forests models in R/Python to predict the likelihood of customer participation in various marketing programs. 
> Designed and developed visualizations and dashboards in R /Tableau that surfaced the primary factors that drove program participation and identified the best targets for future targeted marketing efforts.
> Performed Data Profiling to learn about behaviour with various features such as traffic pattern, location, time, Date and Time etc. Integrating with external data sources and APIs to discover interesting trends.
> Involved in various pre-processing phases of text data like Tokenizing, Stemming, Lemmatization and converting the raw text data to structured data.
> Personalization, Target Marketing, Customer Segmentation and profiling.
Customer Life Time Value Analysis:
> Projected customer lifetime values based on historic customer usage and churn rates using survival models. Understanding customer lifetime values helped business to establish strategies to selectively attract customers who tend to be more profitable for BB & T. 
> It also helped business to establish appropriate marketing strategies based on customer values.
> Performed Data Cleaning, features scaling, featurization, features engineering.
> Used Pandas, NumPy, SciPy, Matplotlib, Seaborn, Scikit-learn in Python at various stages for developing machine learning model and utilized machine learning algorithms such as linear regression, Naive Bayes, Random Forests, Decision Trees, K-means, & KNN.
> Implemented number of Natural Language process mechanism for chatbot.
> Customer segmentation based on their behaviour or specific characteristics like age, region, income, geographical location and applying Clustering algorithms to group the customers based on their similar behaviour patterns.
Customer Segmentation:
> Developed 11 customer segments using unsupervised learning techniques like KMeans.
> The clusters helped business simplify complex patterns to manageable set of 11 patterns that helped set strategic and tactical objectives pertaining to customer retention, acquisition and spend 
Price Optimization and Revenue management:
> Measured the price elasticity for products that experienced price cuts and promotions using regression methods; based on the elasticity, BB&T made selective and cautious price cuts for certain licensing categories
> The results from the segmentation helps to learn the Customer Lifetime Value of every segment and discover high value and low value segments and to improve the customer service to retain the customers.
> Performed Clustering with historical, demographic and behavioural data as features to implement the Personalized marketing that offers right product to right person at the right time on the right device.
> Analyzed high volume, high dimensional client and survey data from different sources using SAS and R.
> Used Principal Component Analysis and t-SNE in feature engineering to analyse high dimensional data.

Bank of America, Jersey City, NJ 
Data Scientist 		         				                                        Aug 2012 - June 2013

Played key role in developing and deploying DFAST Stress Test models across several bank portfolios. Provided architectural leadership on several high priority initiatives including account prioritization, account prospecting, and opportunity scoring. Drove the creation of comprehensive datasets encompassing user s and behaviors, and incorporating a wide variety of signals and data types.

Forecasting Loan balance:
> Forecasted bank-wide loan balances under normal and stressed macroeconomic scenarios using R. Performed variable reduction using the stepwise, lasso, and elastic net algorithms and tuned the models for accuracy using cross validation and grid search techniques.

Top down Models (Commercial Real Estate):
> Automated the scraping and cleaning of data from various data sources in R and Python. Developed Bankss loss forecasting process using relevant forecasting and regression algorithms in R. 
> The projected losses under stress conditions helped bank reserve enough funds per DFAST policies.

Fraud Detection:
> This project includes creation of statistical machine learning models which implements Customer Segmentation, Customer Lifetime Value, Fraud detection, Sales Forecasting, Anomaly Detection, Customer churn prediction, Elasticity Analysis, propensity Modeling.
> Gathered requirements from Business and reviewed business requirements and analyzingdatasources.
> PerformedDatacollection,DataCleaning, features scaling, features engineering, validation, Visualize, interpret, report findings, and develop strategic uses ofdataby python libraries like NumPy, Pandas, SciPy, Scikit-Learn.
> Involved with Recommendation Systems such as Collaborative filtering and content-based filtering.
> Studied and implemented Fraud detection models to monitor the unconventional purchases from customer bases and alert them with updates.
> Analyzed and implemented few research proofs of concept models for Real time fraud detection over credit card and online banking purchases.
> ed with Credit Analysis, Risk modeling algorithms to implement in customer acquisition strategies into the real time business.      
Nationwide Insurance,	Columbus, OH
Data Scientist 						                                       Jan 2012  July 2012 

Policy Payment Default Prediction:
> Built classification models using several features related to customer demographics, macroeconomic dynamics, historic payment behavior, type and size of insurance policy, credit scores and loan to value ratios and with the model predicted the likelihood of default under various stressed conditions.
> Data required extensive cleaning and preparation for machine learning modeling, as some observations were censored without any clear notification
> Carried out segmentation, building predictive models and integrating secondary and primary data using R, python, SPSS and SQL
> Built classification models that predict the probability of a customer's response to a cross-sell campaign using python.
Customer Trial Repeat Analysis:
> Designed and deployed real time Tableau dashboards that identified policies which are most/least liked by the customers using key performance metrics that aided the company for better rationalization of their product offerings.
> Analyzed large data sets for reporting and visualization using R.
> Tracked campaigns and communicate campaign performance and ROI analysis.
> We used modifiers, including L1 regularization, dropout, and Nesterov momentum to enhance the neural net and optimize generalization.
> Provided reporting dashboard for stakeholders and project owners to rapidly provide information via plotly, bokeh, matplotlib, and seaborn.
> We required internally generated data on policies, premiums, and payouts from customer databases, to model policy payout as a function of survival model outputs.
> Retrieved data from devices, which was streamed to our company database using AWS-kinesis (real-time data streaming).
Customer Segmentation:
> Clustered the customers based on demographics, health attributes, policy inclinations using hierarchical clustering models and identified strategies for each of the clusters to better optimize retention, marketing and product offering strategies
> Responsible for managing monitoring and coordinating claims fraud risk management .
> Performed Sentiment Analysis using social media and survey data to address customer grievances and brand awareness using Natural Language Processing (NLP).
> Solved a binary classification problem (transferring to lower risk group or not with given financial incentive) with a logistic regression. 
> An artificial neural net was utilized with Keras/TensorFlow, PyTorch in python to solve binary classification problem for premiums and their intersection with the discriminant.
Social Media Campaign Application:
> For this largest property and casualty insurer in the United States, with over 120 offices located in 54 locations, and offers commercial, property, casualty, specialty and personal insurance services- developed, executed, tracked and analyzed targeted marketing campaigns, Utilized the social media campaign management application to develop and report on complex, multi-step campaigns. Analyze campaign performance, report on key business metrics and develop insights through customer analysis.
> Used unsupervised learning techniques such as K-means clustering and Gaussian Mixture Models to cluster customers into different risk groups based on health parameters provided through wearable technology regarding their activities and health goals
> Multiple statistical modeling approaches were applied to determining the usefulness of the wearable technology data for various insurance products.
> Survival modeling techniques, such as Poisson regression, hidden Markov models, and Cox proportional hazards, were used to model time to different events utilizing wearable data (time to death for life insurance, time to next hospital visit, time to next accident, time to critical illness, etc.)
> Documented methodology, data reports and model results and communicated with the Project Team / Manager and other data scientists to share the knowledge on retention analytics
> Took End-to-end ownership of designing, developing and deploying machine learning models (data preparation => variable selection => model building and evaluation => deployment)

JBS Swift, Greely, CO					          		                                                            DataModeler/DataAnalyst                                                                                   Jun 2010 to Dec 2011

Marketing Campaign Measurement:
> Built executive dashboards in Tableau that measured changes in customer behavior post campaign launch; the ROI measurements helped company to strategically select the effective campaigns 
Credit Risk Scorecards:
> Built credit risk scorecards and marketing response models using SQL and SAS. Evangelized the complex  analysis into easily digestible reports for top executives in the company.
> Developed several interactive dashboards in Tableau to visualize nearly 2 Terabytes of credit data by designing a scalable data cube structure.
Others
> Analyzed large datasets to provide strategic direction to the company. Performed quantitative analysis of ad sales trends to recommend pricing decisions.
> Conducted cost and benefit analysis on new ideas. Scrutinized and tracked customer behavior to identify trends and unmet needs.
> Developed statistical models to forecast inventory and procurement cycles. Assisted in developing internal  for data analysis.
> Achieved a broad spectrum of end results putting into action the ability to find, and interpret rich data sources, merge data sources together, ensure consistency of data-sets, create visualizations to aid in understanding data, build mathematical models using the data, present and communicate the data insights/findings to specialists and scientists in their team
> Implemented full lifecycle inDataModeler/DataAnalyst,Datawarehouses and DataMarts with Star Schemas, Snowflake Schemas, and SCD& Dimensional Modeling Erwin.
>  Performeddatamining ondatausing very complex SQL queries and discovered pattern and used extensive SQL fordataprofiling/analysis to provide guidance in building thedatamodel

HSBC, New York, NY                          
DataAnalyst /DataModeler                                                                                 Oct 2005 to May 2010

> ed with end users and analysts to provide analysis of credit card customer base on demographical basis and forecasting risk. 
> The permissions to grant a credit card to an individual were based on the analysis done. 
> The information of the person applying for the credit card is gathered and processed for further approval or rejection.
> Involved in Analysis & Marketing Team to make business decisions
> Involved with key departments to analyze areas and discuss the primary model requirements for the project
> Documented methodology, data reports and machine learning model results and communicated with the Project Team / Manager to share the knowledge.
> Performed competitor and customer analysis, risk and pricing analysis and forecasted results for credit card holders on demographical basis
> Used Machine Learning algorithms and Natural Language Processing (NLP) for response modeling and fraud detection efforts for Credit cards.
> Developed needs-based segmentation that aided management in gaining a deeper understanding of consumer behavior. These segments assisted management in development and marketing of credit cards.
> Performed machine learning to estimate the probability of a new customer being classified as a good or bad customer.
> Utilized graph clustering algorithms, such as ClusterONE (Clustering with Overlapping Neighborhood Expansion) and MbiRW (measure and Bi-Random walk) which is a k-means-based net cluster algorithm.
> Utilized Natural Language Processing (NLP) techniques, such as term frequency-inverse document frequency (TF-IDF) to measure the importance of terms within the literature when constructing the document net for use with above mentioned graphical clustering algorithms.
> Used machine learning with Theano/Keras to further perform risk assessment of the potential adverse effects for drugs and their motifs. 
> Collaborated with molecular simulation experts to combine literature-based discovery with computer simulations for the purpose of predicting the risk of certain drugs 
> Utilized multiple cross validation frames (k-folds and train-validate-test, where appropriate) to optimize, and ensure generalizability for drug risk assessment.
> Since there is a vast number of features/variables we use principal component analysis (PCA) for dimensionality reduction on large text datasets.
> Design, develop and produce reports that connect quantitative data to insights that drive and change business.
> Supported client by developing Machine Learning Algorithms on big data using PySpark to analyze transaction fraud, Cluster Analysis etc.
> Perform ad hoc custom analysis as needed using SQL and R.
> Designed and published visually rich and intuitively interactive Tableau books and dashboards for executive decision making.
> Maintain and enhance data model with changes and furnish with definitions, notes, reference values and check lists.
> Participated in JAD sessions, gathered information from Business Analysts, end users and other stakeholders to determine the requirements
> Designed theDataWarehouse and MDM hub Conceptual, Logical and Physicaldatamodels
> Used Normalization methods up to 3NF and De-normalization techniques for effective performance in OLTP and OLAP systems.Generated DDL scripts using Forward Engineering technique to create objects and deploy them into the database
> ed with SME's and other stakeholders to determine the requirements to identify Entities and Attributes to build Conceptual, Logical and PhysicaldataModels.
> Used Star Schema methodologies in building and designing the logicaldatamodel into Dimensional Modelsextensively.


 (Not willing to relocate from NJ, can travel up to 60%, 1-2 days remote for non- NJ jobs, I have a Green Card)

",Data Scientist,emailed,indeed job post,resume
" 
 
~  and  ~
 	 
- Python 	- 	Molecular Dynamics Simulation 
- R 	- 	Machine Learning 
- RNA-Seq Analysis 	- 	MATLAB 
- SQL 	- 	Affinity Chromatography 
- Linux 	- 	Electroporation 
- Cheminformatics 	- 	FRET 
- Chimera 	- 	Gel Electrophoresis 
- COMSOL 	- 	Next Generation Sequencing 
- Solids 	- 	Polymerase Chain Reaction (PCR) 
- Swift Programming 	- 	Protein Purification 
 	 
~  ~ 

 
University of California, Riverside | Bachelor of Science in Bioengineering,  Riverside, CA                                                                  2015  2019 
GPA: 3.64 (Graduated Cum Laude)  
 
Completed Coursera Classes: Python for Genomic Data Science, Genomic Data Science with Galaxy, Algorithms for DNA Sequencing, Command Line  for Genomic Data Science, Bioconductor for Genomic Data Science, and Statistics for Genomic Data Science.  
                                                              
 ~ s ~ 

   Girke Bioinformatics Research Lab | University of California, Riverside                                                             August 2018  June 2019 
 Utilized ChemmineR package to parse through sdf files from ChEMBL database.  
 Implemented RSQLite and dplyr libraries in R to develop DrugBankR database.  
 Utilized Linux to submit and manage R scripts to High-Performance Computing Center (HPCC) at UCR.  
 Developed SQL functions to query the DrugBankR database for various assays and drugs.  
 Used RNA-Seq flow and systemPipeR library to analyze 18 paired-end read sets from Arabidposis thaliana. 
 
PillTrac: Capstone Project | University of California, Riverside                                                                            August 2018  June 2019 
 Collaborated with team members to develop blister pack with integrated NFC microcontroller to monitor patient compliance. 
 Utilized Eagle to create circuit designs for Printed Circuit Board (PCB) Manufacturing.  
 Developed LED matrix circuit with Arduino NFC module. 
 Utilized Swift to program iOS 12 smartphone application for monitoring medication regimen. 
 Developed business plan to determine financial viability of product.  
 
	Undergraduate  Research Fellow | University of California, Riverside 	        July 2017  June 2018 
 Obtained $5000 in funding as a stipend to support financial expenses of research project. 
 Computationally analyzed C-Reactive Protein (CRP) and Phosphocholine (PC) to quantify effectiveness in immune system. 
 Utilized Chimera to visualize physiological properties of CRP and PC. 
 Performed Molecular Dynamics simulations to visualize molecular fluctuations between CRP and PC with Linux commands. 
 Developed Python scripts to analyze trajectories of Molecular Dynamics Simulations.  
 Presented research at the 2018 UCR Undergraduate Research Symposium in Riverside, CA. 
 
BioMoDeL Research Lab | University of California, Riverside 	               July 2016  July 2018 
 Computationally analyzed the hormone resistin to evaluate compatibility as a biotherapeutic drug. 
 Applied the PDBePISA software to define stabilizing intermolecular interactions in resistin. 
 Utilized the bioinformatics tool AESOP to investigate electrostatic interactions between amino acids in resistin. 
 Published paper in the 11th edition of the UCR Undergraduate Research Journal.  
 Presented research at the 2017 Southern California Conferences for Undergraduate Research in Pomona, CA. 
 
Bio 155 Lab | University of California, Riverside                                                                       October 2018  December 2018 
 Transformed the CyPet-SUMO1 gene into Top-10 E.Coli cells via electroporation. 
 Verified CyPet-SUMO1 gene sequence through PCR and gel electrophoresis 
 Analyzed coding regions of pET28(b)-CyPet-SUMO1 using CLC Viewer and Chromas. 
 Performed protein expression and purification through centrifugation and affinity chromatography. 
 Measured energy transfer from CyPet-SUMO1 to YPet-Ubc9 through FRET assay.    ",Data Scientist,emailed,indeed job post,resume
"M.S. Statistics graduate looking for a full-time job in data science, machine learning, statistics or quantitative analysis. I am only considering the Los Angeles Metropolitan Area.
Authorized to  in the US for any employer
 Experience

Statistics Intern
Penumbra, Inc - Alameda, CA
May 2019 to Present
ing closely with the statistics team to automate repetitive SAS processes. 
Building the global and local SAS macro library for the statistics team. 
Creating, editing, testing and debugging SAS macros. 
Writing documentation detailing description, purpose, parameter definitions, execution requirements and expected output for each macro. 
Version-controlling SAS macro programs using Git with GitLab
Economic Policy Intern
Environmental Defense Fund - San Francisco, CA
February 2019 to May 2019
Created complex structured datasets from raw unstructured datasets.  
Data wrangling, cleaning and analyzing using R.  
Created raw unstructured datasets from raster and shape GIS files.  
Resampling, spatial joining and calculating zonal statistics using ArcGIS.  
Performed test runs to ensure no defects in the columns, rows or values in the datasets. Maintained version-controlled and thoroughly documented R notebooks.
Data Science Student Intern
State Compensation Insurance Fund - Pleasanton, CA
June 2018 to August 2018
Helped the predicative analytics and modeling team with the implementation of a new actuarial data mart through user acceptance testing. 
Performed UAT test runs on the claims payment aging by date of injury table. 
Debugged source scripts and target scripts to verify data matching test case between two different database schemas for all claims. 
Used pass through queries in SAS to use proc compare to compare the source script and target script tables. 
Solved all defects with exact match between the tables. 
Performed sanity checks to verify valid mathematical relationships between the facts in the claims payment aging table for all claims. 
Documented test runs of test cases with version controlled SAS programs in HP Application Lifecycle Management for possible review by the internal audit team.
SAT Rater
al - Los Angeles, CA
August 2016 to August 2017
Reading and scoring student SAT essays by rubric Completed 16 hours of holistic scoring training in scoring reading, writing, and analysis
Student Records Technician
University of Southern California - Los Angeles, CA October 2016 to February 2017
Processed undergraduate and graduate application materials for admissions review for national and international students. 
Sorted, filed, labeled, indexed and barcoded applications, transcripts, degrees, letters of recommendation, test scores, passports, correspondences, resumes, financial statements and statement of purposes. 
Troubleshooted application materials for potential mistakes in sorting, labeling, indexing and barcoding.
Accounting Specialist
Breakdown Services, Ltd. - Los Angeles, CA December 2015 to May 2016
Accounted for payments, invoices, accounts receivable, and accounts payable using Quickbooks. 
Received payments for services from customers and paid bills to vendors. 
Completed monthly bank reconciliations to accounts. 
Created daily, monthly, and yearly sales reports on Microsoft Excel. Created sales receipts for items purchased by customers.
Engagement Management Coordinator Support
KPMG - Los Angeles, CA
April 2014 to July 2014
Provided accounting, analytical, administrative, and informative support for the tax, audit, and advisory engagements of the firm. 
Accounted for the payments, invoices, accounts receivable, accounts payable of the firms engagements. 
Financial analysis with the different financial data of engagements. 
Set up, approved, and closed out engagements for the engagement team. 
Served as an information reference for firm related accounting and risk management systems.


Master's in Statistics
California State University, East Bay - Hayward, CA
August 2017 to June 2019
Bachelor of Arts in Philosophy and Psychology
California State University, East Bay - Hayward, CA
2012 to 2014


Data Analysis (2 years), Microsoft Office (8 years), Statistical Analysis (3 years), SAS (2 years), Python
(2 years), SQL (2 years), Big Data (2 years), Data Visualization (2 years), Hadoop (2 years), Bayesian Statistics (2 years)
Links

",Data Scientist,emailed,indeed job post,resume
"
Im a data scientist with a background in neural nets, ML models, and cloud computing. I have experience in analytic data frames using Python, SQL, and Excel. With an aptitude for math, my  is to build insightful yet pragmatic machine learning algorithms with a collaborative and driven team. 

 | 
Analysis Stack: Python 3.7 (Pandas, Numpy, Scikit- Learn, Keras, Tensorflow), SQL, Excel
Data Visualization: Python (Matplotlib, Seaborn), Excel, Tableau
Machine Learning: Regression Models, Classifications Models, Ensembles, Dimensionality Reduction (PCA), Clustering, Recommender Systems, Time Series Models, Parameter Tuning, Pipeline
Deep Learning: Keras Neural Nets (RNN, CNN, LSTM)
Natural Language Processing: Python (NLTK Stemming, TfidfVectorizer, VaderSentiment)
Data Science flow: Jupyter Notebook, GitHub, Databricks Cluster
Cloud Computing: AWS EC2, GCP Compute
Customer Relationship Management: Hubspot


Image Classification of Fruits and Vegetables 
 : Classify grocery items using convolutional neural nets 
 Executed and parameter tuned CNN modeling on processed images on Google Cloud Platform Compute. 
 Process: Flickr API, PIL, cv2, Google Cloud Platform, Keras CNN, ImageNet, Flask
 Results: Validation Loss score of .3988, Validation Accuracy score of .9183
Customer Data Metrics and Predictions  
 : Target key metrics to visualize and predict consumer behavior
 Executed feature engineering to implement several customer segmentations to predict customer lifetime value, churn probability, and next day purchase.
 Process: KMeans Clustering, Elbow Curve, Plotly, XGB Classifier, Cross Val Score, Classification Reports

Sales Forecast 
 : Predict future quantity sales
 Conducted times-series feature engineering to perform LSTM modeling to forecast sales quantity for the next six months 
 Process: Keras, EarlyStopping, Adler Test, MinMaxScaler, Inverse Transformation, Plotly 
 Results: Validation Loss score (MSE) of 0.0116, RMSE of 29,665


LLove Inc. 	    August 2018 - May 2019
Fashion wholesale company 
Sales Analyst 
 Analyzed sales report for each trade show with prior years performance, showcased customer retention rate, new customers, and average sales per order 
 Initiated market segmentation to target specific clients and personalize email campaigns 
 Responsible for creating sales dashboards in Excel for upper management
 Taught e-commerce associates Excel  and functions to avoid repetitive tasks (Ku add-on, vlookup, concat, pivot)- saving estimated 1.5 hrs of  per day per employee 
 Created and launched email campaigns by implementing merge tag customization- improving opening rate from 5% to 35% on Mailchimp

SupplyShift 	September 2017  January 2018
Software company focusing on supply chain management traceability, and decision-making
Finance and Operations Intern
 Evaluated quarterly net income to calculate Profits & Loss and Budget variance analysis
 Performed data scrubbing, and created data visualizations of Key Factors  of 2017 for the Board of Directors 
 Tracked and computed average sales cycle (in months) for each stage in SupplyShifts software supply chain (using Hubspot CRM) to elongate customer use 


General Assembly	  Los Angeles, CA
Data Science Immersive                                                                                                                           	2019                     
	  
University of California, Santa Cruz	 Santa Cruz, CA
Bachelors of Arts, Business Economics,  Information Management minor	2018

",Data Scientist,emailed,indeed job post,resume
"
:
* Machine learning in R, and other software
* Python and R programming
* Bioinformatics research, crime database research, real estate research
* Able to extract data from large, free, and available public data bases
* Clean data, organize data, create new fields to help analyze data
* Spreadsheets and database navigator
* filtering and merging of datasets for building relevant reports
* formula building in software to run programs to report credit accurately
* Tableau, Excel Advanced features
* Experience with Virtual Machines with VirtualBox, Cloudera, Hortons Ambari, Microsoft Azure, Apache Spark, Amazon Web Services (AWS) EC2, Linux CLI, SSH
* Experience developing application programs in Hadoop, R, and Python 
* Windows OS, Linux OS, Microsoft Office, Excel, Access, Python, R-Studio, SQL, HDFS, Stata, Html, Google Docs

:
       Master of Science 
       Data Science: Bioinformatics			Expected Graduation:  October 2019
       Lewis University, Romeoville, IL			
       Concentration:  Data Science
       
       Bachelor of Science					Graduated: December 2015
       University of California, San Diego, CA		
       Majors:  Economics and Math

 :  
* Used Linux in VMs to upload using SSH in HDFS, then ran HQL queries to analyze database information on airline arrivals and departures: https://themassagenegotiator.com/blog/f/example-codes-of-python-3-hql-t-sql-and-r 
* Set up different user account and permissions in AWS S3, also using AWS EC2 for cloud computing: https://themassagenegotiator.com/blog/f/aws-amazon-web-services-ec2-elastic-cloud-computing-vm 
* Analyzed crime statistics from 12 policing metropolitan cities and real estate statistics from Zillow using Tableau, Python, Excel, and R: https://themassagenegotiator.com/blog/f/cash-flow-wage-gap-and-theft-crimes-per-top-14-metros 
* Online computational research using GEO, BLAST, ENCODE, PFAM and the like: https://github.com/JanJanJan2018/UL_research_files_Final_Week_Due 
* After ing for a credit reporting agency, I developed a fictitious client database to help with formatting of credit reporting information using the Metro2 standards using random number generators and extracted data over the web of popular names of people and streets using Python and Excel: https://themassagenegotiator.com/blog/f/building-a-fictitious-database-for-credit-reporting-program
* 

* Exploratory analysis on word mining from text documents using R and Orange Biolab software for machine learning on document classification based on text mining: https://themassagenegotiator.com/blog/f/the-bio-lab-orange-word-cloud-software and https://github.com/JanJanJan2018/ufcNFLjloABkylieHaterComments/blob/master/FileRenamePasteName.R 

 EXPERIENCE:

* Crediauto Financial: Database Analyst and Credit Reporting Analyst | | April 2018-July 2018 employee | 2150 Palomar Airport Rd.Suite 209, Carlsbad, CA 92011 | supervisor: Michael Fattahi | 858-382-4077
* Massage Therapist: themassagenegotiator.com | 12+ years independent and employed. Various places employed or contracted with wages from minimum plus tips to only commission plus tips, or minimum plus service, commission, and tips. 
* Massage Heights | Chino Hills 91709 | 13925 City Center Dr. | May 2019  Present | Job Title: LMT | Supervisor: Tamera | 909-536-1477
* Massage Heights | Rancho Cucamonga 91739 | 12188 Foothill Blvd. | Mar 2019  June 2019 | Job Title: LMT | Supervisor: Phil | 909-922-2013
* Heller Chiropractic | Costa Mesa 92626 | 2900 Bristol St. | Feb 2019  Mar 2019 | Job Title: Independent LMT | Supervisor: Nastassia | 714-557-9454
* OC Wellness Center | Westminster  92683 | 14120 Beach Blvd. #214 | Sep 2018  Nov 2018 | Job Title: Independent LMT | Supervisor: Anna | 866-303-9355
* Massage Heights | Newport Beach 92660 | 1334 Bison Ave. | July 2017  Present | Job Title: LMT | Supervisor: Michelle or Kristin | 949-644-7352
* Massage Green | Corona 92881| 1312 E. Ontario Ave. | January 11, 2016  Present | Job Title: Licensed Massage Therapist CA | Supervisor: Libni | (951) 371-7918
* Hand and Stone Massage | Costa Mesa 92627| September 2014-January 2015 | Job Title: Licensed Massage Therapist | Supervisor: Tasha | (949) 645-4823 
* Massage Envy | Yorba Linda 92886 | November 2013-July 2014 | Job Title: Licensed Massage Therapist | Supervisor: Colleen | (714) 701-0200
* Massage Envy | Tustin 92782 | June 2012-October 2013 | Job Title: Licensed Massage Therapist | Supervisor: Brittany | (714) 617-8900 

",Data Scientist,emailed,indeed job post,resume
"As a Financial Systems/Data Analyst within the Global Financial Planning & Analysis team in Brea HQ, you will focus on the implementation of business intelligence solutions, including the development and enhancement of financial analysis and reporting tools that our finance business partners will utilize to deliver key insights and drive improved financial results. You will also work on projects related to database systems, data integration, ETL and report development, primarily in the Microsoft environment (Excel, Access, Power BI). Activities of the department are growing rapidly and we are looking to bolster our IT system capabilities. Team members are currently based in Brea, California, and in Geneva, Switzerland. You'll be trained and mentored by the Manager of Global Financial Analytics and Reporting, and receive on-the-job training potentially in Brazil, Switzerland or China. Provide ongoing support for system enhancements for financial planning and reporting systems Utilize your expert skills in Excel and VBA programming Implement Power BI tools and modeling Develop SQL-based projects to support the development of new reporting ETL (Extract, Transform, Load) as well as to troubleshoot existing financial reporting schedules and related processes. Role Responsibilities Lead financial reporting and system enhancement initiatives by defining requirements, designing solutions, and testing new functionality Develop new functionalities in reporting and pursue continuous improvement where there are gaps in processes or where additional data is needed for the business Develop and implement Power BI reporting to improve existing processes Recommend and implement process improvement/re-engineering in the form of high-impact initiatives as well as more routine items such as existing VBA scripts (using VBA, SQL, PowerShell...) Develop training materials for new and existing users, as needed Bachelor's degree (with 2+ years of direct work experience) or Master's Degree preferably in Computer Science or Management of Information Systems Minimum of two years of experience of demonstrated acumen using Excel, VBA, and Power BI/Tableau or related Second major or degree in Finance/Accounting, Business, or Economics is considered a plus Experience working in a Finance-related role or partnering with Finance staff is considered a plus SQL / Power Shell / Python / Access acumen and experience is highly preferred Analytical thinker with strong problem-solving and organizational skill s Strong attention to detail and ability to deliver high quality work product Able to work under pressure, meet deadlines and handle multiple projects simultaneousl y Demonstrates intellectual curiosity and desire to understand root cause of a problem Team player who is also able to work well independently Strong communication skills(and/or demonstrated desire to improve)-
",Data Analyst,job post,zip recruiter,Job post
